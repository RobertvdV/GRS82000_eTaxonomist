{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b019440f-db73-42cc-a9c1-2afaea19c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad6804b-0d4b-4dc9-b74f-15fc1a42866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data.pkl\", \"rb\")\n",
    "data = pickle.load(pickle_in)\n",
    "\n",
    "# Undict data\n",
    "data = list(chain.from_iterable(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b5d946-5cab-423b-97ea-df9cee73c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame using data\n",
    "df = pd.DataFrame(data, columns =['label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efad22a-8c30-4b78-9b32-3f4f25837dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2021, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2021, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9344a06f-101a-40cd-adf7-7449eeb50be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b73a44-8ab8-4927-9f27-bcc50317f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e35f2f-206b-4776-ae4d-967a0e170e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a DistilBERT configuration\n",
    "configuration = DistilBertConfig()\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = DistilBertModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1afeb3da-d7c2-4f5a-9549-2bd5030b1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 60,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 60,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 60,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4f38bb7-4fc5-4f58-9fb4-4f839c4f9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "436882b8-cdb8-4597-a6cd-5ca197dc4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff67859a-5eb0-4e9f-b388-f2c454e5ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9aed518-ab92-4f05-b16e-ef1dca05aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f74c791-c47a-4e60-ad23-b8c10b1156b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "612f111e-39b6-48b7-863f-ed2a2cf21501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5)          # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e498c8b-6c0e-4dac-b025-668515f0d8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.59234293 3.20729995]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/DL/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass classes=[0 1], y=9062     0\n",
      "4380     0\n",
      "469      0\n",
      "11460    0\n",
      "1130     0\n",
      "        ..\n",
      "15988    1\n",
      "7638     0\n",
      "10953    0\n",
      "6031     0\n",
      "1846     0\n",
      "Name: label, Length: 13708, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17a83f7f-c77a-4b3d-8a3d-4f5c2b80c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6ec668c-6d1b-4fd9-942f-0af87d462716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "634d749f-0548-412c-b046-936387edd3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "            # Calculate elapsed time in minutes.\n",
    "            #elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "            \n",
    "        # model predictions\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the validation loss between actual and predicted values\n",
    "        loss = cross_entropy(preds,labels)\n",
    "\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52fc8dff-981e-4b48-b56c-2562d1efe4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    857.\n",
      "  Batch   100  of    857.\n",
      "  Batch   150  of    857.\n",
      "  Batch   200  of    857.\n",
      "  Batch   250  of    857.\n",
      "  Batch   300  of    857.\n",
      "  Batch   350  of    857.\n",
      "  Batch   400  of    857.\n",
      "  Batch   450  of    857.\n",
      "  Batch   500  of    857.\n",
      "  Batch   550  of    857.\n",
      "  Batch   600  of    857.\n",
      "  Batch   650  of    857.\n",
      "  Batch   700  of    857.\n",
      "  Batch   750  of    857.\n",
      "  Batch   800  of    857.\n",
      "  Batch   850  of    857.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    108.\n",
      "  Batch   100  of    108.\n",
      "\n",
      "Training Loss: 0.001\n",
      "Validation Loss: 0.006\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    857.\n",
      "  Batch   100  of    857.\n",
      "  Batch   150  of    857.\n",
      "  Batch   200  of    857.\n",
      "  Batch   250  of    857.\n",
      "  Batch   300  of    857.\n",
      "  Batch   350  of    857.\n",
      "  Batch   400  of    857.\n",
      "  Batch   450  of    857.\n",
      "  Batch   500  of    857.\n",
      "  Batch   550  of    857.\n",
      "  Batch   600  of    857.\n",
      "  Batch   650  of    857.\n",
      "  Batch   700  of    857.\n",
      "  Batch   750  of    857.\n",
      "  Batch   800  of    857.\n",
      "  Batch   850  of    857.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    108.\n",
      "  Batch   100  of    108.\n",
      "\n",
      "Training Loss: 0.001\n",
      "Validation Loss: 0.006\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    857.\n",
      "  Batch   100  of    857.\n",
      "  Batch   150  of    857.\n",
      "  Batch   200  of    857.\n",
      "  Batch   250  of    857.\n",
      "  Batch   300  of    857.\n",
      "  Batch   350  of    857.\n",
      "  Batch   400  of    857.\n",
      "  Batch   450  of    857.\n",
      "  Batch   500  of    857.\n",
      "  Batch   550  of    857.\n",
      "  Batch   600  of    857.\n",
      "  Batch   650  of    857.\n",
      "  Batch   700  of    857.\n",
      "  Batch   750  of    857.\n",
      "  Batch   800  of    857.\n",
      "  Batch   850  of    857.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    108.\n",
      "  Batch   100  of    108.\n",
      "\n",
      "Training Loss: 0.001\n",
      "Validation Loss: 0.006\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of    857.\n",
      "  Batch   100  of    857.\n",
      "  Batch   150  of    857.\n",
      "  Batch   200  of    857.\n",
      "  Batch   250  of    857.\n",
      "  Batch   300  of    857.\n",
      "  Batch   350  of    857.\n",
      "  Batch   400  of    857.\n",
      "  Batch   450  of    857.\n",
      "  Batch   500  of    857.\n",
      "  Batch   550  of    857.\n",
      "  Batch   600  of    857.\n",
      "  Batch   650  of    857.\n",
      "  Batch   700  of    857.\n",
      "  Batch   750  of    857.\n",
      "  Batch   800  of    857.\n",
      "  Batch   850  of    857.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    108.\n",
      "  Batch   100  of    108.\n",
      "\n",
      "Training Loss: 0.001\n",
      "Validation Loss: 0.006\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of    857.\n",
      "  Batch   100  of    857.\n",
      "  Batch   150  of    857.\n",
      "  Batch   200  of    857.\n",
      "  Batch   250  of    857.\n",
      "  Batch   300  of    857.\n",
      "  Batch   350  of    857.\n",
      "  Batch   400  of    857.\n",
      "  Batch   450  of    857.\n",
      "  Batch   500  of    857.\n",
      "  Batch   550  of    857.\n",
      "  Batch   600  of    857.\n",
      "  Batch   650  of    857.\n",
      "  Batch   700  of    857.\n",
      "  Batch   750  of    857.\n",
      "  Batch   800  of    857.\n",
      "  Batch   850  of    857.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    108.\n",
      "  Batch   100  of    108.\n",
      "\n",
      "Training Loss: 0.001\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of    857.\n",
      "  Batch   100  of    857.\n",
      "  Batch   150  of    857.\n",
      "  Batch   200  of    857.\n",
      "  Batch   250  of    857.\n",
      "  Batch   300  of    857.\n",
      "  Batch   350  of    857.\n",
      "  Batch   400  of    857.\n",
      "  Batch   450  of    857.\n",
      "  Batch   500  of    857.\n",
      "  Batch   550  of    857.\n",
      "  Batch   600  of    857.\n",
      "  Batch   650  of    857.\n",
      "  Batch   700  of    857.\n",
      "  Batch   750  of    857.\n",
      "  Batch   800  of    857.\n",
      "  Batch   850  of    857.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    108.\n",
      "  Batch   100  of    108.\n",
      "\n",
      "Training Loss: 0.001\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of    857.\n",
      "  Batch   100  of    857.\n",
      "  Batch   150  of    857.\n",
      "  Batch   200  of    857.\n",
      "  Batch   250  of    857.\n",
      "  Batch   300  of    857.\n",
      "  Batch   350  of    857.\n",
      "  Batch   400  of    857.\n",
      "  Batch   450  of    857.\n",
      "  Batch   500  of    857.\n",
      "  Batch   550  of    857.\n",
      "  Batch   600  of    857.\n",
      "  Batch   650  of    857.\n",
      "  Batch   700  of    857.\n",
      "  Batch   750  of    857.\n",
      "  Batch   800  of    857.\n",
      "  Batch   850  of    857.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    108.\n",
      "  Batch   100  of    108.\n",
      "\n",
      "Training Loss: 0.001\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of    857.\n",
      "  Batch   100  of    857.\n",
      "  Batch   150  of    857.\n",
      "  Batch   200  of    857.\n",
      "  Batch   250  of    857.\n",
      "  Batch   300  of    857.\n",
      "  Batch   350  of    857.\n",
      "  Batch   400  of    857.\n",
      "  Batch   450  of    857.\n",
      "  Batch   500  of    857.\n",
      "  Batch   550  of    857.\n",
      "  Batch   600  of    857.\n",
      "  Batch   650  of    857.\n",
      "  Batch   700  of    857.\n",
      "  Batch   750  of    857.\n",
      "  Batch   800  of    857.\n",
      "  Batch   850  of    857.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    108.\n",
      "  Batch   100  of    108.\n",
      "\n",
      "Training Loss: 0.001\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of    857.\n",
      "  Batch   100  of    857.\n",
      "  Batch   150  of    857.\n",
      "  Batch   200  of    857.\n",
      "  Batch   250  of    857.\n",
      "  Batch   300  of    857.\n",
      "  Batch   350  of    857.\n",
      "  Batch   400  of    857.\n",
      "  Batch   450  of    857.\n",
      "  Batch   500  of    857.\n",
      "  Batch   550  of    857.\n",
      "  Batch   600  of    857.\n",
      "  Batch   650  of    857.\n",
      "  Batch   700  of    857.\n",
      "  Batch   750  of    857.\n",
      "  Batch   800  of    857.\n",
      "  Batch   850  of    857.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    108.\n",
      "  Batch   100  of    108.\n",
      "\n",
      "Training Loss: 0.001\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of    857.\n",
      "  Batch   100  of    857.\n",
      "  Batch   150  of    857.\n",
      "  Batch   200  of    857.\n",
      "  Batch   250  of    857.\n",
      "  Batch   300  of    857.\n",
      "  Batch   350  of    857.\n",
      "  Batch   400  of    857.\n",
      "  Batch   450  of    857.\n",
      "  Batch   500  of    857.\n",
      "  Batch   550  of    857.\n",
      "  Batch   600  of    857.\n",
      "  Batch   650  of    857.\n",
      "  Batch   700  of    857.\n",
      "  Batch   750  of    857.\n",
      "  Batch   800  of    857.\n",
      "  Batch   850  of    857.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    108.\n",
      "  Batch   100  of    108.\n",
      "\n",
      "Training Loss: 0.001\n",
      "Validation Loss: 0.005\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80ca429f-54be-40b7-a982-b6120c7a8c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ea01358-dc6a-4c59-9124-efd08bf4d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test amount\n",
    "\n",
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq[0:100].to(device), test_mask[0:100].to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a26147cf-03bb-485e-9fa6-4d03c06d8796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83        71\n",
      "           1       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.71       100\n",
      "   macro avg       0.35      0.50      0.42       100\n",
      "weighted avg       0.50      0.71      0.59       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y[0:100], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6a122b6-8c95-46ed-93ef-b9d4f7093073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[0:20])\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697dba65-c488-4137-bc1a-ca83aa11f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b6f29-b86d-42a2-aa41-c08b0d35fabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
