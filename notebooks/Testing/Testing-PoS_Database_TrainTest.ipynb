{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4972715-9f9c-4211-a549-78a985fe3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from netgraph import Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b9bf2e-d590-430b-88db-621b3fcc5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color checker\n",
    "colors = list(mcolors.CSS4_COLORS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2c3d0-34e3-4e66-9063-ebb378e25566",
   "metadata": {},
   "source": [
    "## Create a glossary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77ff15e-7660-473a-9baf-87dd589d9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "URL = 'https://en.wikipedia.org/wiki/Glossary_of_plant_morphology'\n",
    "# Get the page\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, \"lxml\", from_encoding=\"iso-8859-1\")   \n",
    "\n",
    "glossary = collections.defaultdict(list)\n",
    "# Find all H4 \n",
    "for chapter in soup.find_all('h4')[0:]:\n",
    "    # Clean\n",
    "    chapter_text = chapter.text.rstrip('[edit]')\n",
    "    # Find all siblings\n",
    "    for sibling in chapter.find_next_siblings():\n",
    "        # Find the parent\n",
    "        for parent in sibling.find_previous_sibling('h4'):\n",
    "            # Only append if correspond to current chapter\n",
    "            if parent.text == chapter_text:\n",
    "                if 'â' in sibling.text:\n",
    "                    for tag in sibling.find_all('li'):\n",
    "                        candidates = tag.text.split('â')[0]\n",
    "                        candidates = candidates.split('/')\n",
    "                        for candidate in candidates:\n",
    "                            glossary[chapter_text.lower()].append(candidate.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e79d43f1-466a-423e-af57-e92e3ba3df99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['morphology', 'roots', 'stems', 'buds', 'leaves', 'basic flower parts', 'inflorescences', 'insertion of floral parts', 'union of flower parts', 'flower sexuality and presence of floral parts', 'flower symmetry', 'terms for fruits', 'fruit types', 'pteridophytes', 'bryophytes'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42656c0-55ac-4b52-b703-985565efeb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glossary['fruit types'] += [\n",
    "#    'fruit',\n",
    "#]\n",
    "\n",
    "glossary['leaves'] += [\n",
    "    'glume',\n",
    "    'surface',\n",
    "    'margin'\n",
    "]\n",
    "\n",
    "glossary['basic flower parts'] +=[\n",
    "    'floret',\n",
    "    'awn',\n",
    "    \n",
    "]\n",
    "glossary['inflorescences'] += [\n",
    "    'spikelets',\n",
    "    'lemma',\n",
    "    'racemes',\n",
    "    'axis',\n",
    "]\n",
    "glossary['leaves'] += [\n",
    "    'rhachilla'\n",
    "]\n",
    "\n",
    "glossary['other'] += [\n",
    "    'apex',\n",
    "    'culm',\n",
    "    'tube',\n",
    "    'palea',\n",
    "    'crown',\n",
    "    'canopy',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9ceb5-bb96-427e-9a6d-5fcade9ef7bc",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba1afef-91f9-4b05-9af5-2467860f2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGRO_dirty = pickle.load(open('../data/description/descriptions_agroforestry_PLANTS.pkl', 'rb'))\n",
    "AGRO = collections.defaultdict(list)\n",
    "for species in AGRO_dirty.keys():\n",
    "    for (sentence, URL) in AGRO_dirty[species]:\n",
    "        AGRO[species.lower()].append(sentence)\n",
    "        \n",
    "POWO_dirty = pickle.load(open('../data/description/descriptions_powo_PLANTS.pkl', 'rb'))\n",
    "POWO = collections.defaultdict(list)\n",
    "for species in POWO_dirty.keys():\n",
    "    for (sentence, URL) in POWO_dirty[species]:\n",
    "        POWO[species.lower()].append(sentence)\n",
    "        \n",
    "LIFE_dirty = pickle.load(open('../data/description/descriptions_llifeV2_PLANTS.pkl', 'rb'))\n",
    "LIFE = collections.defaultdict(list)\n",
    "for species in LIFE_dirty.keys():\n",
    "    for (sentence, URL) in LIFE_dirty[species]:\n",
    "        names = species.split()\n",
    "        if len(names) >= 2:\n",
    "            new_name = f'{names[0]} {names[1]}'\n",
    "        else:\n",
    "            new_name = species\n",
    "        LIFE[new_name.lower()].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3c6aff-071e-4460-b25f-a54618818b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "POWO_species = [key for key in list(POWO.keys())]\n",
    "AGRO_species = [key for key in list(AGRO.keys())]\n",
    "LIFE_species = [key for key in list(LIFE.keys())]\n",
    "\n",
    "# Test set\n",
    "TEST_species = LIFE_species + AGRO_species\n",
    "# Common_species\n",
    "COMMON_species = list(set(POWO_species) & set(TEST_species))\n",
    "\n",
    "TEST = collections.defaultdict(list)\n",
    "for species in COMMON_species:\n",
    "    TEST[species] += AGRO[species]\n",
    "    TEST[species] += LIFE[species]\n",
    "    \n",
    "DATA = pickle.load(open('../data/description/04_TRAIN_0000000-0014557_PLANTS.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e130d125-8caf-447b-ba7b-bc215a6061da",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_id = collections.defaultdict(list)\n",
    "descriptions = collections.defaultdict(list)\n",
    "\n",
    "compounds = [\n",
    "    'fertile', 'sterile',\n",
    "    'male', 'female', 'bisexual',\n",
    "    'basal', 'developed', \n",
    "    'primary', 'secondary', 'main',\n",
    "    'upper', 'lower', 'greater', 'dorsal', 'alternate', 'lesser', 'apex', 'outer',\n",
    "    'central', 'outermost', 'outer', 'inner', 'uppermost', 'median', 'dorsal', 'central', 'lateral',\n",
    "]\n",
    "\n",
    "def compound_reconstructor(token, doc):\n",
    "    if token.i == 0:\n",
    "        trait = token\n",
    "    elif doc[token.i - 3].dep_ == 'compound':\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 3].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 2].dep_ == 'compound':\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 2].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 1].dep_ == 'compound':\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    elif doc[token.i - 1].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    else:\n",
    "        trait = token   \n",
    "    return trait.lemma_\n",
    "\n",
    "def check_existance(t, doc):\n",
    "    single = next((key for key, value in glossary.items() if t.lemma_.lower() in value), None)\n",
    "    multi = next((key for key, value in glossary.items() if t.text.lower() in value), None)\n",
    "    if single:\n",
    "        return single\n",
    "    elif multi:\n",
    "        return multi\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_advmod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'advmod':\n",
    "            return child\n",
    "        \n",
    "def extract_nummod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'nummod':\n",
    "            return child\n",
    "\n",
    "def extract_conjunction(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    if t.dep_ == 'conj' and t.pos_ == 'ADJ':\n",
    "        return t \n",
    "\n",
    "def extract_amod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'amod':\n",
    "            return child\n",
    "        \n",
    "def extract_measurements(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    obj = None\n",
    "    measurements = ['wide', 'long']\n",
    "    if t.text in measurements or t.lemma_ in measurements:\n",
    "        obj = doc[t.left_edge.i : t.right_edge.i + 1]\n",
    "    return obj\n",
    "\n",
    "def extract_prepositions(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'prep':\n",
    "            return doc[child.left_edge.i : child.right_edge.i + 1]\n",
    "\n",
    "def define_position(x, y, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    if len(x.text.split()) > 1:\n",
    "        return f'{y.text} {x.text}'\n",
    "    else:\n",
    "        try:\n",
    "            if x.i > y.i:\n",
    "                return doc[y.i : x.i + 1]\n",
    "            else:\n",
    "                return doc[x.i : y.i + 1]\n",
    "        except:\n",
    "            return f'{y.text} {x.text}'\n",
    "\n",
    "def extract_noun_verbs(t, doc):\n",
    "    relations = []\n",
    "    objects = []\n",
    "    if t.dep_ not in ['ROOT', 'nsubj', 'nsubjpass', 'csubj', 'csubjpass']:\n",
    "        return '', ''\n",
    "    parent = next((parent for parent in t.ancestors), None)\n",
    "    #print(t, parent)\n",
    "    if parent and parent.pos_ == 'VERB':\n",
    "        prep = extract_verb_preps(parent, doc)\n",
    "        dobj = extract_verb_dobj(parent, doc)\n",
    "        oprd = extract_verb_orpd(parent, doc)\n",
    "        agnt = extract_verb_agnt(parent, doc)\n",
    "        nmod = extract_verb_nmod(parent, doc)\n",
    "        if prep:\n",
    "            obj = extract_verb_pobj(prep, doc)\n",
    "            if obj:\n",
    "                relations.append(f'{parent.text} {prep}')\n",
    "                objects.append(obj.lemma_)\n",
    "        if dobj:\n",
    "            pass\n",
    "            #print(dobj)\n",
    "        if oprd:\n",
    "            pass\n",
    "            #print(oprd)\n",
    "        if agnt:\n",
    "            pass\n",
    "           # print(agnt)\n",
    "        if nmod:\n",
    "            pass\n",
    "           # print(nmod)\n",
    "        grandparent = next((grandparent for grandparent in parent.ancestors), None)\n",
    "        if not grandparent:\n",
    "            relations.append('is')\n",
    "            numbers = extract_nounandverb_nummods(parent, doc)\n",
    "            if numbers:\n",
    "                objects.append(f'{numbers.text} {parent.text}')\n",
    "            else:\n",
    "                objects.append(parent.text)\n",
    "    if not parent:\n",
    "        for child in t.children:\n",
    "            if child.pos_ == 'VERB' and child.dep_ != 'amod':\n",
    "                                \n",
    "                prep = extract_verb_preps(child, doc)\n",
    "                dobj = extract_verb_dobj(child, doc)\n",
    "                oprd = extract_verb_orpd(child, doc)\n",
    "                agnt = extract_verb_agnt(child, doc)\n",
    "                nmod = extract_verb_nmod(child, doc)\n",
    "                if prep:\n",
    "                    noun = extract_verb_pobj(prep, doc)\n",
    "                    if noun:\n",
    "                        relations.append(f'{child.text} {prep}')\n",
    "                    \n",
    "                        objects.append(doc[noun.left_edge.i : noun.right_edge.i + 1].text)\n",
    "                if dobj:\n",
    "                    relations.append(child.text)\n",
    "                    objects.append(doc[dobj.left_edge.i : dobj.right_edge.i + 1].text)     \n",
    "                if oprd:\n",
    "                    oprd_prep = extract_verb_preps(oprd, doc)\n",
    "                    if oprd_prep:\n",
    "                        relations.append(f'{child.text} {oprd.text}')\n",
    "                        objects.append(doc[oprd_prep.left_edge.i : oprd_prep.right_edge.i + 1].text)  \n",
    "                if agnt:\n",
    "                    continue\n",
    "                    #print(agnt)\n",
    "                if nmod:\n",
    "                    relations.append('is')\n",
    "                    objects.append(f'{nmod.text} {child}') \n",
    "\n",
    "    rel = []\n",
    "    obj = []\n",
    "    for relation, object_ in zip(relations, objects):\n",
    "        rel.append(relation)\n",
    "        obj.append(object_.split(',')[0])\n",
    "            \n",
    "    return rel, obj\n",
    "\n",
    "def extract_verb_nmod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'nummod':\n",
    "            return doc[child.left_edge.i : child.right_edge.i + 1] \n",
    "        \n",
    "def extract_verb_preps(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'prep':\n",
    "            return child    \n",
    "\n",
    "def extract_verb_pobj(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'pobj' or child.dep_ == 'pcomp' or child.dep_ == 'prep':\n",
    "            return child\n",
    "        \n",
    "def extract_verb_dobj(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'dobj':\n",
    "            return child\n",
    "        \n",
    "def extract_verb_orpd(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'oprd':\n",
    "            return child    \n",
    "        \n",
    "def extract_verb_agnt(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'agent':\n",
    "            return child \n",
    "        \n",
    "def extract_nounandverb_nummods(t, doc):\n",
    "    obj = None\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'nummod':\n",
    "            obj = doc[child.left_edge.i : child.right_edge.i + 1]\n",
    "            return obj   \n",
    "\n",
    "def extract_dnummod(t, doc):\n",
    "    obj = extract_nounandverb_nummods(t, doc)\n",
    "    if obj:\n",
    "        return obj.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_noun_adjectives(t, doc):\n",
    "    adjs = []\n",
    "    adjectives = []\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'appos':\n",
    "            continue\n",
    "        if child.pos_ =='ADJ' or child.tag_ == 'VBN' and child.dep_ in ['conj', 'amod']:\n",
    "            \n",
    "            \n",
    "            advmod = extract_advmod(child, doc)\n",
    "            measurement = extract_measurements(child, doc)\n",
    "            prep = extract_prepositions(child, doc)\n",
    "            nummod = extract_nummod(child, doc)\n",
    "            amod = extract_amod(child, doc)\n",
    "           \n",
    "            \n",
    "            if child.lemma_.lower() in compounds:\n",
    "                continue\n",
    "            if child.text.lower() in compounds:\n",
    "                continue\n",
    "            elif amod:\n",
    "                obj = define_position(amod, child, doc)\n",
    "                adjs.append(obj)\n",
    "            elif advmod:\n",
    "                #obj = doc[advmod.i : child.i + 1]\n",
    "                obj = define_position(advmod, child, doc)\n",
    "                adjs.append(obj)\n",
    "            elif measurement:\n",
    "                obj = measurement\n",
    "                adjs.append(obj)\n",
    "            elif prep:\n",
    "                obj = define_position(prep, child, doc)\n",
    "                adjs.append(obj)\n",
    "            elif nummod:\n",
    "                obj = define_position(nummod, child, doc)\n",
    "                adjs.append(obj)                \n",
    "            else:\n",
    "                obj = child\n",
    "                adjs.append(obj)\n",
    "            for grandchild in child.subtree:\n",
    "                conj = extract_conjunction(grandchild, doc)\n",
    "                if conj:\n",
    "                    advmod = extract_advmod(conj, doc)\n",
    "                    prep = extract_prepositions(conj, doc)\n",
    "                    nummod = extract_nummod(conj, doc)\n",
    "                    if advmod:\n",
    "                        obj = define_position(advmod, conj, doc)\n",
    "                        adjs.append(obj)\n",
    "                    elif prep:\n",
    "                        obj = define_position(prep, conj, doc)\n",
    "                        adjs.append(obj)\n",
    "                    elif nummod:\n",
    "                        obj = define_position(nummod, conj, doc)\n",
    "                        adjs.append(obj)\n",
    "                    else:\n",
    "                        obj = conj\n",
    "                        adjs.append(obj)            \n",
    "    for adj in adjs:\n",
    "        try:\n",
    "            if adj.pos_ == 'VERB':\n",
    "                adj_text = adj.text.lower()\n",
    "            elif adj.root.pos_ == 'VERB':\n",
    "                adj_text = adj.text.lower()\n",
    "            else:\n",
    "                adj_text = adj.lemma_.lower()\n",
    "        except:\n",
    "                if type(adj) == str:\n",
    "                    adj_text = adj.lower()\n",
    "                else:\n",
    "                    adj_text = adj.text.lower()\n",
    "        for adj_split in adj_text.split(','):\n",
    "            adjectives.append(adj_split.strip())\n",
    "    return adjectives\n",
    "\n",
    "def extract_noun_appos(t, doc):\n",
    "    appos = []\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'appos':\n",
    "            obj = doc[child.left_edge.i : child.right_edge.i + 1].text.lower()\n",
    "            for obj_split in obj.split(','):\n",
    "                appos.append(obj_split.strip())\n",
    "    return appos\n",
    "\n",
    "def check_species(t, species, doc):\n",
    "    if t.text in species.split():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def extract_auxillary(t, doc):\n",
    "     parent = next((parent for parent in t.ancestors if parent.pos_ == 'AUX'), None)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e86b1640-58b5-423a-820b-ac2b5576a1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01689f8d4c754ca996f47764f0cb2aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/556 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions = collections.defaultdict(list)\n",
    "\n",
    "# For plotting purposes\n",
    "parts = []\n",
    "traits = []\n",
    "#for species in tqdm_notebook(list(DATA.keys())[33:34]):\n",
    "#    for idx, text in enumerate(DATA[species][9:10]):\n",
    "\n",
    "\n",
    "for species in tqdm_notebook(COMMON_species[0:]):\n",
    "#for species in tqdm_notebook(list(descriptions_text.keys())):\n",
    "    for idx, text in enumerate(TEST[species][0:]):\n",
    "        # Clean the text\n",
    "        text = re.sub(r'(?<!\\d)\\.(?!\\d)', ' ', text)\n",
    "        text = re.sub(r'\\s×\\s', ' times ', text)\n",
    "        text = f'{text.strip()}.'\n",
    "        # Reset variables\n",
    "        part=trait=rel=obj=adjectives = None \n",
    "        # NLP\n",
    "        doc = nlp(text)\n",
    "        # Init\n",
    "        descriptions[species, idx] = []\n",
    "        triples = []\n",
    "        # Loop over tokens\n",
    "        for t in doc:\n",
    "            if t.dep_ == 'compound':\n",
    "                continue\n",
    "            ### SUBJECTS ###    \n",
    "            if t.pos_ == 'NOUN' or t.pos_ == 'PROPN':\n",
    "                # Check existance of parts\n",
    "                part = check_existance(t, doc)\n",
    "                if part:\n",
    "                    # Reconstruct Compounds & Append\n",
    "                    trait = compound_reconstructor(t, doc)\n",
    "                    triples.append(('species', 'has main part', part))\n",
    "                    triples.append((part, 'has sub part', trait))\n",
    "\n",
    "                    \n",
    "                    # Plotting\n",
    "                    #parts.append(part)\n",
    "                    #traits.append(trait)\n",
    "                    # NOUN ADJECTIVES\n",
    "                    adjectives = extract_noun_adjectives(t, doc)\n",
    "                    for adjective in adjectives:\n",
    "                        triples.append((trait, 'is', adjective))\n",
    "                    # NOUN VERBS\n",
    "                    verbs_rel, verbs_obj = extract_noun_verbs(t, doc)\n",
    "                    for rel, obj in zip(verbs_rel, verbs_obj):\n",
    "                        triples.append((trait, rel, obj))\n",
    "                    # NOUN APPOSITIONAL MODIFIER\n",
    "                    adjectives = extract_noun_appos(t, doc)\n",
    "                    for adjective in adjectives:\n",
    "                        triples.append((trait, 'is', adjective))\n",
    "                    # NOUN NUMMODS\n",
    "                    nummod = extract_dnummod(t, doc)\n",
    "                    triples.append((trait, 'is', nummod))\n",
    "                \n",
    "            #if check_species(t, species, doc):\n",
    "\n",
    "        # APPEND\n",
    "        descriptions[species, idx] = [triple for triple in triples if all(triple)]     \n",
    "        \n",
    "                    \n",
    "        #print(idx, doc)\n",
    "        #print(descriptions[species, idx])\n",
    "        #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2b878e-d4a4-4051-b614-81f606245a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_text = collections.defaultdict(list)\n",
    "descriptions_RDFs = collections.defaultdict(list)\n",
    "\n",
    "for (species, idx) in descriptions.keys():\n",
    "    for (sub, rel, obj) in descriptions[(species, idx)]:\n",
    "        text = f'{sub} {rel} {obj}.'.capitalize()\n",
    "        # Make sure order is the same\n",
    "        descriptions_text[species].append(text)\n",
    "        descriptions_RDFs[species].append((sub, rel, obj))\n",
    "\n",
    "#for species in descriptions_text.keys():\n",
    "#    descriptions_text[species] = list(set(descriptions_text[species]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcc65c75-5012-43a5-98bb-1bd20216b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/RDF_TEXT_TEST_V2_SET_554.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_text, f)      \n",
    "    \n",
    "with open('../data/processed/RDF_TRIPLES_TEST_V2_SET_554.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_RDFs, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efbac31-b1fb-40f6-8461-39cce8d5fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(descriptions_text.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c32c7-03de-4fbe-a601-ff61b5ff6189",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c606cf-e439-4964-9920-764337a46891",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF_dict_text = collections.defaultdict(list)\n",
    "\n",
    "source   = []\n",
    "relation = []\n",
    "target   = []\n",
    "\n",
    "for data_list in list(descriptions.values())[0:]:\n",
    "    for (sub, rel, obj) in data_list:\n",
    "\n",
    "        source.append(sub)\n",
    "        relation.append(rel)\n",
    "        target.append(obj)\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e2717-4d62-4d17-bf81-f0fb2c742ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseparts = list(glossary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c8984-07a5-4ae8-83cc-5be24369a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [(source, target) for source, target in zip(kg_df['source'].values, kg_df['target'].values)]\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.Graph())\n",
    "\n",
    "\n",
    "node_labels = {node : node for idx, node in enumerate(G)}\n",
    "edge_labels = dict(zip(list(zip(kg_df.source, kg_df.target)),\n",
    "                  kg_df['edge'].tolist()))\n",
    "\n",
    "node_size = {}\n",
    "node_color = {}\n",
    "\n",
    "size = 1.5\n",
    "\n",
    "for node in node_labels:\n",
    "    if node in list(DATA.keys()):\n",
    "        node_size[node] = 3.5/size\n",
    "        node_color[node] = 'darkgreen'\n",
    "    elif node in parts:\n",
    "        node_size[node] = 2./size\n",
    "        node_color[node] = 'green'\n",
    "    elif node in traits:\n",
    "        node_size[node] = 1.5/size\n",
    "        node_color[node] = 'lightgreen'\n",
    "    else:\n",
    "        node_size[node] = 1./size\n",
    "        node_color[node] = 'white'\n",
    "        \n",
    "\n",
    "fixed_nodes = {\n",
    "    'Dichanthelium superatum': np.array([-0.00, -0.00]),\n",
    "    'fruit types': coordinates(0),\n",
    "    'stems': coordinates(51),\n",
    "    'leaves': coordinates(102),\n",
    "\n",
    "    'inflorescences': coordinates(153),\n",
    "    'other': coordinates(204),\n",
    "    'terms for fruits': coordinates(255),\n",
    "    'basic flower parts': coordinates(306),\n",
    "\n",
    "}        \n",
    "        \n",
    "\n",
    "#pos = nx.spring_layout(G, pos=fixed_nodes, fixed=fixed_nodes, k = 0.023, iterations=100, seed=3, scale=0.4, center=(0,0), dim=2)\n",
    "pos = nx.spring_layout(G, k = 0.023, iterations=5000, seed=33, dim=2, scale=1, center=(10,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d91093-f71c-4900-b00b-4461e301befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 25))\n",
    "Graph(nodes, \n",
    "      #node_layout='spring', edge_layout='curved', \n",
    "      node_layout=pos, edge_layout='straight', \n",
    "      arrows=True, node_zorder=3, #edge_zorder=1,\n",
    "      node_labels=node_labels, \n",
    "      node_label_offset=0.04, \n",
    "      #edge_labels=edge_labels,\n",
    "      #node_label_offset=(.00, .04), #node_label_offset=(0.00, -0.035),  \n",
    "      node_label_fontdict=dict(size=18, rotation=0, ha='center', clip_on=False), node_edge_width=0.2,\n",
    "      node_size=node_size,  node_color=node_color, #edge_labels=edge_labels,\n",
    "      edge_width=0.2, edge_label_fontdict=dict(size=5,),\n",
    "      #node_layout_kwargs=dict(total_iterations=10000, seed=33, k = 0.000005, ),\n",
    "      #node_layout_kwargs=dict(node_size=1, total_iterations=20),\n",
    "      ax=ax)\n",
    "#plt.savefig('kngraph.pdf', format='pdf', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13585996-417b-4480-b7e1-bf6335e4d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 25))\n",
    "Graph(nodes, \n",
    "      node_layout='dot', edge_layout='straight', \n",
    "      arrows=True, node_zorder=3, #edge_zorder=1,\n",
    "      node_labels=node_labels, \n",
    "      edge_labels=edge_labels,\n",
    "      node_label_offset=(0.00, -0.04),  \n",
    "      node_label_fontdict=dict(size=18, rotation=30, ha='right', clip_on=False), node_edge_width=0.2,\n",
    "      node_size=node_size,  node_color=node_color, #edge_labels=edge_labels,\n",
    "      edge_width=0.3, edge_label_fontdict=dict(size=5,),\n",
    "      #node_layout_kwargs=dict(total_iterations=10000, seed=33, k = 0.000005, ),\n",
    "      #node_layout_kwargs=dict(node_size=1, total_iterations=20),\n",
    "      ax=ax)\n",
    "#plt.savefig('kngraph.pdf', format='pdf', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c8424-af1a-41a5-8944-724ca5d803d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e464ec4-1b51-47a5-98f3-d28320f67afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a3cc5-4ebf-4f42-9b54-7a7570507e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates(angle):\n",
    "    x = 0.14 * math.sin(math.pi * 2 * angle / 360)\n",
    "    y = 0.14 * math.cos(math.pi * 2 * angle / 360)\n",
    "    return np.array([x, y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05805c33-73d5-4782-b32a-906a6380c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF_dict_text = collections.defaultdict(list)\n",
    "\n",
    "source   = []\n",
    "relation = []\n",
    "target   = []\n",
    "\n",
    "for data_list in list(descriptions.values())[0:]:\n",
    "    for (sub, rel, obj) in data_list:\n",
    "        \n",
    "        if rel == 'has main part':\n",
    "        \n",
    "            source.append(sub)\n",
    "            relation.append(rel)\n",
    "            target.append(obj)\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relation})\n",
    "\n",
    "\n",
    "nodes = [(source, target) for source, target in zip(kg_df['source'].values, kg_df['target'].values)]\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.Graph())\n",
    "\n",
    "\n",
    "node_labels = {node : node for idx, node in enumerate(G)}\n",
    "edge_labels = dict(zip(list(zip(kg_df.source, kg_df.target)),\n",
    "                  kg_df['edge'].tolist()))\n",
    "\n",
    "node_size = {}\n",
    "node_color = {}\n",
    "\n",
    "size = 1.5\n",
    "\n",
    "for node in node_labels:\n",
    "    if node in list(DATA.keys()):\n",
    "        node_size[node] = 3.5/size\n",
    "        node_color[node] = 'darkgreen'\n",
    "    elif node in parts:\n",
    "        node_size[node] = 2./size\n",
    "        node_color[node] = 'green'\n",
    "    elif node in traits:\n",
    "        node_size[node] = 1.5/size\n",
    "        node_color[node] = 'lightgreen'\n",
    "    else:\n",
    "        node_size[node] = 1./size\n",
    "        node_color[node] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706fd7d-c102-4e09-ac7f-0ff904e34f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_nodes = {\n",
    "    'Dichanthelium superatum': np.array([-0.00, -0.00]),\n",
    "    'stems': coordinates(90),\n",
    "    'leaves': coordinates(135),\n",
    "    'inflorescences': coordinates(180),\n",
    "    'other': coordinates(235),\n",
    "    'basic flower parts': coordinates(270),\n",
    "\n",
    "}    \n",
    "\n",
    "pos = nx.spring_layout(G, pos=fixed_nodes, fixed=fixed_nodes, k = 0.023, iterations=100, seed=3, scale=0.4, center=(0,0), dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94316fd7-e1bf-419c-9268-989d29233690",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 25))\n",
    "Graph(nodes, \n",
    "      #node_layout='spring', edge_layout='curved', \n",
    "      node_layout=pos, edge_layout='straight', \n",
    "      arrows=True, node_zorder=3, #edge_zorder=1,\n",
    "      node_labels=node_labels, \n",
    "      node_label_offset=0.04, \n",
    "      #edge_labels=edge_labels,\n",
    "      #node_label_offset=(.00, .04), #node_label_offset=(0.00, -0.035),  \n",
    "      node_label_fontdict=dict(size=18, rotation=0, ha='center', clip_on=False), node_edge_width=0.2,\n",
    "      node_size=node_size,  node_color=node_color, #edge_labels=edge_labels,\n",
    "      edge_width=0.2, edge_label_fontdict=dict(size=5,),\n",
    "      #node_layout_kwargs=dict(total_iterations=10000, seed=33, k = 0.000005, ),\n",
    "      #node_layout_kwargs=dict(node_size=1, total_iterations=20),\n",
    "      ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf4b8f-8f9b-4877-a7f0-93edac593a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d35dbe-973a-4350-badb-afdc936b2e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
