{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3fe360-4ffd-45c4-81ef-cb8150ccb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.util import filter_spans\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.en import English\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f0951d-e275-49a5-83a4-62cc508b7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('../../data/description/04_TRAIN_0-472_BIRDS.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "befebe4a-0ed1-4d59-b0f3-9203eed54cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent for sent in data['Common Tern']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c218f5-376e-4a45-9648-806408972557",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = [span for span_list in sents for span in span_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6794f3-e528-4266-9cdd-dec75cb49008",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ['the Brown bears have 4 long claws and 2 short ears.',\n",
    "           'The Brown bear has a brown fur.',\n",
    "            'The Brown bear has a black nose.',\n",
    "            'The Brown bear has a large nose.',\n",
    "            'The Brown bear has a bold belly.',\n",
    "            'Their feet are orange.',\n",
    "            'The Brown bear has similar feet as the black bear.',\n",
    "            'The claws have 5 sharp nails and are 8.5 cm long.',\n",
    "            'The belly is bold and grey',\n",
    "            'The nose is black and shiny',\n",
    "            'It has dark ears',\n",
    "            'legs muscular with curved nails'\n",
    "          ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483437d-4c67-44f5-b311-f403794ddc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(strings[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566892f-c920-4c39-807f-7a333a21b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f0309d-22ed-42fc-85ed-f5b0e0f314f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def real_subject_object(doc):\n",
    "    \n",
    "    \n",
    "    triples = []\n",
    "    empty = nlp(' ')[0]\n",
    "     # Find Root\n",
    "    root = next(token for token in doc if token.dep_ == 'ROOT')\n",
    "    if root:\n",
    "        # Find normal subject + compound if exists\n",
    "        subject_base = next((child for child in root.children if child.dep_ == 'nsubj'), empty)\n",
    "        subject_comp = next((child for child in subject_base.children if child.dep_ == 'compound' \n",
    "                                                                      if child.pos_ == 'PROPN'), empty)\n",
    "\n",
    "        # Lemmatize and lower\n",
    "        subject_base_string = subject_base.lemma_.lower()\n",
    "        subject_comp_string = subject_comp.lemma_.lower()\n",
    "        # Concat\n",
    "        subject_string = f'{subject_comp_string} {subject_base_string}'\n",
    "\n",
    "        # Find direct object\n",
    "        object_ = next((child for child in root.children if child.dep_ == 'dobj' \n",
    "                                                         if child.pos_ == 'NOUN'), empty)\n",
    "        \n",
    "        # Lemmatize and lower\n",
    "        object_string = object_.lemma_.lower()   \n",
    "        # Lemmatize root\n",
    "        root_string = root.lemma_.lower()\n",
    "        \n",
    "        # Find conjectures related to object:\n",
    "        object_cc = [child for child in object_.children if child.dep_ == 'conj']\n",
    "        #print(object_cc)\n",
    "        if object_cc:\n",
    "            for conjectures in object_cc:\n",
    "                object_cc_string = conjectures.lemma_.lower()\n",
    "                triples.append((subject_string.strip(), root_string.strip(), object_cc_string.strip()))\n",
    "    \n",
    "        triples.append((subject_string.strip(), root_string.strip(), object_string.strip()))\n",
    "        \n",
    "        return triples\n",
    "    \n",
    "def real_subject_acomp(doc):\n",
    "    triples = []\n",
    "    empty = nlp(' ')[0]\n",
    "     # Find Root\n",
    "    root = next(token for token in doc if token.dep_ == 'ROOT')\n",
    "    if root:\n",
    "        # Find normal subject + compound if exists\n",
    "        subject_base = next((child for child in root.children if child.dep_ == 'nsubj'), empty)\n",
    "        subject_comp = next((child for child in subject_base.children if child.dep_ == 'compound' \n",
    "                                                                      if child.pos_ == 'PROPN'), empty)\n",
    "\n",
    "        # Lemmatize and lower\n",
    "        subject_base_string = subject_base.lemma_.lower()\n",
    "        subject_comp_string = subject_comp.lemma_.lower()\n",
    "        # Concat\n",
    "        subject_string = f'{subject_comp_string} {subject_base_string}'\n",
    "\n",
    "        # Find direct object\n",
    "        object_ = next((child for child in root.children if child.dep_ == 'acomp'), empty)\n",
    "        \n",
    "        # Lemmatize and lower\n",
    "        object_string = object_.lemma_.lower()   \n",
    "        # Lemmatize root\n",
    "        root_string = root.lemma_.lower()\n",
    "        \n",
    "        # Find conjectures related to object:\n",
    "        object_cc = [child for child in object_.children if child.dep_ == 'conj']\n",
    "        #print(object_cc)\n",
    "        if object_cc:\n",
    "            for conjectures in object_cc:\n",
    "                object_cc_string = conjectures.lemma_.lower()\n",
    "                triples.append((subject_string.strip(), root_string.strip(), object_cc_string.strip()))\n",
    "    \n",
    "        triples.append((subject_string.strip(), root_string.strip(), object_string.strip()))\n",
    "        \n",
    "        return triples    \n",
    "\n",
    "def direct_object_amod(doc):\n",
    "    \n",
    "    triples = []\n",
    "    empty = nlp(' ')[0]\n",
    "    # Find direct object\n",
    "    object_ = next((token for token in doc if token.dep_ == 'dobj' \n",
    "                                           if token.pos_ == 'NOUN'), empty)\n",
    "    # Find amod\n",
    "    amod = next((child for child in object_.children if child.dep_ == 'amod'), empty)\n",
    "    amod_string = amod.lemma_.lower()\n",
    "    \n",
    "    # Lemmatize and lower\n",
    "    object_ = object_.lemma_.lower()\n",
    "    \n",
    "    if amod.pos_ == 'NOUN':\n",
    "        triples.append((object_.strip(), 'have', amod_string))\n",
    "    else:\n",
    "        triples.append((object_.strip(), 'be', amod_string))\n",
    "        \n",
    "    return triples\n",
    "    \n",
    "def noun_numbers(doc):\n",
    "    \n",
    "    triples = []\n",
    "    empty = nlp(' ')[0]\n",
    "    # Find nouns\n",
    "    nouns = [token for token in doc if token.pos_ == 'NOUN']\n",
    "    #print(nouns)\n",
    "    if nouns:\n",
    "        for noun in nouns:\n",
    "            children = [child for child in noun.children if child.dep_ == 'nummod']\n",
    "            for child in children:\n",
    "                #print(child)\n",
    "                noun_string = noun.lemma_.strip()\n",
    "                triples.append((noun_string.lower(), 'number', child.lemma_))\n",
    "            \n",
    "    return triples\n",
    "\n",
    "def measurements(doc):\n",
    "    triples = []\n",
    "    empty = nlp(' ')[0]\n",
    "    # Find adjectives\n",
    "    adjectives = [token for token in doc if token.pos_ == 'ADJ']\n",
    "    #print(adjectives)\n",
    "    if adjectives:\n",
    "        for adjective in adjectives:\n",
    "            for child in adjective.children:\n",
    "                #print(child)\n",
    "                if child.dep_ == 'npadvmod':\n",
    "                    adjective_string = adjective.lemma_.lower()\n",
    "                    child_string = child.lemma_.strip()\n",
    "                    #print(child_string)\n",
    "                    triples.append((adjective_string.strip(), 'measurement', child_string.strip()))\n",
    "                    \n",
    "    return triples\n",
    "\n",
    "def possesions(doc):\n",
    "    triples = []\n",
    "    \n",
    "    nouns = [token for token in doc if token.pos_ == 'NOUN']\n",
    "    for noun in nouns:\n",
    "        for child in noun.children:\n",
    "            if child.pos_ == 'PRON':\n",
    "                noun_string = noun.lemma_.lower()\n",
    "                triples.append((species.lower(), 'have', noun_string.strip()))\n",
    "                \n",
    "    return triples\n",
    "\n",
    "def noun_root(doc):\n",
    "    \n",
    "    triples = []\n",
    "    \n",
    "    root = next(token for token in doc if token.dep_ == 'ROOT')\n",
    "    #print(root.pos_)\n",
    "    if root.pos_ not in ['VERB', 'AUX']:\n",
    "        # If root is noun append to species\n",
    "        if root.pos_ == 'NOUN':\n",
    "            root_string = root.lemma_.lower()\n",
    "            triples.append((species.lower(), 'have', root_string.strip()))\n",
    "            \n",
    "            # Extract ADJs\n",
    "            for child in root.children:\n",
    "                if child.pos_ == 'ADJ':\n",
    "                    child_string = child.lemma_.lower()\n",
    "                    triples.append((root_string.strip(), 'be', child_string.lower()))\n",
    "            \n",
    "    return triples\n",
    "\n",
    "def conjecture_noun(doc):\n",
    "    \n",
    "    triples = []\n",
    "    empty = nlp(' ')[0]\n",
    "    \n",
    "    root = next((token for token in doc if token.dep_ == 'conj'), empty)\n",
    "    if root.pos_ not in ['VERB', 'AUX']:\n",
    "        # If root is noun append to species\n",
    "        if root.pos_ == 'NOUN':\n",
    "            root_string = root.lemma_.lower()\n",
    "            triples.append((species.lower(), 'have', root_string.strip()))\n",
    "            \n",
    "            # Extract ADJs\n",
    "            for child in root.children:\n",
    "                if child.pos_ == 'ADJ':\n",
    "                    child_string = child.lemma_.lower()\n",
    "                    triples.append((root_string.strip(), 'be', child_string.lower()))\n",
    "            \n",
    "    return triples\n",
    "\n",
    "\n",
    "def noun_chunks(doc):\n",
    "    \n",
    "    triples = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe1524-9909-486f-86b2-2cbc8efefcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunks(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3254a5-870e-4046-9422-12808e040704",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sents[3])\n",
    "print(doc)\n",
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1298b-2991-4b02-802f-690607a2fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = []\n",
    "species = 'Pigeon Guillemot'\n",
    "\n",
    "for idx, sent in enumerate(tqdm(spans)):\n",
    "    doc = nlp(sent)\n",
    "    \n",
    "    # Neural coref (BASIC)\n",
    "    text = ' '.join([species if token.pos_ == 'PRON' else token.text for token in doc])\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    triples.append(real_subject_object(doc))\n",
    "    triples.append(real_subject_acomp(doc))\n",
    "    triples.append(direct_object_amod(doc))\n",
    "    triples.append(noun_numbers(doc))\n",
    "    triples.append(measurements(doc))\n",
    "    triples.append(possesions(doc))\n",
    "    triples.append(noun_root(doc))\n",
    "    triples.append(conjecture_noun(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c282a-ea19-4069-af84-516b639d66af",
   "metadata": {},
   "source": [
    "### VIZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7dd335-86fc-47be-bab3-3d712ca5b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = [RDF for RDF_list in triples for RDF in RDF_list]\n",
    "\n",
    "source   = []\n",
    "relation = []\n",
    "target   = []\n",
    "\n",
    "\n",
    "for triple in triples:\n",
    "    if not all(triple):\n",
    "        continue\n",
    "    source.append(triple[0])\n",
    "    relation.append(triple[1])\n",
    "    target.append(triple[2])\n",
    "    \n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc8f91-c302-46f2-a541-c8ae62822e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "trait_list = list(kg_df[kg_df.source == 'pigeon guillemot']['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf044f4b-cec7-4e0b-b380-abed00047cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([kg_df[kg_df.source == 'pigeon guillemot'], kg_df[kg_df['source'].isin(trait_list)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9b39e-0196-4958-ab76-c3fd5c5d4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_df = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191610a-b3c7-4aec-bd17-f41253214ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(df_test, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "labels = dict(zip(list(zip(kg_df.source, kg_df.target)),\n",
    "                  kg_df['edge'].tolist()))\n",
    "pos = nx.spring_layout(G, k = 1)\n",
    "nx.draw(G, with_labels=True, node_size=2500, pos=graphviz_layout(G))\n",
    "nx.draw_networkx_edge_labels(G, pos=graphviz_layout(G), edge_labels=labels,\n",
    "                                 font_color='red')\n",
    "#plt.show()\n",
    "plt.savefig('plot.pdf', format='pdf', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3461b-e03f-4574-881f-1f7bf7a73f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe00af-94fa-4aba-9dbf-7194673be9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0dc467-eb0a-40b9-84dc-beb161e15ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
