{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64101143-27e2-4a19-8803-728239392d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from netgraph import Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a8fd77-80d9-4fed-8296-b05529f3faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(mcolors.CSS4_COLORS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a014546-3861-4b52-8802-8c3acef885b4",
   "metadata": {},
   "source": [
    "### Get Plant Glossary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5f684b-b42b-4a93-bced-ab9b855ba1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "URL = 'https://en.wikipedia.org/wiki/Glossary_of_plant_morphology'\n",
    "# Get the page\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, \"lxml\", from_encoding=\"iso-8859-1\")   \n",
    "\n",
    "glossary = collections.defaultdict(list)\n",
    "# Find all H4 \n",
    "for chapter in soup.find_all('h4')[0:]:\n",
    "    # Clean\n",
    "    chapter_text = chapter.text.rstrip('[edit]')\n",
    "    # Find all siblings\n",
    "    for sibling in chapter.find_next_siblings():\n",
    "        # Find the parent\n",
    "        for parent in sibling.find_previous_sibling('h4'):\n",
    "            # Only append if correspond to current chapter\n",
    "            if parent.text == chapter_text:\n",
    "                if 'â' in sibling.text:\n",
    "                    for tag in sibling.find_all('li'):\n",
    "                        candidates = tag.text.split('â')[0]\n",
    "                        candidates = candidates.split('/')\n",
    "                        for candidate in candidates:\n",
    "                            glossary[chapter_text.lower()].append(candidate.strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d991ba-b788-4281-b464-d262b1a0fb1f",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7788b3-f551-45a3-a15a-fa1184f82f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary['leaves'] += [\n",
    "    'glume',\n",
    "    'surface',\n",
    "    'margin'\n",
    "]\n",
    "\n",
    "glossary['basic flower parts'] +=[\n",
    "    'floret'\n",
    "    \n",
    "]\n",
    "glossary['inflorescences'] += [\n",
    "    'spikelets',\n",
    "    'lemma',\n",
    "    'racemes',\n",
    "    'axis'\n",
    "]\n",
    "glossary['leaves'] += [\n",
    "    'rhachilla'\n",
    "]\n",
    "\n",
    "glossary['other'] += [\n",
    "    'apex'\n",
    "    'culms'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a5e4a-4b26-40c4-8519-1d6f71771737",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036540dd-c2e2-411f-bf3f-456ed01f0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unsorted = pickle.load(open('../data/description/04_TRAIN_0000000-0014557_PLANTS.pkl', 'rb'))\n",
    "\n",
    "data_sorted =  {k: v for k, v in sorted(data_unsorted.items(), key = lambda item : len(item[1]), reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1fbf3e-961d-467b-9269-faf4ef00cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[species]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ccd0e-5f07-45d7-bb57-ab4ac63eccc5",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7931c-5a85-429b-a6b8-abbe6daeed12",
   "metadata": {},
   "source": [
    "### Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99c5d76a-6804-4585-a869-bc41e074bb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c04b51036a47ab87ffd47661e4e90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions_id = collections.defaultdict(list)\n",
    "descriptions = collections.defaultdict(list)\n",
    "\n",
    "### PLOTTING ###\n",
    "parts = []\n",
    "### PLOTTING ###\n",
    "\n",
    "compounds = [\n",
    "    'upper', 'lower', 'apex', \n",
    "    'dorsal', 'central', 'lateral',\n",
    "    'fertile', 'sterile',\n",
    "    'male', 'female',\n",
    "]\n",
    "\n",
    "def compound_reconstructor(token, doc):\n",
    "    if token.i == 0:\n",
    "        trait = token\n",
    "    elif doc[token.i - 3].dep_ == 'compound':\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 3].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 2].dep_ == 'compound':\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 2].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 1].dep_ == 'compound':\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    elif doc[token.i - 1].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    else:\n",
    "        trait = token   \n",
    "    return trait.lemma_\n",
    "\n",
    "def check_existance(t, doc):\n",
    "    single = next((key for key, value in glossary.items() if t.lemma_.lower() in value), None)\n",
    "    multi = next((key for key, value in glossary.items() if t.text.lower() in value), None)\n",
    "    if single:\n",
    "        return single\n",
    "    elif multi:\n",
    "        return multi\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_advmod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'advmod':\n",
    "            return child\n",
    "\n",
    "def extract_conjunction(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    if t.dep_ == 'conj' and t.pos_ == 'ADJ':\n",
    "        return t \n",
    "    \n",
    "def extract_measurements(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    subj = None\n",
    "    measurements = ['wide', 'long']\n",
    "    if t.text in measurements or t.lemma_ in measurements:\n",
    "        subj = doc[t.left_edge.i : t.right_edge.i + 1]\n",
    "    return subj\n",
    "\n",
    "def extract_prepositions(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'prep':\n",
    "            return doc[child.left_edge.i : child.right_edge.i + 1]\n",
    "\n",
    "\n",
    "def define_position(x, y, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    try:\n",
    "        if x.i > y.i:\n",
    "            return doc[y.i : x.i + 1]\n",
    "        else:\n",
    "            return doc[x.i : y.i + 1]\n",
    "    except:\n",
    "        if x[-1].i > y.i:\n",
    "            return nlp(f'{y} {x}')\n",
    "        else:\n",
    "            return nlp(f'{x} {y}')\n",
    "\n",
    "def extract_adjectives(t, doc):\n",
    "    adjs = []\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'appos':\n",
    "            continue\n",
    "        if child.pos_ =='ADJ' or child.tag_ == 'VBN' and child.dep_ in ['conj', 'amod']:\n",
    "            \n",
    "            advmod = extract_advmod(child, doc)\n",
    "            measurement = extract_measurements(child, doc)\n",
    "            prep = extract_prepositions(child, doc)\n",
    "            \n",
    "            if child.lemma_.lower() in compounds:\n",
    "                continue\n",
    "            if child.text.lower() in compounds:\n",
    "                continue\n",
    "            elif advmod:\n",
    "                subj = doc[advmod.i : child.i + 1]\n",
    "                adjs.append(subj)\n",
    "            \n",
    "            elif measurement:\n",
    "                subj = measurement\n",
    "                adjs.append(subj)\n",
    "            \n",
    "            #print(prep)\n",
    "            elif prep:\n",
    "                subj = define_position(prep, child, doc)\n",
    "                adjs.append(subj)\n",
    "            else:\n",
    "                subj = child\n",
    "                adjs.append(subj)\n",
    "            for grandchild in child.subtree:\n",
    "                conj = extract_conjunction(grandchild, doc)\n",
    "                if conj:\n",
    "                    advmod = extract_advmod(conj, doc)\n",
    "                    prep = extract_prepositions(conj, doc)\n",
    "                    if advmod:\n",
    "                        subj = define_position(advmod, conj, doc)\n",
    "                        adjs.append(subj)\n",
    "                    \n",
    "                    elif prep:\n",
    "                        subj = define_position(prep, conj, doc)\n",
    "                        adjs.append(subj)\n",
    "                    else:\n",
    "                        subj = conj\n",
    "                        adjs.append(subj)\n",
    "    return adjs\n",
    "\n",
    "def extract_appos_mods(t, doc):\n",
    "    appos = []\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'appos':\n",
    "            subj = doc[child.left_edge.i : child.right_edge.i + 1]\n",
    "            appos.append(subj)\n",
    "    return appos\n",
    "\n",
    "\n",
    "def extract_verb_preps(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'prep':\n",
    "            return child    \n",
    "\n",
    "def extract_verb_subj(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'pobj':\n",
    "            return child\n",
    "        \n",
    "def extract_verb_dobj(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'dobj':\n",
    "            return child\n",
    "def extract_verb_orpd(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'oprd':\n",
    "            return child    \n",
    "        \n",
    "def extract_verb_agnt(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'agent':\n",
    "            return child \n",
    "        \n",
    "def extract_verbs(t, doc):\n",
    "    rel=sub=None\n",
    "    root = next((tok for tok in doc if tok.dep_ == 'ROOT' if tok.pos_ == 'VERB'), None)\n",
    "    if root:\n",
    "        prep = extract_verb_preps(root, doc)\n",
    "        if prep:\n",
    "            rel = root.text + ' ' + prep.text\n",
    "            sub = extract_verb_subj(prep, doc)\n",
    "        else:\n",
    "            rel = 'TBD'\n",
    "            sub = root\n",
    "    elif not root:\n",
    "        for child in t.children:\n",
    "            if child.tag_ == 'VBG':\n",
    "                prep = extract_verb_preps(child, doc)\n",
    "                dobj = extract_verb_dobj(child, doc)\n",
    "                oprd = extract_verb_orpd(child, doc)\n",
    "                agnt = extract_verb_agnt(child, doc)\n",
    "                if prep:\n",
    "                    rel = child.text + ' ' + prep.text\n",
    "                    sub = extract_verb_subj(prep, doc)\n",
    "                elif dobj:\n",
    "                    rel = child.text\n",
    "                    sub = dobj\n",
    "                elif oprd:\n",
    "                    rel = child.text\n",
    "                    sub = oprd\n",
    "                else:\n",
    "                    continue\n",
    "                    print(311111, doc)\n",
    "            elif child.tag_ == 'VBN':\n",
    "                prep = extract_verb_preps(child, doc)\n",
    "                agnt = extract_verb_agnt(child, doc)\n",
    "                if prep:\n",
    "                    rel = child.text + ' ' + prep.text\n",
    "                    sub = extract_verb_subj(prep, doc)\n",
    "                elif agnt:\n",
    "                    #print(agnt)\n",
    "                    rel = child.text + ' ' + agnt.text\n",
    "                    sub = extract_verb_subj(agnt, doc)                    \n",
    "            else:\n",
    "                #print(child)\n",
    "                #print(56465)\n",
    "                continue\n",
    "    \n",
    "    return rel, sub\n",
    "\n",
    "       \n",
    "def create_triple(species, obj, rel, subj):\n",
    "    if not rel and type(obj) == str and type(subj) == str:\n",
    "        descriptions[species].append(('species', 'has main parts', obj))\n",
    "        descriptions[species].append((obj, 'has sub parts', subj))\n",
    "    elif rel:\n",
    "        descriptions[species].append((obj, rel, subj))\n",
    "    else:\n",
    "        descriptions[species].append((obj, 'TBD', subj))\n",
    "        \n",
    "def create_idx_triples(idx, data, species):\n",
    "    descriptions_id[(species, idx)] = data[species]\n",
    "        \n",
    "        \n",
    "for species in tqdm_notebook(list(data_unsorted.keys())[0:1]):\n",
    "    for idx, text in enumerate(data_unsorted[species][0:1]):\n",
    "        # Clean the text\n",
    "        text = re.sub(r'(?<!\\d)\\.(?!\\d)', ' ', text)\n",
    "        text = f'{text.strip()}.'\n",
    "        descriptions = collections.defaultdict(list)\n",
    "\n",
    "        # Reset variables\n",
    "        part=trait=rel=subj=conjs=subj_adj = None \n",
    "\n",
    "        doc = nlp(text)\n",
    "        # Loop over tokens\n",
    "        for t in doc:\n",
    "            if t.dep_ == 'compound':\n",
    "                continue\n",
    "            ### SUBJECTS ###    \n",
    "            elif t.dep_ == 'nsubj' and t.pos_ == 'NOUN':\n",
    "                # Check existance of parts\n",
    "                part = check_existance(t, doc)\n",
    "                if part:\n",
    "                    # Reconstruct Compounds & Append\n",
    "                    trait = compound_reconstructor(t, doc)\n",
    "                    create_triple(species, part, None, trait)\n",
    "                    # Extract Trait adjectives\n",
    "                    trait_adjs = extract_adjectives(t, doc)\n",
    "                    for adj in trait_adjs:\n",
    "                        create_triple(species, trait, None, adj)\n",
    "                    # Extract VERBS \n",
    "                    rel, obj = extract_verbs(t, doc)\n",
    "                    create_triple(species, trait, rel, obj)\n",
    "                    # Subject Adjectives\n",
    "                    if obj:\n",
    "                        object_adjs = extract_adjectives(obj, doc)\n",
    "                        for adj in object_adjs: \n",
    "                            create_triple(species, obj, None, adj)\n",
    "\n",
    "\n",
    "            ### ROOTS ###\n",
    "            elif t.dep_ == 'ROOT' and t.pos_ == 'NOUN':\n",
    "                part = check_existance(t, doc)\n",
    "                if part:\n",
    "                    # Reconstruct Compounds & Append\n",
    "                    trait = compound_reconstructor(t, doc)\n",
    "                    create_triple(species, part, None, trait)\n",
    "                    # Extract Trait adjectives\n",
    "                    trait_adjs = extract_adjectives(t, doc)\n",
    "                    for adj in trait_adjs:\n",
    "                        create_triple(species, trait, None, adj)\n",
    "                    # Extract appositional modifier\n",
    "                    appos_l = extract_appos_mods(t, doc)\n",
    "                    for subj in appos_l:\n",
    "                        create_triple(species, trait, None, subj)\n",
    "                    # Exract VERBS\n",
    "                    rel, obj = extract_verbs(t, doc)\n",
    "                    create_triple(species, trait, rel, obj)\n",
    "                    # Subject Adjectives\n",
    "                    if obj:\n",
    "                        # Adjectives\n",
    "                        object_adjs = extract_adjectives(obj, doc)\n",
    "                        for adj in object_adjs: \n",
    "                            create_triple(species, obj, None, adj)\n",
    "                        # VERBS\n",
    "                        rel_v, obj_v = extract_verbs(obj, doc)\n",
    "                        create_triple(species, obj, rel_v, obj_v)\n",
    "                        \n",
    "        create_idx_triples(idx, descriptions, species)\n",
    "        \n",
    "        #print(doc)\n",
    "        #for tr in descriptions[species]:\n",
    "        #    print(tr)\n",
    "        #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30565e15-84d2-4e3f-9177-02f4412dc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptions_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15313ca4-b0f8-4e24-80a2-1410b656fe74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9080fb3404734dc991388478c29ff9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75668 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDF_dict = collections.defaultdict(list)\n",
    "RDF_dict_text = collections.defaultdict(list)\n",
    "\n",
    "def relations(t):\n",
    "    \n",
    "    distance = ['mm', 'cm', 'm',\n",
    "                'milimeter', 'centimeter', 'meter']\n",
    "    rel = 'is'\n",
    "    if t.pos_ == 'NOUN':\n",
    "        rel = 'has'\n",
    "    if t.lemma_ in distance:\n",
    "        rel = 'is'\n",
    "    return rel\n",
    "\n",
    "def subjects(t):\n",
    "    #print(t, t.pos_)\n",
    "    sub = t.lemma_\n",
    "    if t.pos_ in ['VERB', 'NOUN']:\n",
    "        sub = t.text\n",
    "    return sub\n",
    "\n",
    "\n",
    "for (species, idx) in tqdm_notebook(descriptions_id.keys()):\n",
    "    data = descriptions_id[(species, idx)]\n",
    "    for (obj, rel, sub) in data:\n",
    "        # Aleady clean\n",
    "        if type(obj) == str and type(rel) == str and type(sub) == str:\n",
    "            RDF_dict[species].append((obj, rel, sub))\n",
    "        # TBD\n",
    "        elif type(rel) == str and rel == 'TBD' and sub:\n",
    "            # Get length\n",
    "            length = len(sub.text.split())\n",
    "            # If singular\n",
    "            if length == 1:\n",
    "                # Catch spans\n",
    "                try:\n",
    "                    RDF_dict[species].append((obj, relations(sub), subjects(sub)))\n",
    "                except:\n",
    "                    RDF_dict[species].append((obj, relations(sub[0]), subjects(sub[0])))\n",
    "            else:\n",
    "                if ',' in sub.text:\n",
    "                    spans = sub.text.split(',')\n",
    "                    for span in spans:\n",
    "                        # Catch empty strings\n",
    "                        if span:\n",
    "                            doc = nlp(span.strip())\n",
    "                            RDF_dict[species].append((obj, relations(doc[0]), doc.text))\n",
    "                else:\n",
    "                    RDF_dict[species].append((obj, relations(sub[0]), sub.text))\n",
    "\n",
    "        elif type(rel) != str and rel:\n",
    "            length = len(sub.text.split())\n",
    "            # If singular\n",
    "            if length == 1:\n",
    "                # Catch spans\n",
    "                try:\n",
    "                    RDF_dict[species].append((obj, rel.text, subjects(sub)))\n",
    "                except:\n",
    "                    RDF_dict[species].append((obj, rel.text, subjects(sub[0])))\n",
    "            else:\n",
    "                print( length, obj, sub, rel)\n",
    "        elif not sub:\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                RDF_dict[species].append((obj.text, rel, subjects(sub)))\n",
    "            except:\n",
    "                RDF_dict[species].append((obj, rel, subjects(sub)))\n",
    "            #print(f'{sub} --- {rel} --- {obj}')\n",
    "\n",
    "        \n",
    "for species in RDF_dict.keys():\n",
    "    RDF_dict[species] = list(set(RDF_dict[species]))\n",
    "\n",
    "for species in RDF_dict.keys():\n",
    "    temp = []\n",
    "    for (subj, rel, obj) in RDF_dict[species]:\n",
    "\n",
    "        if type(subj) != str:\n",
    "            subj = subj.text\n",
    "        if type(rel) != str:\n",
    "            rel = rel.text\n",
    "        if type(obj) != str:\n",
    "            obj = obj.text\n",
    "        temp.append((subj, rel, obj))\n",
    "        RDF_dict_text[species].append(f'{subj} {rel} {obj}.'.capitalize())\n",
    "    RDF_dict[species] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53531aec-9bb6-42f3-a5fb-59aae982f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/RDF_text_plants_2000.pkl', 'wb') as f:\n",
    "    pickle.dump(RDF_dict_text, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31d29c-dfdf-43b2-a281-4c07e5099a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF_dict_text = collections.defaultdict(list)\n",
    "\n",
    "\n",
    "source   = []\n",
    "relation = []\n",
    "target   = []\n",
    "\n",
    "for species in list(RDF_dict.keys())[0:5]:\n",
    "    for (_1, _2, _3) in RDF_dict[species]:\n",
    "        \n",
    "        if type(_1) != str:\n",
    "            _1 = _1.text\n",
    "        if type(_2) != str:\n",
    "            _2 = _2.text\n",
    "        if type(_3) != str:\n",
    "            _3 = _4.text\n",
    "            \n",
    "            \n",
    "        source.append(_1)\n",
    "        relation.append(_2)\n",
    "        target.append(_3)\n",
    "    \n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cdb5bb-ea67-495b-9fed-d92e3081d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('../data/description/04_TRAIN_0000000-0014557_PLANTS.pkl', 'rb'))\n",
    "species = list(data.keys())\n",
    "baseparts = list(glossary.keys())\n",
    "\n",
    "nodes = [(source, target) for source, target in zip(kg_df['source'].values, kg_df['target'].values)]\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.Graph())\n",
    "\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2ee23-396a-49fd-9e3a-1b650173df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "Graph(nodes, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682413c9-458d-4e0d-a65f-02ec395a6aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b475c-153d-4a8d-956a-f868038d071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('../data/description/04_TRAIN_0000000-0014557_PLANTS.pkl', 'rb'))\n",
    "species = list(data.keys())\n",
    "baseparts = list(glossary.keys())\n",
    "\n",
    "nodes = [(source, target) for source, target in zip(kg_df['source'].values, kg_df['target'].values)]\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.Graph())\n",
    "node_labels = {node : node for idx, node in enumerate(G)}\n",
    "node_size = {}\n",
    "node_color = {}\n",
    "node_labels_large = {}\n",
    "\n",
    "edge_labels = dict(zip(list(zip(kg_df.source, kg_df.target)),\n",
    "                  kg_df['edge'].tolist()))\n",
    "\n",
    "for node in node_labels:\n",
    "    if node in species:\n",
    "        node_size[node] = 2.2\n",
    "        node_color[node] = 'blue'\n",
    "        node_labels_large[node] = node\n",
    "    elif node in baseparts:\n",
    "        node_size[node] = 2\n",
    "        node_color[node] = 'green'\n",
    "        node_labels_large[node] = node\n",
    "    elif node in parts:\n",
    "        node_size[node] = 1.5\n",
    "        node_color[node] = 'black'\n",
    "        node_labels_large[node] = ''\n",
    "    else:\n",
    "        node_size[node] = .8\n",
    "        node_color[node] = 'white'\n",
    "        node_labels_large[node] = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbff224-ad56-4f83-b3c1-8cb4a57bfe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "Graph(nodes, node_layout='spring', edge_layout='straight', arrows=True,\n",
    "      node_labels=node_labels, node_label_offset=(0.00, -0.035), node_size=node_size, \n",
    "      node_label_fontdict=dict(size=8, rotation=30, ha='right'), node_color=node_color,\n",
    "      edge_labels=edge_labels, edge_width=0.4, edge_label_fontdict=dict(size=7))\n",
    "\n",
    "#plt.savefig('plot.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a860fac-4aa6-4660-9e45-89a4dcf3364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d134f-d62e-458c-b83f-0867168de43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "# create random graph with hubs and leafs\n",
    "hubs = np.random.randint(5, 15, size=10)\n",
    "leafs = np.ones((np.sum(hubs)), dtype=int)\n",
    "degrees = np.concatenate([hubs, leafs])\n",
    "g = nx.configuration_model(degrees, create_using=nx.Graph)\n",
    "giant_component = next(nx.connected_components(g))\n",
    "h = g.subgraph(giant_component)\n",
    "\n",
    "# generate random labels\n",
    "def random_string(length):\n",
    "    # https://stackoverflow.com/a/2030081/2912349\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for ii in range(length))\n",
    "\n",
    "labels = {node : random_string(5) for node in h}\n",
    "\n",
    "# generate node sizes\n",
    "node_size = dict(g.degree)\n",
    "nx_node_size = np.array([100*node_size[node] for node in h])\n",
    "ng_node_size = {node : np.sqrt(size) for node, size in node_size.items()}\n",
    "\n",
    "# same positions for comparability\n",
    "node_layout = nx.spring_layout(h)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "\n",
    "\n",
    "Graph(h, node_layout=node_layout, node_label_offset=0.1, node_size=ng_node_size, ax=ax, arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4243fc2-ea5f-4176-a4f6-4fce437748bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
