{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d087e90e-8989-40ad-b026-f7296931150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import random\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/models/')\n",
    "sys.path.insert(0, '../../src/features/')\n",
    "import predict_model\n",
    "model = predict_model.loadBERT(\"../../models/\", 'saved_weights_inf_FIXED_boot.pt')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b25909-30b2-48b1-9c4a-49676222bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpanPredictor(span, pred_values=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "         \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        # Print the prediction values\n",
    "        if pred_values:\n",
    "            return span_class, exps[0]\n",
    "        else:\n",
    "            return span_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec891f-870f-4ea6-b619-f8a026443a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#google_results = pickle.load(open('google_test.pkl', 'rb'))\n",
    "#ddg_results = pickle.load(open('ddg_test.pkl', 'rb'))\n",
    "combi_results = pickle.load(open('combi_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9bdcb4-5843-4873-99a8-9ec75773dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dict\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "\n",
    "regexes = [\n",
    "    r' \\(\\d+.+?Close\\n\\t\\n\\)',\n",
    "    r'\\(.+\\)',\n",
    "    r'\\[.+\\]',\n",
    "    r'\\xa0'\n",
    "]\n",
    "\n",
    "for species, URL_list in tqdm(combi_results.items()):\n",
    "    URL_list = list(set(URL_list))\n",
    "    for URL in URL_list:\n",
    "        try:\n",
    "            #print(URL)\n",
    "            page = requests.get(URL, timeout=5)\n",
    "            # Skip PDF files for now\n",
    "            if page.headers['Content-Type'].startswith('application/pdf'):\n",
    "                continue\n",
    "            # Soup the result\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            # Skip Embedded PDF's\n",
    "            if 'pdf' in soup.title.text.lower():\n",
    "                continue\n",
    "\n",
    "            ## CLEANING\n",
    "            # Get the cleaned strings\n",
    "            #text_list = [text for text in soup.stripped_strings]\n",
    "            \n",
    "            text_list = [text.strip() for text in soup.get_text().split('\\n')\n",
    "                        if len(text) > 2]\n",
    "            # Init\n",
    "            text_list_cleaned = []\n",
    "            # Loop over current list and clean more\n",
    "            for idx, text in enumerate(text_list):\n",
    "                # Replace rubbish\n",
    "                text = text.replace('\\n', \"\")\n",
    "                text = text.replace('\\r', \"\")\n",
    "                text_list_cleaned.append(text)\n",
    "\n",
    "            # Create string\n",
    "            cleaned_text = '. '.join(text_list_cleaned)\n",
    "            cleaned_text = re.sub(r'\\.\\.', '. ', cleaned_text, flags=re.DOTALL)\n",
    "            ## SPACY\n",
    "            # nlp the text\n",
    "            doc = nlp(cleaned_text)\n",
    "            # Extract the sents\n",
    "            sentences = [i for i in doc.sents]\n",
    "\n",
    "            # Loop over the individual sentences\n",
    "            for sentence in sentences:                    \n",
    "                # Create string object\n",
    "                sentence_str = str(sentence)\n",
    "                # Loop over the regexes\n",
    "                for regex in regexes:\n",
    "                    # Clean\n",
    "                    sentence_str = re.sub(regex, '', sentence_str, flags=re.DOTALL)\n",
    "                # Skip single crap\n",
    "                if len(sentence_str[0:-1].split()) <= 5:\n",
    "                    continue\n",
    "                # Skip measurement crap\n",
    "                #if sentence_str[0].isdigit():\n",
    "                #    continue\n",
    "                doc = nlp(sentence_str)\n",
    "                # Get the bad tokens\n",
    "                non_eng = [token.is_oov for token in doc].count(True)\n",
    "                # Continue if more than 3\n",
    "                if  non_eng > 3:\n",
    "                    continue\n",
    "                # Continue if the ratio is bad\n",
    "                if non_eng != 0:\n",
    "                    if non_eng / len(doc) > .2:\n",
    "                        continue\n",
    "                        \n",
    "                # Remove UNICODE\n",
    "                #sentence_str = sentence_str.encode(\"ascii\", \"ignore\")\n",
    "                #sentence_str = sentence_str.decode()\n",
    "                # If description append\n",
    "                if SpanPredictor(sentence_str):\n",
    "                    data[species].append(sentence_str)\n",
    "                    print(sentence_str, URL)\n",
    "        except: \n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512fc6ed-f935-4f4c-985d-e988fb29d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178677bb-7a68-49a7-aeb2-0ab09d36be7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
