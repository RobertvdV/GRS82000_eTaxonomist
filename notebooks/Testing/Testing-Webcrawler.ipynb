{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b16434b-0b56-4d3e-aed0-f819fe949ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import urllib.parse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/models/')\n",
    "sys.path.insert(0, '../src/features/')\n",
    "#sys.path.insert(0, '../src/visualization/')\n",
    "import predict_model\n",
    "from build_features import text_cleaner\n",
    "#import visualize as vis\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e379adb4-9c9e-4653-b7b1-ba96e1af306e",
   "metadata": {},
   "source": [
    "## Load the models and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd21d8e7-545d-4c22-901b-dbd965875fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "model = predict_model.loadBERT(\"../models/\", 'saved_weights_inf_FIXED_boot_beta80.pt')\n",
    "sim_model = predict_model.load_simBERT()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6749d3-6465-46ca-887d-8a1cac1e5476",
   "metadata": {},
   "source": [
    "## Functions used in the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335bc5bc-270f-4eed-9394-066e8c58d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_Duck(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    Queries DuckDuckGo and returns a URL list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get results \n",
    "    page = requests.get('https://duckduckgo.com/html/?q={0}'.format(query), \n",
    "                        headers={'user-agent': 'Descriptor/0.0.1'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    results = soup.find_all('a', attrs={'class':'result__a'}, href=True)\n",
    "    # Init list\n",
    "    links = []\n",
    "    # Clean results\n",
    "    for link in results:\n",
    "        url = link['href']\n",
    "        o = urllib.parse.urlparse(url)\n",
    "        d = urllib.parse.parse_qs(o.query)\n",
    "        links.append(d['uddg'][0])\n",
    "    return links\n",
    "\n",
    "def search_Bing(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    Queries Bing and returns a URL list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get results\n",
    "    page = requests.get('https://www.bing.com/search?form=MOZLBR&pc=MOZI&q={0}'.format(query), \n",
    "                        headers={'user-agent': 'Descriptor/0.0.1'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Init list\n",
    "    links = [] \n",
    "    # Clean results\n",
    "    for i in soup.find_all('a', attrs={'h':re.compile('ID=SERP.+')}, href=True):\n",
    "        link = i['href']\n",
    "        if link.startswith('http') and 'microsoft' not in link and 'bing' not in link:\n",
    "            links.append(link)        \n",
    "    return links\n",
    "\n",
    "def SpanPredictor(span, pred_values=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "         \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(**inputs)\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        # Print the prediction values\n",
    "        if pred_values:\n",
    "            return span_class, exps[0]\n",
    "        else:\n",
    "            return span_class\n",
    "    \n",
    "def VisualizeDoc(text, per_sentence=False, save=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates and HTML file (can be rendered in a notebook) by using the SpaCy \n",
    "    Displacy.\n",
    "    \n",
    "    per_sentence : Returns the visualization per sentence instead of a whole doc.\n",
    "    save         : If True returns the html string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # nlp the text\n",
    "    doc = nlp(text)\n",
    "    # Extract the sents\n",
    "    sentences = [i for i in doc.sents]\n",
    "    # Init color map\n",
    "    cmap = cm.get_cmap('Spectral')\n",
    "    # Init color dict\n",
    "    colors = {}\n",
    "    # Init option dict\n",
    "    options = {\"ents\": [],\n",
    "               \"colors\": colors,\n",
    "               \"distance\": 75}\n",
    "    # Init matcher\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    # Loop over the sentences\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        \n",
    "        # Get the prediction values    \n",
    "        prediction = SpanPredictor(str(sentence), pred_values=True)[1][1].numpy().item()\n",
    "        \n",
    "        # String ID            \n",
    "        #text = '#{0} - {1:.2f}'.format(idx, prediction)\n",
    "        text = f'{prediction:.3f}'\n",
    "        # Add the patterns        \n",
    "        pattern = nlp(str(sentence))\n",
    "        matcher.add(text, None, pattern)\n",
    "\n",
    "        # Colorize the strings\n",
    "        if prediction > .5:\n",
    "            colors[text] = matplotlib.colors.rgb2hex(cmap(prediction))\n",
    "        else:\n",
    "            colors[text] = matplotlib.colors.rgb2hex(cmap(prediction)) #+ '90'\n",
    "        # Add the new ENTS to the doc\n",
    "        options[\"ents\"].append(text)\n",
    "\n",
    "    # Match the enitities in the doc\n",
    "    matches = matcher(doc)\n",
    "    # Reset the current ENTS\n",
    "    doc.ents = ()\n",
    "    # Loop over the matches\n",
    "    for match_id, start, end in matches:\n",
    "        # Add the sentencen as a ENT\n",
    "        span = Span(doc, start, end, label=match_id)\n",
    "        #doc.ents = filter_spans(doc.ents)\n",
    "        try:\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Set title\n",
    "    #doc.user_data[\"title\"] = \"Description Predictor\"\n",
    "    sentence_spans = list(doc.sents)\n",
    "    \n",
    "    if save and per_sentence:\n",
    "        html = displacy.render(sentence_spans, style='ent', options=options, page=True, jupyter=False, minify=False)\n",
    "        return html\n",
    "    elif save and not per_sentence:\n",
    "        html = displacy.render(doc, style='ent', options=options, page=True, jupyter=False, minify=False)\n",
    "        return html\n",
    "    elif not save and per_sentence:\n",
    "        displacy.render(sentence_spans, style='ent', options=options)\n",
    "    elif not save and not per_sentence:\n",
    "        displacy.render(doc, style='ent', options=options)\n",
    "        \n",
    "def colorize_prediction(sentence_list, tex=False):\n",
    "\n",
    "    # Get prediction values\n",
    "    sentence_pred = [SpanPredictor(sent, pred_values=True)[1][1].item() for sent in sentence_list]\n",
    "    # Get color map\n",
    "    sentence_cmap = matplotlib.cm.BuGn\n",
    "    # Resample to prevent dark green\n",
    "    \n",
    "    template = \"\"\"  <mark class=\"entity\" style=\"\n",
    "    background: {}; \n",
    "    padding: 0.4em 0.0em; \n",
    "    margin: 0.0em; \n",
    "    line-height: 2; \n",
    "    border-radius: 0.75em;\n",
    "    \">{}    \n",
    "    <span style=\"\n",
    "    font-size: 0.8em; \n",
    "    font-weight: bold; \n",
    "    line-height: 1; \n",
    "    border-radius: 0.75em;\n",
    "    text-align: justify;\n",
    "    text-align-last:center;\n",
    "    vertical-align: middle;\n",
    "    margin-left: 0rem\">\n",
    "    </span>\\n</mark>\"\"\"\n",
    "\n",
    "    colored_string = ''\n",
    "    \n",
    "    # Tex list\n",
    "    tex_colors = []\n",
    "    tex_text = []\n",
    "    HTML = 'HTML'\n",
    "    \n",
    "    # Map the values\n",
    "    normalized_and_mapped = matplotlib.cm.ScalarMappable(cmap=sentence_cmap).to_rgba(sentence_pred)\n",
    "    # Color overlay the values\n",
    "    for idx, (sentence, color, prediction) in enumerate(zip(sentence_list, normalized_and_mapped, sentence_pred)):\n",
    "        \n",
    "        sentence = f'{sentence} < {prediction:.3f} >'\n",
    "        color = matplotlib.colors.rgb2hex(color)\n",
    "        colored_string += template.format(color, sentence)\n",
    "        \n",
    "        ## TEX PART\n",
    "        if tex:\n",
    "            tex_colors.append(f'\\definecolor{{color{idx+1}}}{{{HTML}}}{{{color[1:]}}}')\n",
    "            tex_text.append(f'\\sethlcolor{{color{idx+1}}}\\hl{{{sentence}}}')\n",
    "            \n",
    "    if tex:\n",
    "        print('Copy paste this in the .tex file')\n",
    "        print('\\n'.join(tex_colors))\n",
    "        print('\\n'.join(tex_text))\n",
    "    \n",
    "    \n",
    "    #display(HTML(colored_string))\n",
    "    #output_path = Path(\"test.html\")\n",
    "    #output_path.open(\"w\", encoding=\"utf-8\").write(colored_string)\n",
    "    return colored_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9cc6a-91a1-4843-863a-23a3bb376e24",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Check which plant species have the most sentences and use these species for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99353279-6a85-4cd0-bdaf-4379797c1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load POWO\n",
    "#plants_list = pickle.load(open('../data/external/species_plants.pkl', 'rb'))\n",
    "\n",
    "# Load keys\n",
    "plants_dict = pickle.load(open('../data/processed/descriptions_powo_PLANTS.pkl', 'rb'))\n",
    "plants_list =[key for key in plants_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e12158c6-9e25-4d66-ad06-333e8a4d298b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35198"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(plant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf881be9-fcd7-49dd-b171-7ecf6c7082f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 2/2 [00:21<00:00, 10.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# Init dict\n",
    "data = collections.defaultdict(list)\n",
    "# Init dict\n",
    "\n",
    "# DEBUGGING\n",
    "data_link = collections.defaultdict(list)\n",
    "#data_with_source = collections.defaultdict(list)\n",
    "\n",
    "query = 'description'\n",
    "\n",
    "for count, family in enumerate(tqdm(plants_list[10:12])):\n",
    "\n",
    "    \n",
    "    # Empty list\n",
    "    search_links = []\n",
    "    # create q\n",
    "    species_q = species.replace(' ', '+')\n",
    "    species_q = f'\"{species_q}\"+{query}'\n",
    "    # species_q = f'\"{species_q}\"+{query}'\n",
    "    try:\n",
    "        search_links += search_Duck(species_q)\n",
    "        search_links += search_Bing(species_q)\n",
    "    except:\n",
    "        # Skip connection timeout\n",
    "        continue\n",
    "    # Drop duplicates\n",
    "    search_links = list(set(search_links))\n",
    "    # DEBUGGING\n",
    "    data_link[species] += search_links\n",
    "    \n",
    "    # Loop over the URLs\n",
    "    for URL in search_links:\n",
    "        # Skip google archives\n",
    "        if 'google' in URL:\n",
    "            continue\n",
    "        # PDF and TXT\n",
    "        if URL.endswith('txt') or URL.endswith('pdf'):\n",
    "            \n",
    "            \"\"\"\n",
    "            Continue for now, insert the text/pdf processor here\n",
    "            \"\"\"\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            #print(URL)\n",
    "            page = requests.get(URL, timeout=5)\n",
    "            # Skip PDF files for now\n",
    "            if page.headers['Content-Type'].startswith('application/pdf'):\n",
    "                \n",
    "                \"\"\"\n",
    "                Continue for now, insert the pdf processor here\n",
    "                \"\"\"\n",
    "                continue\n",
    "                \n",
    "            # Soup the result\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                \n",
    "            # Skip Embedded PDF's\n",
    "            if 'pdf' in soup.title.text.lower():\n",
    "                continue\n",
    "            \n",
    "            #print(soup.title.text, species)\n",
    "            # Check if species exists somewhere within title\n",
    "            if bool(set(species.split()).intersection(soup.title.text.split())):\n",
    "                # Get text\n",
    "                dirty_text = soup.get_text(\". \", strip=True)\n",
    "                # Clean and break into sents\n",
    "                sentences = text_cleaner(dirty_text)\n",
    "                # Loop over the individual sentences\n",
    "                for sentence in sentences:                    \n",
    "                    # Create string object\n",
    "                    sentence_str = str(sentence)\n",
    "                    # Check if description\n",
    "                    if SpanPredictor(sentence_str):\n",
    "                        data[species].append((sentence_str, URL))\n",
    "                        #data_with_source[species].append(tuple([sentence_str, URL]))\n",
    "                            \n",
    "        except: \n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "437f9dad-17b4-431b-8611-a1ffbac7be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d116f09e-d670-4fac-b32e-e200c8a1191a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://robertvandevlasakker.com/',\n",
       " 'https://robertvandevlasakker.medium.com/',\n",
       " 'https://nl.linkedin.com/in/robertvandevlasakker',\n",
       " 'https://robertvandevlasakker.medium.com/list/50c8b547dc29',\n",
       " 'https://robertvandevlasakker.medium.com/followers',\n",
       " 'https://medium.com/nerd-for-tech/cartoonize-images-with-python-10e2a466b5fb',\n",
       " 'https://www.linkedin.com/in/robertvandewalle',\n",
       " 'https://creepypasta.fandom.com/wiki/Robert_the_Doll',\n",
       " 'https://au.linkedin.com/in/robert-van-de-berg-90362312',\n",
       " 'https://uk.linkedin.com/in/robert-van-der-meer-1b13162']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[result['href'] for result in results if result['href'].startswith('https')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad53fc-3fc7-4e04-9d26-3de1ef6c37de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671cd2d-6419-4997-a33b-33a6ac470afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f998c2b-1fb2-47d8-a240-2f2c3394a673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538fbe3-b75c-4c28-adb5-606632f88ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../data/processed/scrapeddata_train_species_description_random_0-1000_PLANTS.pkl', 'wb') as f:\n",
    "#    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6482e-f102-4f72-9aef-615c46b94a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_source['Dombeya shupangae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec021ba-f21e-471f-b200-16649feeed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_source['Forpus passerinus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af6bee-77d7-4b55-ab87-0c9618dceb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://independent-travellers.com/namibia/c39_c43/41.php'\n",
    "#URL = 'http://db.worldagroforestry.org//species/properties/Enterolobium_cyclocarpum'\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "dirty_text = soup.get_text(\". \", strip=True)\n",
    "sentences = text_cleaner(dirty_text)\n",
    "colored_string = colorize_sents(sentences)\n",
    "display(HTML(colored_string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
