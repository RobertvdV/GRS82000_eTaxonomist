{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4972715-9f9c-4211-a549-78a985fe3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from netgraph import Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b9bf2e-d590-430b-88db-621b3fcc5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color checker\n",
    "colors = list(mcolors.CSS4_COLORS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2c3d0-34e3-4e66-9063-ebb378e25566",
   "metadata": {},
   "source": [
    "## Create a glossary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77ff15e-7660-473a-9baf-87dd589d9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "URL = 'https://en.wikipedia.org/wiki/Glossary_of_plant_morphology'\n",
    "# Get the page\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, \"lxml\", from_encoding=\"iso-8859-1\")   \n",
    "\n",
    "glossary = collections.defaultdict(list)\n",
    "# Find all H4 \n",
    "for chapter in soup.find_all('h4')[0:]:\n",
    "    # Clean\n",
    "    chapter_text = chapter.text.rstrip('[edit]')\n",
    "    # Find all siblings\n",
    "    for sibling in chapter.find_next_siblings():\n",
    "        # Find the parent\n",
    "        for parent in sibling.find_previous_sibling('h4'):\n",
    "            # Only append if correspond to current chapter\n",
    "            if parent.text == chapter_text:\n",
    "                if 'â' in sibling.text:\n",
    "                    for tag in sibling.find_all('li'):\n",
    "                        candidates = tag.text.split('â')[0]\n",
    "                        candidates = candidates.split('/')\n",
    "                        for candidate in candidates:\n",
    "                            glossary[chapter_text.lower()].append(candidate.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e79d43f1-466a-423e-af57-e92e3ba3df99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['morphology', 'roots', 'stems', 'buds', 'leaves', 'basic flower parts', 'inflorescences', 'insertion of floral parts', 'union of flower parts', 'flower sexuality and presence of floral parts', 'flower symmetry', 'terms for fruits', 'fruit types', 'pteridophytes', 'bryophytes'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42656c0-55ac-4b52-b703-985565efeb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glossary['fruit types'] += [\n",
    "#    'fruit',\n",
    "#]\n",
    "\n",
    "glossary['leaves'] += [\n",
    "    'glume',\n",
    "    'surface',\n",
    "    'margin',\n",
    "    'leaves', \n",
    "    'auricles', \n",
    "    'spatheole',\n",
    "    'ovate',\n",
    "    'lanceolate',\n",
    "]\n",
    "\n",
    "glossary['basic flower parts'] +=[\n",
    "    'floret',\n",
    "    'awn',\n",
    "    \n",
    "]\n",
    "glossary['inflorescences'] += [\n",
    "    'spikelets',\n",
    "    'lemma',\n",
    "    'racemes',\n",
    "    'axis',\n",
    "]\n",
    "glossary['leaves'] += [\n",
    "    'rhachilla'\n",
    "]\n",
    "\n",
    "glossary['other parts'] += [\n",
    "    'apex',\n",
    "    'culm',\n",
    "    'tube',\n",
    "    'palea',\n",
    "    'crown',\n",
    "    'canopy',\n",
    "    'base',\n",
    "    'callus',\n",
    "    'hair',\n",
    "    'anther',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9339f7b8-16db-426f-876d-ea4909941b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/glossaries/plants.pkl', 'wb') as f:\n",
    "    pickle.dump(glossary, f)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9ceb5-bb96-427e-9a6d-5fcade9ef7bc",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba1afef-91f9-4b05-9af5-2467860f2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pickle.load(open('../../data/description/04_TRAIN_0000000-0014557_PLANTS.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e130d125-8caf-447b-ba7b-bc215a6061da",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_id = collections.defaultdict(list)\n",
    "descriptions = collections.defaultdict(list)\n",
    "\n",
    "compounds = [\n",
    "    'fertile', 'sterile',\n",
    "    'male', 'female', 'bisexual',\n",
    "    'basal', 'developed', \n",
    "    'primary', 'secondary', 'main',\n",
    "    'upper', 'lower', 'greater', 'dorsal', 'alternate', 'lesser', 'apex', 'outer',\n",
    "    'central', 'outermost', 'outer', 'inner', 'uppermost', 'median', 'dorsal', 'central', 'lateral',\n",
    "]\n",
    "\n",
    "def compound_reconstructor(token, doc):\n",
    "    if token.i == 0:\n",
    "        trait = token\n",
    "    elif doc[token.i - 1].pos_ == 'DET':\n",
    "        trait = token\n",
    "    elif doc[token.i - 3].dep_ == 'compound':\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 3].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 2].dep_ == 'compound':\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 2].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 1].dep_ == 'compound':\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    elif doc[token.i - 1].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    else:\n",
    "        trait = token   \n",
    "    return trait.lemma_\n",
    "\n",
    "def check_existance(t, doc):\n",
    "    \n",
    "    # Check prep\n",
    "    parent = next((parent for parent in t.ancestors if parent.dep_ == 'prep'), None)\n",
    "    if parent:\n",
    "        parent = parent.text\n",
    "    single = next((key for key, value in glossary.items() if t.lemma_.lower() in value), None)\n",
    "    multi = next((key for key, value in glossary.items() if t.text.lower() in value), None)\n",
    "    if single:\n",
    "        return parent, single\n",
    "    elif multi:\n",
    "        return parent, multi\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def extract_advmod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'advmod':\n",
    "            return child\n",
    "        \n",
    "def extract_nummod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'nummod':\n",
    "            return child\n",
    "\n",
    "def extract_conjunction(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    if t.dep_ == 'conj' and t.pos_ == 'ADJ':\n",
    "        return t \n",
    "\n",
    "def extract_amod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'amod':\n",
    "            return child\n",
    "        \n",
    "def extract_measurements(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    obj = None\n",
    "    measurements = ['wide', 'long', 'high',]\n",
    "    if t.text in measurements or t.lemma_ in measurements:\n",
    "        obj = doc[t.left_edge.i : t.right_edge.i + 1]\n",
    "    return obj\n",
    "\n",
    "def extract_prepositions(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'prep':\n",
    "            return doc[child.left_edge.i : child.right_edge.i + 1]\n",
    "\n",
    "def define_position(x, y, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    if len(x.text.split()) > 1:\n",
    "        return f'{y.text} {x.text}'\n",
    "    else:\n",
    "        try:\n",
    "            if x.i > y.i:\n",
    "                return doc[y.i : x.i + 1]\n",
    "            else:\n",
    "                return doc[x.i : y.i + 1]\n",
    "        except:\n",
    "            return f'{y.text} {x.text}'\n",
    "\n",
    "\n",
    "def extract_verb_advm(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'advmod':\n",
    "            return child\n",
    "\n",
    "def extract_verb_nmod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'nummod':\n",
    "            return doc[child.left_edge.i : child.right_edge.i + 1] \n",
    "        \n",
    "def extract_verb_prep(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'prep':\n",
    "            return child    \n",
    "\n",
    "def extract_verb_pobj(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'pobj' or child.dep_ == 'pcomp' or child.dep_ == 'prep':\n",
    "            return child\n",
    "        \n",
    "def extract_verb_dobj(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'dobj':\n",
    "            return child\n",
    "        \n",
    "def extract_verb_orpd(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'oprd':\n",
    "            return child    \n",
    "        \n",
    "def extract_verb_agnt(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'agent':\n",
    "            return child \n",
    "        \n",
    "def extract_verb_acomp(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'acomp':\n",
    "            obj = doc[child.left_edge.i : child.right_edge.i + 1].text.lower()\n",
    "            return obj \n",
    "        \n",
    "def extract_nounandverb_nummods(t, doc):\n",
    "    obj = None\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'nummod':\n",
    "            obj = doc[child.left_edge.i : child.right_edge.i + 1]\n",
    "            return obj   \n",
    "\n",
    "def extract_dnummod(t, doc):\n",
    "    obj = extract_nounandverb_nummods(t, doc)\n",
    "    if obj:\n",
    "        return obj.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_noun_adjectives(t, doc):\n",
    "    adjs = []\n",
    "    adjectives = []\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'appos':\n",
    "            continue\n",
    "        if child.dep_ == 'compound':\n",
    "            continue\n",
    "        if child.pos_ =='ADJ' or child.tag_ == 'VBN' and child.dep_ in ['conj', 'amod']:\n",
    "            \n",
    "            \n",
    "            advmod = extract_advmod(child, doc)\n",
    "            measurement = extract_measurements(child, doc)\n",
    "            prep = extract_prepositions(child, doc)\n",
    "            nummod = extract_nummod(child, doc)\n",
    "            amod = extract_amod(child, doc)\n",
    "           \n",
    "            \n",
    "            if child.lemma_.lower() in compounds:\n",
    "                continue\n",
    "            if child.text.lower() in compounds:\n",
    "                continue\n",
    "            elif measurement:\n",
    "                obj = measurement\n",
    "                adjs.append(obj)\n",
    "            elif amod:\n",
    "                obj = define_position(amod, child, doc)\n",
    "                adjs.append(obj)\n",
    "            elif advmod:\n",
    "                #obj = doc[advmod.i : child.i + 1]\n",
    "                obj = define_position(advmod, child, doc)\n",
    "                adjs.append(obj)\n",
    "            elif prep:\n",
    "                obj = define_position(prep, child, doc)\n",
    "                adjs.append(obj)\n",
    "            elif nummod:\n",
    "                obj = define_position(nummod, child, doc)\n",
    "                adjs.append(obj)                \n",
    "            else:\n",
    "                obj = child\n",
    "                adjs.append(obj)\n",
    "            for grandchild in child.subtree:\n",
    "                conj = extract_conjunction(grandchild, doc)\n",
    "                if conj:\n",
    "                    advmod = extract_advmod(conj, doc)\n",
    "                    prep = extract_prepositions(conj, doc)\n",
    "                    nummod = extract_nummod(conj, doc)\n",
    "                    if advmod:\n",
    "                        obj = define_position(advmod, conj, doc)\n",
    "                        adjs.append(obj)\n",
    "                    elif prep:\n",
    "                        obj = define_position(prep, conj, doc)\n",
    "                        adjs.append(obj)\n",
    "                    elif nummod:\n",
    "                        obj = define_position(nummod, conj, doc)\n",
    "                        adjs.append(obj)\n",
    "                    else:\n",
    "                        obj = conj\n",
    "                        adjs.append(obj)            \n",
    "    for adj in adjs:\n",
    "        try:\n",
    "            if adj.pos_ == 'VERB':\n",
    "                adj_text = adj.text.lower()\n",
    "            elif adj.root.pos_ == 'VERB':\n",
    "                adj_text = adj.text.lower()\n",
    "            else:\n",
    "                adj_text = adj.lemma_.lower()\n",
    "        except:\n",
    "                if type(adj) == str:\n",
    "                    adj_text = adj.lower()\n",
    "                else:\n",
    "                    adj_text = adj.text.lower()\n",
    "        for adj_split in adj_text.split(','):\n",
    "            adjectives.append(adj_split.strip())\n",
    "    return adjectives\n",
    "\n",
    "def extract_noun_appos(t, doc):\n",
    "    appos = []\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'appos':\n",
    "            obj = doc[child.left_edge.i : child.right_edge.i + 1].text.lower()\n",
    "            for obj_split in obj.split(','):\n",
    "                appos.append(obj_split.strip())\n",
    "    return appos\n",
    "\n",
    "def extract_noun_prep(t, doc):\n",
    "    preps = []\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'prep':\n",
    "            obj = doc[child.left_edge.i : child.right_edge.i + 1].text.lower()\n",
    "            for obj_split in obj.split(','):\n",
    "                preps.append(obj_split.strip())\n",
    "    return preps\n",
    "\n",
    "def check_species(t, species, doc):\n",
    "    if t.text in species.split():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def extract_noun_verbs_ROOT(t, doc):\n",
    "    relations = []\n",
    "    objects = []\n",
    "    if t.dep_ not in ['ROOT', 'nsubj', 'nsubjpass', 'csubj', 'csubjpass']:\n",
    "        return '', ''\n",
    "    parent = next((parent for parent in t.ancestors), None)\n",
    "    if parent and parent.pos_ == 'VERB' or parent and parent.pos_ == 'AUX':\n",
    "        try:\n",
    "            prep = extract_verb_prep(parent, doc)\n",
    "            if prep: \n",
    "                relations.append(f'{parent.text} {prep}'), objects.append(extract_verb_pobj(prep, doc).lemma_)\n",
    "            dobj = extract_verb_dobj(parent, doc)\n",
    "            if dobj: \n",
    "                #print(dobj)\n",
    "                relations.append(parent.text), objects.append(extract_verb_pobj(dobj, doc).lemma_)\n",
    "            oprd = extract_verb_orpd(parent, doc)\n",
    "            if oprd: \n",
    "                relations.append(parent.text), objects.append(oprd.text)\n",
    "            agnt = extract_verb_agnt(parent, doc)\n",
    "            if agnt: \n",
    "                relations.append(f'{parent.text} {agnt}'), objects.append(extract_verb_pobj(agnt, doc).lemma_)\n",
    "            nmod = extract_verb_nmod(parent, doc)\n",
    "            if nmod: \n",
    "                print(nmod)\n",
    "            advm = extract_verb_advm(parent, doc)\n",
    "            if advm: \n",
    "                relations.append(parent.text), objects.append(advm.text)\n",
    "            acomp = extract_verb_acomp(parent, doc)\n",
    "            if acomp: \n",
    "                relations.append(parent.text), objects.append(acomp) # Already text\n",
    "            if not any((prep, dobj, oprd, agnt, nmod, advm, acomp)): \n",
    "                relations.append('are'), objects.append(parent.text)\n",
    "        except:\n",
    "            pass\n",
    "       \n",
    "    return clean_verbs(relations, objects)\n",
    "\n",
    "def extract_noun_verbs_NON_ROOT(t, doc):\n",
    "    relations = []\n",
    "    objects = []\n",
    "    if t.dep_ not in ['ROOT', 'nsubj', 'nsubjpass', 'csubj', 'csubjpass']:\n",
    "        return '', ''\n",
    "    # Double check\n",
    "    parent = next((parent for parent in t.ancestors), None)\n",
    "    if not parent:\n",
    "        try:\n",
    "            for child in t.children:\n",
    "                if child.pos_ == 'VERB' and child.dep_ != 'amod':\n",
    "                    prep = extract_verb_prep(child, doc)\n",
    "                    if prep: \n",
    "                        noun = extract_verb_pobj(prep, doc)\n",
    "                        relations.append(f'{child.text} {prep}')\n",
    "                        objects.append(doc[noun.left_edge.i : noun.right_edge.i + 1].text)\n",
    "                    dobj = extract_verb_dobj(child, doc)\n",
    "                    if dobj:\n",
    "                        relations.append(child.text)\n",
    "                        objects.append(doc[dobj.left_edge.i : dobj.right_edge.i + 1].text) \n",
    "                    oprd = extract_verb_orpd(child, doc)\n",
    "                    if oprd:\n",
    "                        oprd_prep = extract_verb_prep(oprd, doc)\n",
    "                        if oprd_prep:\n",
    "                            relations.append(f'{child.text} {oprd.text}')\n",
    "                            objects.append(doc[oprd_prep.left_edge.i : oprd_prep.right_edge.i + 1].text) \n",
    "                        else:\n",
    "                            relations.append(child.text), objects.append(oprd.text)\n",
    "                    agnt = extract_verb_agnt(child, doc)\n",
    "                    if agnt:\n",
    "                        noun = extract_verb_pobj(agnt, doc)\n",
    "                        relations.append(f'{child.text} {agnt.text}')\n",
    "                        objects.append(doc[noun.left_edge.i : noun.right_edge.i + 1].text)\n",
    "                    nmod = extract_verb_nmod(child, doc)\n",
    "                    if nmod:\n",
    "                        relations.append('is')\n",
    "                        objects.append(f'{nmod.text} {child}') \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "    return clean_verbs(relations, objects)\n",
    "\n",
    "def extract_verbs_OTHER(t, doc):\n",
    "    \n",
    "    '''\n",
    "    NOT YET IN USE\n",
    "    '''\n",
    "    \n",
    "    relations = []\n",
    "    objects   = []\n",
    "    \n",
    "    empty   = ('', '')\n",
    "    empties = ([], [])\n",
    "    \n",
    "    if extract_noun_verbs_ROOT(t, doc) == empty  and extract_noun_verbs_NON_ROOT(t, doc) == empty:\n",
    "        \n",
    "        parent = next((child for child in t.children if child.pos_ == 'VERB'), None)\n",
    "        prep = extract_verb_prep(parent, doc)\n",
    "        if prep: \n",
    "            relations.append(f'{parent.text} {prep}'), objects.append(extract_verb_pobj(prep, doc).lemma_)\n",
    "        dobj = extract_verb_dobj(parent, doc)\n",
    "        if dobj: \n",
    "            relations.append(parent.text), objects.append(extract_verb_pobj(prep, doc).lemma_)\n",
    "        oprd = extract_verb_orpd(parent, doc)\n",
    "        if oprd: \n",
    "            relations.append(parent.text), objects.append(oprd.text)\n",
    "        agnt = extract_verb_agnt(parent, doc)\n",
    "        if agnt: \n",
    "            relations.append(parent.text), objects.append(agnt.text)\n",
    "        nmod = extract_verb_nmod(parent, doc)\n",
    "        if nmod: \n",
    "            print(nmod)\n",
    "        advm = extract_verb_advm(parent, doc)\n",
    "        if advm: \n",
    "            relations.append(parent.text), objects.append(advm.text)\n",
    "        acomp = extract_verb_acomp(parent, doc)\n",
    "        if acomp: \n",
    "            relations.append(parent.text), objects.append(acomp.text)\n",
    "        if not any((prep, dobj, oprd, agnt, nmod, advm, acomp)): \n",
    "            relations.append('are'), objects.append(parent.text)\n",
    "            \n",
    "    return relations, objects\n",
    "\n",
    "\n",
    "def clean_verbs(relations, objects):\n",
    "    \n",
    "    rel = []\n",
    "    obj = []\n",
    "    for relation, object_ in zip(relations, objects):\n",
    "        #rel.append(relation)\n",
    "        #obj.append(object_.split(',')[0])\n",
    "        for obj_split in object_.split(','):\n",
    "            #print(relation, obj_split)\n",
    "            rel.append(relation.lower().strip())\n",
    "            obj.append(obj_split.lower().strip())            \n",
    "            \n",
    "    return rel, obj\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e86b1640-58b5-423a-820b-ac2b5576a1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39ea8d5121d4ce3a03523a2af960eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two\n",
      "one\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "descriptions = collections.defaultdict(list)\n",
    "\n",
    "# For plotting purposes\n",
    "parts = []\n",
    "traits = []\n",
    "for species in tqdm_notebook(list(DATA.keys())[0:1000]):\n",
    "    for idx, text in enumerate(DATA[species][0:]):\n",
    "\n",
    "\n",
    "#for species in tqdm_notebook(COMMON_species[0:]):\n",
    "#for species in tqdm_notebook(list(descriptions_text.keys())):\n",
    "#    for idx, text in enumerate(TEST[species][0:]):\n",
    "        # Clean the text\n",
    "        text = re.sub(r'(?<!\\d)\\.(?!\\d)', ' ', text)\n",
    "        text = re.sub(r'\\s×\\s', ' times ', text)\n",
    "        text = re.sub(r'\\xa0', ' ', text)\n",
    "        text = f'{text.strip()}.'\n",
    "        # Reset variables\n",
    "        part=trait=rel=obj=adjectives = None \n",
    "        # NLP\n",
    "        doc = nlp(text)\n",
    "        # Init\n",
    "        descriptions[species, idx] = []\n",
    "        triples = []\n",
    "        # Loop over tokens\n",
    "        for t in doc:\n",
    "            if t.dep_ == 'compound':\n",
    "                continue\n",
    "            ### SUBJECTS ###    \n",
    "            if t.pos_ == 'NOUN' or t.pos_ == 'PROPN':\n",
    "                # Check existance of parts\n",
    "                prep, part = check_existance(t, doc)\n",
    "                if part:\n",
    "                    # Reconstruct Compounds & Append\n",
    "                    trait = compound_reconstructor(t, doc)\n",
    "                    triples.append(('species', 'has main part', part))\n",
    "                    if prep:\n",
    "                        triples.append((part, f'{prep}', trait))\n",
    "                    else:\n",
    "                        triples.append((part, f'has sub part', trait))\n",
    "                    # NOUN ADJECTIVES\n",
    "                    adjectives = extract_noun_adjectives(t, doc)\n",
    "                    for adjective in adjectives:\n",
    "                        triples.append((trait, 'is', adjective))\n",
    "                    # NOUN ROOT VERBS\n",
    "                    verbs_rel, verbs_obj = extract_noun_verbs_ROOT(t, doc)\n",
    "                    for rel, obj in zip(verbs_rel, verbs_obj):\n",
    "                        triples.append((trait, rel, obj))\n",
    "                    # NOUN NON ROOT VERBS\n",
    "                    verbs_rel, verbs_obj = extract_noun_verbs_NON_ROOT(t, doc)\n",
    "                    for rel, obj in zip(verbs_rel, verbs_obj):\n",
    "                        triples.append((trait, rel, obj))\n",
    "                    # NOUN APPOSITIONAL MODIFIER\n",
    "                    adjectives = extract_noun_appos(t, doc)\n",
    "                    for adjective in adjectives:\n",
    "                        triples.append((trait, 'is', adjective))\n",
    "                    # NOUN NUMMODS\n",
    "                    nummod = extract_dnummod(t, doc)\n",
    "                    triples.append((trait, 'is', nummod))\n",
    "                    # NOUN PREPOSITIONAL MODIFIER\n",
    "                    adjectives = extract_noun_prep(t, doc)\n",
    "                    for adjective in adjectives:\n",
    "                        triples.append((trait, 'is', adjective))                \n",
    "\n",
    "        # APPEND\n",
    "        descriptions[species, idx] = [triple for triple in triples if all(triple)]     \n",
    "        \n",
    "                    \n",
    "        #print(idx, doc)\n",
    "        #print(descriptions[species, idx])\n",
    "        #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1d7df41-bf96-46ec-a2ba-3dc81a459dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_text = collections.defaultdict(list)\n",
    "descriptions_RDFs = collections.defaultdict(list)\n",
    "\n",
    "for (species, idx) in descriptions.keys():\n",
    "    for (sub, rel, obj) in descriptions[(species, idx)]:\n",
    "        \n",
    "        sub = sub.strip().lower().rstrip('.')\n",
    "        rel = rel.strip().lower().rstrip('.')\n",
    "        obj = obj.strip().lower().rstrip('.')\n",
    "        \n",
    "        text = f'{sub} {rel} {obj}.'.capitalize()\n",
    "        # Make sure order is the same\n",
    "        descriptions_text[species].append(text)\n",
    "        descriptions_RDFs[species].append((sub, rel, obj))\n",
    "        \n",
    "\n",
    "for species in descriptions_text.keys():\n",
    "    descriptions_text[species] = list(set(descriptions_text[species]))\n",
    "    descriptions_RDFs[species] = list(set(descriptions_RDFs[species]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e08f3e3d-1167-4b4f-a887-beca6417db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/RDF_TEXT_TRAIN_SET_1000.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_text, f)      \n",
    "    \n",
    "with open('../../data/processed/RDF_TRIPLES_TRAIN_SET_1000.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_RDFs, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23e84f-201f-4137-aa12-7c8200c4f4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
