{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec230a9a-65f2-4aa7-b55b-ab44db80249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "from spacy import displacy\n",
    "import collections\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, logging\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as colors\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e53dad6-2731-44de-bc43-17fee9b0f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src/models/')\n",
    "sys.path.insert(0, '../src/features/')\n",
    "\n",
    "import predict_model\n",
    "from build_features import text_cleaner\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c58a50-f003-467a-9cd1-1fd17fe032e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "URL = 'https://en.wikipedia.org/wiki/Glossary_of_plant_morphology'\n",
    "# Get the page\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, \"lxml\", from_encoding=\"iso-8859-1\")   \n",
    "\n",
    "glossary = collections.defaultdict(list)\n",
    "# Find all H4 \n",
    "for chapter in soup.find_all('h4')[0:]:\n",
    "    # Clean\n",
    "    chapter_text = chapter.text.rstrip('[edit]')\n",
    "    # Find all siblings\n",
    "    for sibling in chapter.find_next_siblings():\n",
    "        # Find the parent\n",
    "        for parent in sibling.find_previous_sibling('h4'):\n",
    "            # Only append if correspond to current chapter\n",
    "            if parent.text == chapter_text:\n",
    "                if 'â' in sibling.text:\n",
    "                    for tag in sibling.find_all('li'):\n",
    "                        candidates = tag.text.split('â')[0]\n",
    "                        candidates = candidates.split('/')\n",
    "                        for candidate in candidates:\n",
    "                            glossary[chapter_text.lower()].append(candidate.strip().lower())  \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc84d6f-9be4-4411-929d-3e06c3c87eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary['leaves'] += [\n",
    "    'glume', 'surface', 'margin',\n",
    "    'leaves', 'auricles', 'spatheole',\n",
    "    'ovate', 'lanceolate',\n",
    "]\n",
    "\n",
    "glossary['basic flower parts'] += [\n",
    "    'floret', 'awn',\n",
    "    'pod', 'lobe',\n",
    "    \n",
    "]\n",
    "glossary['inflorescences'] += [\n",
    "    'spikelets', 'lemma', 'racemes',\n",
    "    'axis',\n",
    "]\n",
    "glossary['leaves'] += [\n",
    "    'rhachilla',\n",
    "    'needles',\n",
    "]\n",
    "\n",
    "glossary['other parts'] += [\n",
    "    'apex', 'culm', 'tube',\n",
    "    'palea', 'crown', 'canopy',\n",
    "    'base', 'callus', 'hair',\n",
    "    'anther', 'tuberculate'\n",
    "\n",
    "]\n",
    "\n",
    "glossary['plant property'] += [\n",
    "    'tree', 'shrub',\n",
    "    'life-span', 'life', 'span',\n",
    "    'bloom-time',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c7bcb3a-5e8e-446f-bef8-aeed9aea9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/glossaries/plants.pkl', 'wb') as f:\n",
    "    pickle.dump(glossary, f)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67e5ca0-cfee-4c27-8f56-086fe8168981",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = [\n",
    "    'fertile', 'sterile',\n",
    "    'male', 'female', 'bisexual',\n",
    "    'basal', 'developed', \n",
    "    'primary', 'secondary', 'main',\n",
    "    'upper', 'lower', 'greater', 'dorsal', 'alternate', 'lesser', 'apex', 'outer',\n",
    "    'central', 'outermost', 'outer', 'inner', 'uppermost', 'median', 'dorsal', 'central', 'lateral',\n",
    "    'young', 'mature',\n",
    "]\n",
    "\n",
    "rubbish = [\n",
    "    '.', ',', '-',\n",
    "]\n",
    "\n",
    "\n",
    "#with open('../../data/glossaries/plants_compounds.pkl', 'wb') as f:\n",
    "#    pickle.dump(compounds, f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "817000e0-29aa-4d49-ba9e-2eaa553d47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_reconstructor(token, doc):\n",
    "    if token.i == 0:\n",
    "        trait = token\n",
    "    elif doc[token.i - 1].pos_ == 'DET':\n",
    "        trait = token\n",
    "    elif doc[token.i - 3].dep_ == 'compound':\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 3].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 2].dep_ == 'compound':\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 2].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 1].dep_ == 'compound':\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    elif doc[token.i - 1].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    else:\n",
    "        trait = token\n",
    "    if ','  in trait.lemma_:\n",
    "        return None\n",
    "    return trait.lemma_\n",
    "\n",
    "def check_existance(t, doc):\n",
    "    \n",
    "    # Check prep\n",
    "    single = next((key for key, value in glossary.items() if t.lemma_.lower() in value), None)\n",
    "    multi = next((key for key, value in glossary.items() if t.text.lower() in value), None)\n",
    "    if single:\n",
    "        return single\n",
    "    elif multi:\n",
    "        return multi\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def verbal_helper(t, doc, attribute):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == attribute:\n",
    "            obj = doc[child.left_edge.i : child.right_edge.i + 1]\n",
    "            return obj \n",
    "        \n",
    "def clean_verbs(relations, objects):\n",
    "    rel = []\n",
    "    obj = []\n",
    "    for relation, object_ in zip(relations, objects):       \n",
    "        for obj_split in re.split(',|and', object_):\n",
    "            rel.append(relation.lower().strip())\n",
    "            obj.append(obj_split.lower().strip())            \n",
    "            \n",
    "    return rel, obj\n",
    "\n",
    "\n",
    "def clean_nouns(objects_dirty):\n",
    "    \n",
    "    \n",
    "    objects = []\n",
    "    for object_dirty in objects_dirty:\n",
    "        split_objects = re.split(',|and', object_dirty)\n",
    "        if len(split_objects) == 1:\n",
    "            objects.append(split_objects[0].strip())\n",
    "        elif len(split_objects) == 2:\n",
    "            objects.append(split_objects[0].strip())\n",
    "            objects.append(split_objects[1].strip())\n",
    "        else:\n",
    "            objects.append(split_objects[0].strip())\n",
    "            objects.append(split_objects[1].strip())\n",
    "    return objects\n",
    "        \n",
    "def extract_verbal_information(t, doc):\n",
    "    \n",
    "    attributes = [\"attr\", \"prep\", \"pobj\", \n",
    "                  \"oprd\", \"agnt\", \"nmod\", \n",
    "                  \"advm\", \"acomp\", \"pcomp\",\n",
    "                  \"acl\"]\n",
    "    \n",
    "    allowed_items = ['ROOT', 'nsubj', 'nsubjpass', \n",
    "                     'csubj', 'csubjpass']\n",
    "    \n",
    "    relations = []\n",
    "    objects   = []\n",
    "        \n",
    "    if t.dep_ in allowed_items:\n",
    "        parent = next((parent for parent in t.ancestors), None)\n",
    "        if parent and parent.pos_ == 'VERB' or parent and parent.pos_ == 'AUX':\n",
    "            for attribute in attributes:\n",
    "                item = verbal_helper(parent, doc, attribute)\n",
    "                if item:\n",
    "                    relations.append(parent.text), \n",
    "                    objects.append(item.text)\n",
    "        else:\n",
    "            for child in t.children:\n",
    "                if child.pos_ == 'VERB' and child.dep_ != 'amod':\n",
    "                    for attribute in attributes:\n",
    "                        item = verbal_helper(child, doc, attribute)\n",
    "                        if item:\n",
    "                            relations.append(child.text), \n",
    "                            objects.append(item.text)\n",
    "    \n",
    "    return clean_verbs(relations, objects)\n",
    "\n",
    "def noun_helper(t, doc, attribute):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    if t.dep_ == attribute:\n",
    "        obj = doc[t.left_edge.i : t.right_edge.i + 1]\n",
    "        return obj     \n",
    "    \n",
    "def conjunction_helper(t, doc):\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'conj':\n",
    "            return child\n",
    "    \n",
    "def extract_noun_information(t, doc):\n",
    "    \n",
    "    attributes = [\"amod\", \"appos\", \"prep\"]    \n",
    "    objectives = []\n",
    "   \n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'compound' or child.lemma_.lower() in compounds or child.text.lower() in compounds:\n",
    "            continue\n",
    "        for attribute in attributes:\n",
    "            item = noun_helper(child, doc, attribute)\n",
    "            if item:\n",
    "                objectives.append(item.lemma_)\n",
    "              \n",
    "    #return objectives\n",
    "    return clean_nouns(objectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5246-e92d-41e5-8d3a-a7e91dba80bd",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b94819bb-bd8e-4ba3-aedf-1b1fd930156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGRO_dirty = pickle.load(open('../data/description/descriptions_agroforestry_PLANTS.pkl', 'rb'))\n",
    "AGRO = collections.defaultdict(list)\n",
    "for species in AGRO_dirty.keys():\n",
    "    for (sentence, URL) in AGRO_dirty[species]:\n",
    "        AGRO[species.lower()].append(sentence)\n",
    "        \n",
    "POWO_dirty = pickle.load(open('../data/description/descriptions_powo_PLANTS.pkl', 'rb'))\n",
    "POWO = collections.defaultdict(list)\n",
    "for species in POWO_dirty.keys():\n",
    "    for (sentence, URL) in POWO_dirty[species]:\n",
    "        POWO[species.lower()].append(sentence)\n",
    "        \n",
    "LIFE_dirty = pickle.load(open('../data/description/descriptions_llifeV2_PLANTS.pkl', 'rb'))\n",
    "LIFE = collections.defaultdict(list)\n",
    "for species in LIFE_dirty.keys():\n",
    "    for (sentence, URL) in LIFE_dirty[species]:\n",
    "        LIFE[species.lower()].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7fc1cf-5500-4e07-8a07-aed7a35df06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "POWO_species = [key for key in list(POWO.keys())]\n",
    "AGRO_species = [key for key in list(AGRO.keys())]\n",
    "LIFE_species = [key for key in list(LIFE.keys())]\n",
    "\n",
    "# Test set\n",
    "TEST_species = LIFE_species + AGRO_species\n",
    "# Common_species\n",
    "COMMON_species = list(set(POWO_species) & set(TEST_species))\n",
    "\n",
    "TEST = collections.defaultdict(list)\n",
    "for species in COMMON_species:\n",
    "    TEST[species] += AGRO[species]\n",
    "    TEST[species] += LIFE[species]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a8169-8846-4a9a-bfba-c7a2e270d56c",
   "metadata": {},
   "source": [
    "### TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7b144b3-e3e2-4752-ac7d-0b676a7f7f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d323b5536494df69079d694573e002b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions = collections.defaultdict(list)\n",
    "\n",
    "# For plotting purposes\n",
    "baseparts = []\n",
    "traits = []\n",
    "for species in tqdm_notebook(COMMON_species[0:]):\n",
    "    for idx, text in enumerate(POWO[species][0:]):\n",
    "\n",
    "        # Clean the text\n",
    "        text = re.sub(r'(?<!\\d)\\.(?!\\d)', ' ', text)\n",
    "        text = re.sub(r'\\s×\\s', ' times ', text)\n",
    "        text = re.sub(r'\\xa0', ' ', text)\n",
    "        text = f'{text.strip()}.'\n",
    "        # NLP\n",
    "        doc = nlp(text)\n",
    "        # Init\n",
    "        descriptions[species, idx] = []\n",
    "        triples = []\n",
    "        # Loop over tokens\n",
    "        for t in doc:\n",
    "            if t.dep_ == 'compound':\n",
    "                continue\n",
    "            ### SUBJECTS ###    \n",
    "            if t.pos_ == 'NOUN' or t.pos_ == 'PROPN':\n",
    "                # Check existance of parts\n",
    "                part = check_existance(t, doc)\n",
    "                if part:\n",
    "                    # Reconstruct Compounds & Append\n",
    "                    trait = compound_reconstructor(t, doc)\n",
    "                    triples.append(('species', 'has main part', part))\n",
    "                    triples.append((part, f'has sub part', trait))\n",
    "                    \n",
    "                    ## Plotting\n",
    "                    baseparts.append(part)\n",
    "                    traits.append(trait)\n",
    "                    \n",
    "                    # VERBS\n",
    "                    relations, objects = extract_verbal_information(t, doc)\n",
    "                    for rel, obj in zip(relations, objects):\n",
    "                        if obj not in rubbish:\n",
    "                            triples.append((trait, rel, obj))\n",
    "                    # ADJECTIVES\n",
    "                    objects = extract_noun_information(t, doc)\n",
    "                    for obj in objects:\n",
    "                        if obj not in rubbish:\n",
    "                            triples.append((trait, 'property', obj))             \n",
    "                        \n",
    "        # APPEND\n",
    "        descriptions[species, idx] = [triple for triple in triples if all(triple)]     \n",
    "        \n",
    "descriptions_text = collections.defaultdict(list)\n",
    "descriptions_RDFs = collections.defaultdict(list)\n",
    "\n",
    "for (species, idx) in descriptions.keys():\n",
    "    #print(species)\n",
    "    for (sub, rel, obj) in descriptions[(species, idx)]:\n",
    "        text = f'{sub} {rel} {obj}.'.capitalize()\n",
    "        # Make sure order is the same\n",
    "        descriptions_text[species].append(text)\n",
    "        descriptions_RDFs[species].append((sub, rel, obj))\n",
    "        \n",
    "        \n",
    "with open('../data/processed/KN_eval_TRAINSET_text.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_text, f)      \n",
    "    \n",
    "with open('../data/processed/KN_eval_TRAINSET_triples.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_RDFs, f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63be8be-05c6-4125-9777-12ea4ffc96cc",
   "metadata": {},
   "source": [
    "### TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "816e292a-58af-421d-97d3-095572794398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6613e8d15a45108a3df57dd8c8ba04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions = collections.defaultdict(list)\n",
    "\n",
    "# For plotting purposes\n",
    "baseparts = []\n",
    "traits = []\n",
    "for species in tqdm_notebook(descriptions_RDFs.keys()):\n",
    "    for idx, text in enumerate(TEST[species][0:]):\n",
    "\n",
    "        # Clean the text\n",
    "        text = re.sub(r'(?<!\\d)\\.(?!\\d)', ' ', text)\n",
    "        text = re.sub(r'\\s×\\s', ' times ', text)\n",
    "        text = re.sub(r'\\xa0', ' ', text)\n",
    "        text = f'{text.strip()}.'\n",
    "        # NLP\n",
    "        doc = nlp(text)\n",
    "        # Init\n",
    "        descriptions[species, idx] = []\n",
    "        triples = []\n",
    "        # Loop over tokens\n",
    "        for t in doc:\n",
    "            if t.dep_ == 'compound':\n",
    "                continue\n",
    "            ### SUBJECTS ###    \n",
    "            if t.pos_ == 'NOUN' or t.pos_ == 'PROPN':\n",
    "                # Check existance of parts\n",
    "                part = check_existance(t, doc)\n",
    "                if part:\n",
    "                    # Reconstruct Compounds & Append\n",
    "                    trait = compound_reconstructor(t, doc)\n",
    "                    triples.append(('species', 'has main part', part))\n",
    "                    triples.append((part, f'has sub part', trait))\n",
    "                    \n",
    "                    ## Plotting\n",
    "                    baseparts.append(part)\n",
    "                    traits.append(trait)\n",
    "                    \n",
    "                    # VERBS\n",
    "                    relations, objects = extract_verbal_information(t, doc)\n",
    "                    for rel, obj in zip(relations, objects):\n",
    "                        if obj not in rubbish:\n",
    "                            triples.append((trait, rel, obj))\n",
    "                    # ADJECTIVES\n",
    "                    objects = extract_noun_information(t, doc)\n",
    "                    for obj in objects:\n",
    "                        if obj not in rubbish:\n",
    "                            triples.append((trait, 'property', obj))             \n",
    "                        \n",
    "        # APPEND\n",
    "        descriptions[species, idx] = [triple for triple in triples if all(triple)]     \n",
    "        \n",
    "descriptions_text = collections.defaultdict(list)\n",
    "descriptions_RDFs = collections.defaultdict(list)\n",
    "\n",
    "for (species, idx) in descriptions.keys():\n",
    "    #print(species)\n",
    "    for (sub, rel, obj) in descriptions[(species, idx)]:\n",
    "        text = f'{sub} {rel} {obj}.'.capitalize()\n",
    "        # Make sure order is the same\n",
    "        descriptions_text[species].append(text)\n",
    "        descriptions_RDFs[species].append((sub, rel, obj))\n",
    "        \n",
    "        \n",
    "with open('../data/processed/KN_eval_TESTSET_text.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_text, f)      \n",
    "    \n",
    "with open('../data/processed/KN_eval_TESTSET_triples.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_RDFs, f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a197a-d22f-476b-9952-3115950d92aa",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e8d7dc0-63e4-47f5-af51-67af0c95e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from predict_model import load_CUB_Bert as load_model\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1838a599-6a0c-436e-b5a6-62de08514687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Success\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"../models/\", 'saved_weights_PlantSpecies_KN_eval_553.pt', outputsize=553)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6321f9b-e0b5-4560-9f74-0a9b85a8875c",
   "metadata": {},
   "source": [
    "Load the data and get the model keys (are still arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "95360c17-2603-4ec1-872d-9709e7eb0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = pickle.load(open('../data/processed/KN_eval_TRAINSET_text.pkl', 'rb'))\n",
    "TEST = pickle.load(open('../data/processed/KN_eval_TESTSET_text.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "db77503e-fd31-48d0-ade1-6ea2953e0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "keys = np.array([key for key in TRAIN.keys()])\n",
    "keys_encoded = lb.fit_transform(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3c8fff9-50d4-423a-a4b6-24772eb7c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for prediction\n",
    "def WhichPlant(span, model):\n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(**inputs)\n",
    "    return np.squeeze(outputs.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b23015d7-8481-4589-abbb-3acbc65f66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the species\n",
    "plants = list(TEST.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64dbec2d-d515-4fb7-ac2b-776fdaf3b1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136a53691669437eb4aa04d3e4857169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correct = []\n",
    "incorrect = []\n",
    "\n",
    "for plant in tqdm_notebook(plants[0:]):\n",
    "    h, w = (len(TEST[plant]), 553)    \n",
    "    logstack = np.zeros((h, w), float)\n",
    "    probstack = np.zeros((h, w), float)\n",
    "    # Plant index\n",
    "    if plant not in TRAIN.keys():\n",
    "        continue\n",
    "    plantID = np.where(lb.classes_ == plant)[0][0]\n",
    "    for idx, text in enumerate((TEST[plant][0:])):\n",
    "        probs = WhichPlant(text, model=model)\n",
    "        if idx == 0:\n",
    "            logstack[idx] = probs\n",
    "        else:\n",
    "            logstack[idx] = logstack[idx-1] + probs\n",
    "    correct.append(logstack[-1][plantID])\n",
    "    incorrect.append(np.delete(logstack[-1], plantID).mean())\n",
    "    \n",
    "correct = np.array(correct)\n",
    "incorrect = np.array(incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25579228-2b6a-4e46-a29a-ae4e5a3dd8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plant in COMMON_species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07eb48c-7f23-4e12-a77b-74f2d6b01f78",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "95b08385-9c64-48b9-8544-26510bb45e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "24cf264d-406b-4eaa-8c46-92c9f2c33627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-625.4463096261024\n",
      "-895.8897697759343\n",
      "\n",
      "\n",
      "-921.3569289754428\n",
      "-990.0546084702802\n",
      "\n",
      "\n",
      "(224,)\n",
      "(224,)\n"
     ]
    }
   ],
   "source": [
    "print(np.median(correct))\n",
    "print(correct.mean())\n",
    "print('\\n')\n",
    "print(np.median(incorrect))\n",
    "print(incorrect.mean())\n",
    "print('\\n')\n",
    "print(correct.shape)\n",
    "print(incorrect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f4f7dd44-f2eb-4095-99fe-e48a8e6ae780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=18403.0, pvalue=1.1425307556202456e-09)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.wilcoxon(correct, incorrect, alternative='greater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "25fa0100-be8f-426f-91c1-09838b3d4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Plotting\n",
    "d = {'Correct': correct, 'Incorrect': incorrect}\n",
    "df = pd.DataFrame(dict([ (k, pd.Series(v)) for k, v in d.items() ]))\n",
    "df_m = df.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "97a36580-ace1-4de2-b462-46137029c738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='variable', ylabel='value'>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=df_m,\n",
    "              showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1419eb72-e871-4216-8798-9478799e6fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
