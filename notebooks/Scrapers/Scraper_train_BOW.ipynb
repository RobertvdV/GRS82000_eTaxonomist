{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f21df-02e6-44a8-bb27-a5fbeace22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422cc81a-3376-4c69-b226-9dde93d0fc8e",
   "metadata": {},
   "source": [
    "## Scrape Species List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a5324-02d3-471a-bc28-906cd9848245",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://birdsoftheworld.org/bow/specieslist'\n",
    "#URL = 'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/specieslist'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "BOWlist = soup.find_all('a')\n",
    "\n",
    "URL = 'https://birdsoftheworld.org/bow/specieslist'\n",
    "#URL = 'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/specieslist'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "'''\n",
    "# Create a list with links\n",
    "BOWpages = ['https://birdsoftheworld.org' + pages.get('href').replace('introduction', '') for pages in BOWlist \n",
    "                   if pages.get('href') != None\n",
    "                   if pages.get('href').startswith('/bow/species')]\n",
    "\n",
    "'''\n",
    "# Create a list with links\n",
    "BOWpages = ['https://birdsoftheworld-org.ezproxy.library.wur.nl' + pages.get('href').replace('introduction', '') for pages in BOWlist \n",
    "                   if pages.get('href') != None\n",
    "                   if pages.get('href').startswith('/bow/species')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dddf42e-3bd6-435d-b0c8-7b14e6aebecc",
   "metadata": {},
   "source": [
    "## Open Driver (Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d31f4-c372-4c3f-a073-c1aa8f8d6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init driver\n",
    "browser = webdriver.Safari()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Login URL\n",
    "browser.get(\"https://edepot.wur.nl/541124\")\n",
    "\n",
    "time.sleep(8)\n",
    "\n",
    "# Current page\n",
    "main_page = browser.current_window_handle\n",
    "\n",
    "# Hit Okay\n",
    "Ok = browser.find_element_by_id(\"loginlinkBtn\")\n",
    "Ok.send_keys(u'\\ue007')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# changing the handles to access login page\n",
    "for handle in browser.window_handles:\n",
    "    if handle != main_page:\n",
    "        login_page = handle\n",
    "          \n",
    "# change the control to signin page        \n",
    "browser.switch_to.window(login_page)\n",
    "\n",
    "# Username\n",
    "username = browser.find_element_by_id(\"userNameInput\")\n",
    "username.clear()\n",
    "username.send_keys(\"robert.vandevlasakker@wur.nl\")\n",
    "\n",
    "# Password\n",
    "password = browser.find_element_by_name(\"Password\")\n",
    "password.clear()\n",
    "password.send_keys(\"XXXXXX\")\n",
    "\n",
    "# Hit Okay\n",
    "Ok = browser.find_element_by_id(\"submitButton\")\n",
    "Ok.send_keys(u'\\ue007')\n",
    "\n",
    "time.sleep(8)\n",
    "\n",
    "'''\n",
    "# changing the handles to access login page\n",
    "for handle in browser.window_handles:\n",
    "    if handle != main_page or handle!= login_page:\n",
    "        BOW = handle\n",
    "'''\n",
    "\n",
    "# change the control to signin page        \n",
    "browser.switch_to.window(main_page)\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadc540-6dd4-4252-9973-de5a6f715a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible pages\n",
    "Pages = ['Introduction', 'Appearance', 'Systematics', 'Distribution',\n",
    "         'Habitat', 'Movement', 'Foodhabits', 'Sounds',\n",
    "         'Behavior', 'Breeding', 'Demography', 'Conservation',\n",
    "         'Other', 'Priorities']\n",
    "\n",
    "for BirdWebPage in BOWpages[8000:]:\n",
    "    \n",
    "    # Init page\n",
    "    BirdURL = BirdWebPage\n",
    "\n",
    "    # Get the bird page\n",
    "    browser.get(BirdURL)\n",
    "\n",
    "    # Set a sleep timer\n",
    "    SleepTime = random.randint(3, 4)\n",
    "\n",
    "    # Read the XML file\n",
    "    HTML = browser.page_source\n",
    "\n",
    "    # Species name\n",
    "    soup = BeautifulSoup(HTML, 'html.parser')\n",
    "    SpeciesName = soup.title.text.split(' - ')[0].lstrip()\n",
    "\n",
    "    # Create folder\n",
    "    os.mkdir('../data/raw/' + SpeciesName)\n",
    "\n",
    "    # Dump the HTML file\n",
    "    with open('../data/raw/' + SpeciesName + '/' + Pages[0] + '.html', \"w\") as f:\n",
    "          f.write(HTML)\n",
    "\n",
    "    # Check URL for shorter account\n",
    "    URL_intro = browser.current_url\n",
    "\n",
    "    # Sleep Just in case\n",
    "    time.sleep(SleepTime)\n",
    "\n",
    "    # Loop over possible pages\n",
    "    for Index, Page in enumerate(Pages[1:]):\n",
    "\n",
    "        # Create URL\n",
    "        URL = BirdURL + Page.lower()\n",
    "        # Go to URL\n",
    "        browser.get(URL)\n",
    "\n",
    "        # Get current URL\n",
    "        URL_check = browser.current_url\n",
    "\n",
    "        # Check if short account\n",
    "        if URL_check == URL_intro and Index < 2:\n",
    "            time.sleep(4)\n",
    "            break\n",
    "        # Check if links are missing    \n",
    "        elif URL_check == URL_intro and Index > 2:\n",
    "            time.sleep(4)\n",
    "            continue            \n",
    "\n",
    "        # Get the HTML info \n",
    "        HTML = browser.page_source\n",
    "\n",
    "        # Dump the HTML file\n",
    "        with open('../data/raw/' + SpeciesName + '/' + Page + '.html', \"w\") as f:\n",
    "              f.write(HTML)\n",
    "\n",
    "        # Sleep \n",
    "        time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba0dc5-6cf7-4d58-b912-267397b669c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible pages\n",
    "Pages = ['Introduction', 'Appearance', 'Systematics', 'Distribution',\n",
    "         'Habitat', 'Movement', 'Foodhabits', 'Sounds',\n",
    "         'Behavior', 'Breeding', 'Demography', 'Conservation',\n",
    "         'Other', 'Priorities']\n",
    "\n",
    "Testpages = ['https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/bertin1/cur/',\n",
    "             'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/norcar/cur/',\n",
    "             'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/ostric2/cur/']\n",
    "\n",
    "for BirdWebPage in Testpages:\n",
    "    \n",
    "    # Init page\n",
    "    BirdURL = BirdWebPage\n",
    "\n",
    "    # Get the bird page\n",
    "    browser.get(BirdURL)\n",
    "\n",
    "    # Set a sleep timer\n",
    "    SleepTime = random.randint(3, 4)\n",
    "\n",
    "    # Read the XML file\n",
    "    HTML = browser.page_source\n",
    "\n",
    "    # Species name\n",
    "    soup = BeautifulSoup(HTML, 'html.parser')\n",
    "    SpeciesName = soup.title.text.split(' - ')[0].lstrip()\n",
    "\n",
    "    # Create folder\n",
    "    os.mkdir('../data/raw/' + SpeciesName)\n",
    "\n",
    "    # Dump the HTML file\n",
    "    with open('../data/raw/' + SpeciesName + '/' + Pages[0] + '.html', \"w\") as f:\n",
    "          f.write(HTML)\n",
    "\n",
    "    # Check URL for shorter account\n",
    "    URL_intro = browser.current_url\n",
    "    print(URL_intro)\n",
    "\n",
    "    # Sleep Just in case\n",
    "    time.sleep(SleepTime)\n",
    "\n",
    "    # Loop over possible pages\n",
    "    for Index, Page in enumerate(Pages[1:]):\n",
    "\n",
    "        # Create URL\n",
    "        URL = BirdURL + Page.lower()\n",
    "        # Go to URL\n",
    "        browser.get(URL)\n",
    "\n",
    "        # Get current URL\n",
    "        URL_check = browser.current_url\n",
    "        print(URL_check)\n",
    "\n",
    "        # Check if short account\n",
    "        if URL_check == URL_intro and Index < 2:\n",
    "            time.sleep(4)\n",
    "            break\n",
    "        # Check if links are missing    \n",
    "        elif URL_check == URL_intro and Index > 2:\n",
    "            time.sleep(4)\n",
    "            continue            \n",
    "\n",
    "        # Get the HTML info \n",
    "        HTML = browser.page_source\n",
    "\n",
    "        # Dump the HTML file\n",
    "        with open('../data/raw/' + SpeciesName + '/' + Page + '.html', \"w\") as f:\n",
    "              f.write(HTML)\n",
    "\n",
    "        # Sleep \n",
    "        time.sleep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b1ea9-fbda-4080-bbbd-13590b5adb70",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dd1e9-12bb-41b2-91cf-2530a403978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SpeciesList = glob.glob('../../data/raw/*')\n",
    "SingleList = [Species + '/Introduction.html' for Species in SpeciesList if len(glob.glob(Species + '/*')) == 1]\n",
    "\n",
    "DataBOW = collections.defaultdict(list)\n",
    "\n",
    "SourceRemover = [\n",
    "    ' \\(\\d+.+?Close\\n\\t\\n\\)'\n",
    "]\n",
    "\n",
    "for BirdXML in SingleList[0:]:\n",
    "    with open(BirdXML) as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "    \n",
    "    # Birdname\n",
    "    BirdName = soup.title.text.split(' - ')[0].lstrip()\n",
    "    \n",
    "    # Chapters\n",
    "    Paragraphs = soup.find_all('p')\n",
    "    \n",
    "    for Para in Paragraphs:\n",
    "        try:\n",
    "            Chapter = Para.find_previous_sibling().find('h2').text\n",
    "            \n",
    "            # Clean text                  \n",
    "            for Remover in SourceRemover:\n",
    "                TextCleaned = re.sub(Remover, '', Para.text, flags=re.DOTALL)\n",
    "            TextCleaned = TextCleaned.replace('\\n', '') \n",
    "            \n",
    "            if Chapter == 'Identification':\n",
    "                DataBOW[BirdName].append(tuple([1, TextCleaned]))\n",
    "            else:\n",
    "                DataBOW[BirdName].append(tuple([0, TextCleaned]))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "with open('../../data/processed/dataBOW.pkl', 'wb') as f:\n",
    "    pickle.dump(DataBOW, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227c8bb-59cd-454c-9eea-40a95a47c80c",
   "metadata": {},
   "source": [
    "## BOW Description data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c27dd9-cb49-40ea-83e4-d8bee971bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all html files\n",
    "species_folder = glob.glob('../../data/raw/BOW/*')\n",
    "multi_list = [Species + '/Appearance.html' for Species in species_folder if len(glob.glob(Species + '/*')) != 1]\n",
    "single_list = [Species + '/Introduction.html' for Species in species_folder if len(glob.glob(Species + '/*')) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758faca-9acd-4f8f-b1c2-c5f13d2357e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for html in tqdm(multi_list[0:2]):\n",
    "    # Open the html\n",
    "    with open(html) as f:\n",
    "        # Structure it\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        try:\n",
    "            # Extract name\n",
    "            name = soup.title.text.strip().split(' - ')[2]\n",
    "            # Spans\n",
    "            spans = [span.text for span in soup.find_all('p') if not 'fig' in span.text]\n",
    "            # Append\n",
    "            data_bow[name] = spans\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231b190-1a65-49b3-8421-a42b074dab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.text.strip().split(' - ')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac440d-ecd6-46c0-a5e8-b64da372740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned data\n",
    "data_bow = collections.defaultdict(list)\n",
    "\n",
    "for html in tqdm(multi_list):\n",
    "    # Open the html\n",
    "    with open(html) as f:\n",
    "        # Structure it\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        try:\n",
    "            # Extract name\n",
    "            name = soup.title.text.strip().split(' - ')[2]\n",
    "            # Spans\n",
    "            spans = [span.text for span in soup.find_all('p') if not 'fig' in span.text]\n",
    "            # Append\n",
    "            data_bow[name] = spans\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "for html in tqdm(single_list):    \n",
    "    # Open the html\n",
    "    with open(html) as f:\n",
    "        # Structure it\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        # Extract name\n",
    "        name = soup.title.text.strip().split(' - ')[1]\n",
    "        # Spans\n",
    "        for span in soup.find_all('p'):\n",
    "            #print(span.text)\n",
    "            try:\n",
    "                chapter = span.find_previous_sibling().find('h2').text\n",
    "                #print(chapter)\n",
    "                # Check if part of Description chapter\n",
    "                if chapter == 'Identification':\n",
    "                    data_bow[name] = span.text\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "with open('../data/processed/descriptions_birds_bow.pkl', 'wb') as f:\n",
    "    pickle.dump(data_bow, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32733a5e-4725-44ae-b991-d401369b5ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
