{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f21df-02e6-44a8-bb27-a5fbeace22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422cc81a-3376-4c69-b226-9dde93d0fc8e",
   "metadata": {},
   "source": [
    "## Scrape Species List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a5324-02d3-471a-bc28-906cd9848245",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://birdsoftheworld.org/bow/specieslist'\n",
    "#URL = 'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/specieslist'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "BOWlist = soup.find_all('a')\n",
    "\n",
    "URL = 'https://birdsoftheworld.org/bow/specieslist'\n",
    "#URL = 'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/specieslist'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "'''\n",
    "# Create a list with links\n",
    "BOWpages = ['https://birdsoftheworld.org' + pages.get('href').replace('introduction', '') for pages in BOWlist \n",
    "                   if pages.get('href') != None\n",
    "                   if pages.get('href').startswith('/bow/species')]\n",
    "\n",
    "'''\n",
    "# Create a list with links\n",
    "BOWpages = ['https://birdsoftheworld-org.ezproxy.library.wur.nl' + pages.get('href').replace('introduction', '') for pages in BOWlist \n",
    "                   if pages.get('href') != None\n",
    "                   if pages.get('href').startswith('/bow/species')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dddf42e-3bd6-435d-b0c8-7b14e6aebecc",
   "metadata": {},
   "source": [
    "## Open Driver (Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d31f4-c372-4c3f-a073-c1aa8f8d6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init driver\n",
    "browser = webdriver.Safari()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Login URL\n",
    "browser.get(\"https://edepot.wur.nl/541124\")\n",
    "\n",
    "time.sleep(8)\n",
    "\n",
    "# Current page\n",
    "main_page = browser.current_window_handle\n",
    "\n",
    "# Hit Okay\n",
    "Ok = browser.find_element_by_id(\"loginlinkBtn\")\n",
    "Ok.send_keys(u'\\ue007')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# changing the handles to access login page\n",
    "for handle in browser.window_handles:\n",
    "    if handle != main_page:\n",
    "        login_page = handle\n",
    "          \n",
    "# change the control to signin page        \n",
    "browser.switch_to.window(login_page)\n",
    "\n",
    "# Username\n",
    "username = browser.find_element_by_id(\"userNameInput\")\n",
    "username.clear()\n",
    "username.send_keys(\"robert.vandevlasakker@wur.nl\")\n",
    "\n",
    "# Password\n",
    "password = browser.find_element_by_name(\"Password\")\n",
    "password.clear()\n",
    "password.send_keys(\"XXXXXX\")\n",
    "\n",
    "# Hit Okay\n",
    "Ok = browser.find_element_by_id(\"submitButton\")\n",
    "Ok.send_keys(u'\\ue007')\n",
    "\n",
    "time.sleep(8)\n",
    "\n",
    "'''\n",
    "# changing the handles to access login page\n",
    "for handle in browser.window_handles:\n",
    "    if handle != main_page or handle!= login_page:\n",
    "        BOW = handle\n",
    "'''\n",
    "\n",
    "# change the control to signin page        \n",
    "browser.switch_to.window(main_page)\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadc540-6dd4-4252-9973-de5a6f715a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible pages\n",
    "Pages = ['Introduction', 'Appearance', 'Systematics', 'Distribution',\n",
    "         'Habitat', 'Movement', 'Foodhabits', 'Sounds',\n",
    "         'Behavior', 'Breeding', 'Demography', 'Conservation',\n",
    "         'Other', 'Priorities']\n",
    "\n",
    "for BirdWebPage in BOWpages[8000:]:\n",
    "    \n",
    "    # Init page\n",
    "    BirdURL = BirdWebPage\n",
    "\n",
    "    # Get the bird page\n",
    "    browser.get(BirdURL)\n",
    "\n",
    "    # Set a sleep timer\n",
    "    SleepTime = random.randint(3, 4)\n",
    "\n",
    "    # Read the XML file\n",
    "    HTML = browser.page_source\n",
    "\n",
    "    # Species name\n",
    "    soup = BeautifulSoup(HTML, 'html.parser')\n",
    "    SpeciesName = soup.title.text.split(' - ')[0].lstrip()\n",
    "\n",
    "    # Create folder\n",
    "    os.mkdir('../data/raw/' + SpeciesName)\n",
    "\n",
    "    # Dump the HTML file\n",
    "    with open('../data/raw/' + SpeciesName + '/' + Pages[0] + '.html', \"w\") as f:\n",
    "          f.write(HTML)\n",
    "\n",
    "    # Check URL for shorter account\n",
    "    URL_intro = browser.current_url\n",
    "\n",
    "    # Sleep Just in case\n",
    "    time.sleep(SleepTime)\n",
    "\n",
    "    # Loop over possible pages\n",
    "    for Index, Page in enumerate(Pages[1:]):\n",
    "\n",
    "        # Create URL\n",
    "        URL = BirdURL + Page.lower()\n",
    "        # Go to URL\n",
    "        browser.get(URL)\n",
    "\n",
    "        # Get current URL\n",
    "        URL_check = browser.current_url\n",
    "\n",
    "        # Check if short account\n",
    "        if URL_check == URL_intro and Index < 2:\n",
    "            time.sleep(4)\n",
    "            break\n",
    "        # Check if links are missing    \n",
    "        elif URL_check == URL_intro and Index > 2:\n",
    "            time.sleep(4)\n",
    "            continue            \n",
    "\n",
    "        # Get the HTML info \n",
    "        HTML = browser.page_source\n",
    "\n",
    "        # Dump the HTML file\n",
    "        with open('../data/raw/' + SpeciesName + '/' + Page + '.html', \"w\") as f:\n",
    "              f.write(HTML)\n",
    "\n",
    "        # Sleep \n",
    "        time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba0dc5-6cf7-4d58-b912-267397b669c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible pages\n",
    "Pages = ['Introduction', 'Appearance', 'Systematics', 'Distribution',\n",
    "         'Habitat', 'Movement', 'Foodhabits', 'Sounds',\n",
    "         'Behavior', 'Breeding', 'Demography', 'Conservation',\n",
    "         'Other', 'Priorities']\n",
    "\n",
    "Testpages = ['https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/bertin1/cur/',\n",
    "             'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/norcar/cur/',\n",
    "             'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/ostric2/cur/']\n",
    "\n",
    "for BirdWebPage in Testpages:\n",
    "    \n",
    "    # Init page\n",
    "    BirdURL = BirdWebPage\n",
    "\n",
    "    # Get the bird page\n",
    "    browser.get(BirdURL)\n",
    "\n",
    "    # Set a sleep timer\n",
    "    SleepTime = random.randint(3, 4)\n",
    "\n",
    "    # Read the XML file\n",
    "    HTML = browser.page_source\n",
    "\n",
    "    # Species name\n",
    "    soup = BeautifulSoup(HTML, 'html.parser')\n",
    "    SpeciesName = soup.title.text.split(' - ')[0].lstrip()\n",
    "\n",
    "    # Create folder\n",
    "    os.mkdir('../data/raw/' + SpeciesName)\n",
    "\n",
    "    # Dump the HTML file\n",
    "    with open('../data/raw/' + SpeciesName + '/' + Pages[0] + '.html', \"w\") as f:\n",
    "          f.write(HTML)\n",
    "\n",
    "    # Check URL for shorter account\n",
    "    URL_intro = browser.current_url\n",
    "    print(URL_intro)\n",
    "\n",
    "    # Sleep Just in case\n",
    "    time.sleep(SleepTime)\n",
    "\n",
    "    # Loop over possible pages\n",
    "    for Index, Page in enumerate(Pages[1:]):\n",
    "\n",
    "        # Create URL\n",
    "        URL = BirdURL + Page.lower()\n",
    "        # Go to URL\n",
    "        browser.get(URL)\n",
    "\n",
    "        # Get current URL\n",
    "        URL_check = browser.current_url\n",
    "        print(URL_check)\n",
    "\n",
    "        # Check if short account\n",
    "        if URL_check == URL_intro and Index < 2:\n",
    "            time.sleep(4)\n",
    "            break\n",
    "        # Check if links are missing    \n",
    "        elif URL_check == URL_intro and Index > 2:\n",
    "            time.sleep(4)\n",
    "            continue            \n",
    "\n",
    "        # Get the HTML info \n",
    "        HTML = browser.page_source\n",
    "\n",
    "        # Dump the HTML file\n",
    "        with open('../data/raw/' + SpeciesName + '/' + Page + '.html', \"w\") as f:\n",
    "              f.write(HTML)\n",
    "\n",
    "        # Sleep \n",
    "        time.sleep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b1ea9-fbda-4080-bbbd-13590b5adb70",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dd1e9-12bb-41b2-91cf-2530a403978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SpeciesList = glob.glob('../../data/raw/*')\n",
    "SingleList = [Species + '/Introduction.html' for Species in SpeciesList if len(glob.glob(Species + '/*')) == 1]\n",
    "\n",
    "DataBOW = collections.defaultdict(list)\n",
    "\n",
    "SourceRemover = [\n",
    "    ' \\(\\d+.+?Close\\n\\t\\n\\)'\n",
    "]\n",
    "\n",
    "for BirdXML in SingleList[0:]:\n",
    "    with open(BirdXML) as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "    \n",
    "    # Birdname\n",
    "    BirdName = soup.title.text.split(' - ')[0].lstrip()\n",
    "    \n",
    "    # Chapters\n",
    "    Paragraphs = soup.find_all('p')\n",
    "    \n",
    "    for Para in Paragraphs:\n",
    "        try:\n",
    "            Chapter = Para.find_previous_sibling().find('h2').text\n",
    "            \n",
    "            # Clean text                  \n",
    "            for Remover in SourceRemover:\n",
    "                TextCleaned = re.sub(Remover, '', Para.text, flags=re.DOTALL)\n",
    "            TextCleaned = TextCleaned.replace('\\n', '') \n",
    "            \n",
    "            if Chapter == 'Identification':\n",
    "                DataBOW[BirdName].append(tuple([1, TextCleaned]))\n",
    "            else:\n",
    "                DataBOW[BirdName].append(tuple([0, TextCleaned]))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "with open('../../data/processed/dataBOW.pkl', 'wb') as f:\n",
    "    pickle.dump(DataBOW, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227c8bb-59cd-454c-9eea-40a95a47c80c",
   "metadata": {},
   "source": [
    "## BOW Description data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c27dd9-cb49-40ea-83e4-d8bee971bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all html files\n",
    "species_folder = glob.glob('../../data/raw/BOW/*')\n",
    "multi_list = [Species + '/Appearance.html' for Species in species_folder if len(glob.glob(Species + '/*')) != 1]\n",
    "single_list = [Species + '/Introduction.html' for Species in species_folder if len(glob.glob(Species + '/*')) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758faca-9acd-4f8f-b1c2-c5f13d2357e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for html in tqdm(multi_list[0:2]):\n",
    "    # Open the html\n",
    "    with open(html) as f:\n",
    "        # Structure it\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        try:\n",
    "            # Extract name\n",
    "            name = soup.title.text.strip().split(' - ')[2]\n",
    "            # Spans\n",
    "            spans = [span.text for span in soup.find_all('p') if not 'fig' in span.text]\n",
    "            # Append\n",
    "            data_bow[name] = spans\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231b190-1a65-49b3-8421-a42b074dab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.text.strip().split(' - ')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac440d-ecd6-46c0-a5e8-b64da372740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned data\n",
    "data_bow = collections.defaultdict(list)\n",
    "\n",
    "for html in tqdm(multi_list):\n",
    "    # Open the html\n",
    "    with open(html) as f:\n",
    "        # Structure it\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        try:\n",
    "            # Extract name\n",
    "            name = soup.title.text.strip().split(' - ')[2]\n",
    "            # Spans\n",
    "            spans = [span.text for span in soup.find_all('p') if not 'fig' in span.text]\n",
    "            # Append\n",
    "            data_bow[name] = spans\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "for html in tqdm(single_list):    \n",
    "    # Open the html\n",
    "    with open(html) as f:\n",
    "        # Structure it\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        # Extract name\n",
    "        name = soup.title.text.strip().split(' - ')[1]\n",
    "        # Spans\n",
    "        for span in soup.find_all('p'):\n",
    "            #print(span.text)\n",
    "            try:\n",
    "                chapter = span.find_previous_sibling().find('h2').text\n",
    "                #print(chapter)\n",
    "                # Check if part of Description chapter\n",
    "                if chapter == 'Identification':\n",
    "                    data_bow[name] = span.text\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "with open('../data/processed/descriptions_birds_bow.pkl', 'wb') as f:\n",
    "    pickle.dump(data_bow, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118f7e4-279b-43b7-abcb-522df6ce86ca",
   "metadata": {},
   "source": [
    "# BOW Sentences per Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d9c60e-fb62-4f1d-afa0-530fee92e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/models/')\n",
    "sys.path.insert(0, '../../src/features/')\n",
    "#sys.path.insert(0, '../src/visualization/')\n",
    "\n",
    "import predict_model\n",
    "from build_features import text_cleaner\n",
    "#import visualize as vis\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee34ac5-1319-429d-9482-3b0586fbe991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "model = predict_model.loadBERT(\"../../models/\", 'saved_weights_inf_FIXED_boot.pt')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "def SpanPredictor(span, pred_values=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "         \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(**inputs)\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        # Print the prediction values\n",
    "        if pred_values:\n",
    "            return span_class, exps[0]\n",
    "        else:\n",
    "            return span_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4531d4-319c-4029-bb6d-aef12e394b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all html files\n",
    "species_folder = glob.glob('../../data/raw/BOW/*')\n",
    "multi_list = [Species + '/Appearance.html' for Species in species_folder if len(glob.glob(Species + '/*')) != 1]\n",
    "single_list = [Species + '/Introduction.html' for Species in species_folder if len(glob.glob(Species + '/*')) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1438f59f-fd48-4709-a998-16a39eb14d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9090/9090 [5:01:34<00:00,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "data = collections.defaultdict(list)\n",
    "\n",
    "for html in tqdm(single_list):\n",
    "    with open(html) as f:\n",
    "        # Structure it\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        # Extract name\n",
    "        species = soup.title.text.strip().split(' - ')[1]\n",
    "        # Find all non Identification Spans\n",
    "        spans = [span for span in soup.find_all('p') \n",
    "                 if not 'fig' in span.text]\n",
    "        for span in spans:\n",
    "            if span.find_previous_sibling() != None:\n",
    "                if span.find_previous_sibling().find('h2') != None:\n",
    "                    # Locate Identification\n",
    "                    if span.find_previous_sibling().find('h2').text == 'Identification':\n",
    "                        text_id = span.text.strip().replace('\\n', \"\").replace('; ', '. ')\n",
    "                        text_id = re.sub(' +',' ', text_id)\n",
    "                        text_list = text_id.split('. ')\n",
    "                        for sentence in text_list:\n",
    "                            if SpanPredictor(sentence):\n",
    "                                #print(URL)\n",
    "                                data[species].append(sentence)\n",
    "                    else:\n",
    "                        sentences = text_cleaner(span.text)\n",
    "                        # Loop over the individual sentences\n",
    "                        for sentence in sentences:                    \n",
    "                            # Create string object\n",
    "                            sentence_str = str(sentence)\n",
    "                            #print(sentence_str)\n",
    "\n",
    "                            if SpanPredictor(sentence_str):\n",
    "                                #print(URL)\n",
    "                                data[species].append(sentence_str)\n",
    "                else:\n",
    "                    sentences = text_cleaner(span.text)\n",
    "                    # Loop over the individual sentences\n",
    "                    for sentence in sentences:                    \n",
    "                        # Create string object\n",
    "                        sentence_str = str(sentence)\n",
    "                        #print(sentence_str)\n",
    "\n",
    "                        if SpanPredictor(sentence_str):\n",
    "                            #print(URL)\n",
    "                            data[species].append(sentence_str)\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45eb0e24-0c78-4f2f-922c-e3a3596d021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/scrapeddata_train_species_description_bow.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "307aec4d-8ff7-4bc7-8e57-2c7c525636f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Juvenile has head to throat, breast and flanks dark brown, supercilium tinged yellowish, underparts pale yellow.',\n",
       " 'Race chloronota is greener above than nominate, with more extensive chestnut-brown on cheeks and ear-coverts.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sylvietta leucophrys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b632dad-acf5-46e5-96ff-afcc33395210",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428b4ea-1947-4552-8918-c03d53a21108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f000bd0-3bbd-4871-ab7f-54f61145db8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
