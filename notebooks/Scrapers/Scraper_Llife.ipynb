{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d364341-b3ec-41c7-84a1-5c696fde676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "import requests\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e04d92-e44e-4630-a9a4-4d6a32d696a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init index list\n",
    "tree_links_index = []\n",
    "# Extract index pages\n",
    "for i in range(1, 8):\n",
    "    tree_links_index.append('http://www.llifle.com/Encyclopedia/TREES/Species/all/{0}/100/'.format(i))\n",
    "\n",
    "# Init empty list\n",
    "tree_links = []\n",
    "\n",
    "for index_pages in tqdm(tree_links_index):\n",
    "    # Extract XML\n",
    "    URL = index_pages\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Extract links incomplete\n",
    "    tree_links_half = soup.find_all('a')\n",
    "\n",
    "    # Complete the links\n",
    "    tree_links_temp = ['http://www.llifle.com' + pages.get('href') for pages in tree_links_half\n",
    "                           if pages.get('href') != None \n",
    "                           if pages.get('href').startswith('/Encyclopedia/TREES/Family/')]\n",
    "    # Add to all trees\n",
    "    tree_links += tree_links_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece7989-3257-4680-a7a0-08055a6136d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/intermediate_web_llifle_treelist.pkl', 'wb') as f:\n",
    "    pickle.dump(tree_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9b88e-cefe-447c-88d6-7ed5e328b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty dict\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "for URL in tqdm(tree_links):\n",
    "\n",
    "    # Get Page\n",
    "    page = requests.get(URL)\n",
    "    # Structure page\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Name\n",
    "    name = soup.title.text.replace('\\n', '')\n",
    "    # Loop over text\n",
    "    for text in soup.find_all('p'):\n",
    "        try:\n",
    "            if text.find('b').text.strip().startswith('Description'):\n",
    "                data[name].append(tuple([1, text.text]))\n",
    "            else:\n",
    "                data[name].append(tuple([0, text.text]))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6cd07-5c17-4375-b054-141beb82bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/descriptions_web_trees_llifle.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f289632-681b-46ea-b260-8aa0d907386f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2127d5-f2c3-401d-875b-adb22f5d451b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb17cc7f-69da-44c8-a1c5-0e5c1bb46ceb",
   "metadata": {},
   "source": [
    "# V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c5e42-61ac-4677-be1f-004941b6e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9cdc5-395b-450b-a2ce-27c6b5a6d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init index list\n",
    "links_index = []\n",
    "# Extract TREES index pages\n",
    "for i in range(1, 8):\n",
    "    links_index.append('http://www.llifle.com/Encyclopedia/TREES/Species/all/{0}/100/'.format(i))\n",
    "for i in range(1, 8):\n",
    "    links_index.append('http://www.llifle.com/Encyclopedia/BROMELIADS/Species/all/{0}/100/'.format(i))    \n",
    "for i in range(1, 16):\n",
    "    links_index.append('http://www.llifle.com/Encyclopedia/PALMS%20AND%20CYCADS/Species/all/{0}/100/'.format(i))    \n",
    "for i in range(1, 11):\n",
    "    links_index.append('http://www.llifle.com/Encyclopedia/BULBS/Species/all/{0}/100/'.format(i)) \n",
    "for i in range(1, 179):\n",
    "    links_index.append('http://www.llifle.com/Encyclopedia/CACTI/Species/all/{0}/100/'.format(i)) \n",
    "for i in range(1, 139):\n",
    "    links_index.append('http://www.llifle.com/Encyclopedia/SUCCULENTS/Species/all/{0}/100/'.format(i))       \n",
    "    \n",
    "    \n",
    "# Init empty list\n",
    "links = []\n",
    "\n",
    "for index_pages in tqdm_notebook(links_index[0:]):\n",
    "    # Extract XML\n",
    "    URL = index_pages\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    baselinks = soup.find_all('a')\n",
    "    \n",
    "    for baselink in baselinks:\n",
    "        try:\n",
    "            endlink = baselink['href']\n",
    "            full_link = 'http://www.llifle.com' + endlink\n",
    "            links.append(full_link)\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640f0fc-ef19-4b3a-b700-2699d770de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/intermediate_web_llifle_links.pkl', 'wb') as f:\n",
    "    pickle.dump(links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61dfd5-2ad4-4598-9503-181b125df650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty dict\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "for URL in tqdm_notebook(links[0:]):\n",
    "    try:\n",
    "        # Get Page\n",
    "        page = requests.get(URL)\n",
    "        # Structure page\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        # Name\n",
    "        name = soup.title.text.replace('\\n', '')\n",
    "        # Loop over text\n",
    "        strings = [string for string in soup.stripped_strings if len(string.split()) > 10]\n",
    "        for string in strings:\n",
    "            doc = nlp(string)\n",
    "            for sentence in doc.sents:\n",
    "                data[name].append((URL, sentence.text))\n",
    "    except:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d54eed-5860-4950-883c-31881bb56c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/descriptions_llifeV2_PLANTS.pkl.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2fcfc-aeb9-4493-802e-a9a907d2d13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
