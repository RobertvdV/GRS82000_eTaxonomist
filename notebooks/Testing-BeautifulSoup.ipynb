{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init dict\n",
    "Data = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for database\n",
    "#URL = 'https://en.wikipedia.org/wiki/List_of_birds_of_the_Netherlands'\n",
    "#URL = 'https://en.wikipedia.org/wiki/List_of_mammals_of_the_Netherlands'\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_mammals_of_Europe'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find all wikiparts\n",
    "Animals = soup.find_all('a')\n",
    "# Create links \n",
    "AnimalsWikiPages = ['https://en.wikipedia.org/' + pages.get('href') for pages in Animals \n",
    "                       if pages.get('href') != None \n",
    "                       if pages.get('href').startswith('/wiki/')]\n",
    "                       # Reduces the retrieved pages (does not work)\n",
    "                       #if pages.span != None \n",
    "                       #if pages.span.attrs['class'][0] == 'tocnumber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init dicts\n",
    "DescriptionData = {}\n",
    "NonDescriptionData = {}\n",
    "\n",
    "# Removes references in text\n",
    "ReferenceRemover = '\\[\\d*\\]'\n",
    "\n",
    "for WikiPage in AnimalsWikiPages[:]:\n",
    "    \n",
    "    # Open the page\n",
    "    page = requests.get(WikiPage, timeout=5)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    for Tags in soup.find_all('h2'):\n",
    "\n",
    "        # Skip useless/empty stuff\n",
    "        if Tags.span == None:\n",
    "            continue\n",
    "\n",
    "        # Set chapter variable    \n",
    "        Chapter = Tags.span.attrs['id']\n",
    "\n",
    "        # Check if the chapter is description (or similar)\n",
    "        if Chapter == 'Characteristics'or \\\n",
    "           Chapter == 'Description' or \\\n",
    "           Chapter == 'Appearance':\n",
    "\n",
    "\n",
    "            # Get species name\n",
    "            Species = soup.title\\\n",
    "                            .string\\\n",
    "                            .split(' - ')[0]\\\n",
    "                            .rstrip(' ')\n",
    "\n",
    "\n",
    "            # Get the next sibling (text)\n",
    "            for Text in Tags.find_next_siblings('p'):\n",
    "\n",
    "                # Add description data to dict\n",
    "                if Chapter in Text.find_previous_siblings('h2')[0].text.strip():\n",
    "                    # Remove source\n",
    "                    Paragraph = re.sub(ReferenceRemover, '', Text.text)\n",
    "                    # Split into Sentences\n",
    "                    SentenceList = Paragraph.split('. ')\n",
    "                    # Add to the dict\n",
    "                    Data[Species] += [(1, Sentence) for Sentence in SentenceList]\n",
    "\n",
    "                # Add non description data to dict\n",
    "                elif Chapter not in Text.find_previous_siblings('h2')[0].text.strip():\n",
    "                    # Remove source\n",
    "                    Paragraph = re.sub(ReferenceRemover, '', Text.text)\n",
    "                    # Split into Sentences\n",
    "                    SentenceList = Paragraph.split('. ')\n",
    "                    # Add to the dict\n",
    "                    Data[Species] += [(0, Sentence) for Sentence in SentenceList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump(Data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animals A-Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://a-z-animals.com/animals/'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Finds all links\n",
    "Animals = soup.find_all('a')\n",
    "\n",
    "# Create a list with links\n",
    "AnimalsA_Zpages = [pages.get('href') for pages in Animals \n",
    "                   if pages.get('href') != None\n",
    "                   if pages.get('href').startswith('https://a-z-animals.com')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes references in text\n",
    "ReferenceRemover = '\\[\\d*\\]'\n",
    "\n",
    "for AnimalPage in AnimalsA_Zpages:\n",
    "    \n",
    "    # Open the page\n",
    "    page = requests.get(AnimalPage, timeout=5)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    for Tags in soup.find_all('h2'):\n",
    "\n",
    "        # Get the chapters\n",
    "        try:\n",
    "            Chapter = Tags['id']\n",
    "        # skip other stuff\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Finds descriptions or similar\n",
    "        if Chapter.endswith('appearance'):\n",
    "            \n",
    "            '''\n",
    "            # Get species name\n",
    "            Species = soup.title\\\n",
    "                            .string\\\n",
    "                            .split(' Animal')[0]\\\n",
    "                            .lower()\\\n",
    "                            .capitalize()\n",
    "            '''\n",
    "            \n",
    "            # Get species name\n",
    "            Species = soup.find('h1').text\\\n",
    "                                        .lower()\\\n",
    "                                        .capitalize()\n",
    "\n",
    "\n",
    "            # Get the next sibling (text)\n",
    "            for Text in Tags.find_next_siblings('p'):\n",
    "\n",
    "                # Add description data to dict\n",
    "                if Chapter in Text.find_previous_siblings('h2')[0]['id']:\n",
    "                    # Remove source\n",
    "                    Paragraph = re.sub(ReferenceRemover, '', Text.text)\n",
    "                    # Split into Sentences\n",
    "                    SentenceList = Paragraph.split('. ')\n",
    "                    # Add to the dict\n",
    "                    Data[Species] += [(1, Sentence) for Sentence in SentenceList]\n",
    "\n",
    "                # Add non description data to dict\n",
    "                elif Chapter not in Text.find_previous_siblings('h2')[0]['id']:\n",
    "                    # Remove source\n",
    "                    Paragraph = re.sub(ReferenceRemover, '', Text.text)\n",
    "                    # Split into Sentences\n",
    "                    SentenceList = Paragraph.split('. ')\n",
    "                    # Add to the dict\n",
    "                    Data[Species] += [(0, Sentence) for Sentence in SentenceList]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump(Data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total values\n",
    "Total = sum(len(v) for v in list(Data.values()))\n",
    "print(Total)\n",
    "# Total Truths\n",
    "TotalValues = [v for v in list(Data.values())]\n",
    "Truths = [[ones[0] for ones in Values if ones[0] == 1] for Values in TotalValues]\n",
    "AmountofTruths = sum(len(x) for x in Truths)\n",
    "AmountofFalse = Total - AmountofTruths\n",
    "print(AmountofTruths)\n",
    "print(AmountofFalse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birds of the World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://birdsoftheworld.org/bow/specieslist'\n",
    "#URL = 'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/specieslist'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "BOWlist = soup.find_all('a')\n",
    "\n",
    "URL = 'https://birdsoftheworld.org/bow/specieslist'\n",
    "#URL = 'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/specieslist'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Create a list with links\n",
    "BOWpages = ['https://birdsoftheworld.org' + pages.get('href').replace('introduction', '') for pages in BOWlist \n",
    "                   if pages.get('href') != None\n",
    "                   if pages.get('href').startswith('/bow/species')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOWpages[3333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOWpages.index('https://birdsoftheworld.org/bow/species/norcar/cur/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL = BOWpages[10566] + 'appearance'\n",
    "URL = 'https://birdsoftheworld.org/bow/species/anteup1/cur/introduction'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start the session\n",
    "session = requests.Session()\n",
    "\n",
    "# Create the payload\n",
    "payload = {'username':'RobertvdV', \n",
    "           'password':'XXXXXXXXX',\n",
    "         }\n",
    "\n",
    "agent = {\"User-Agent\": 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}\n",
    "\n",
    "# Load the page\n",
    "page = session.get(\"https://birdsoftheworld.org/bow/species/citwar1/cur/introduction?login\")\n",
    "\n",
    "# Post the payload to the site to log in\n",
    "page = session.post(\"https://secure.birds.cornell.edu/cassso/login?service=https%3A%2F%2Fbirdsoftheworld.org%2Flogin%2Fcas\", \n",
    "                    data=payload,\n",
    "                    headers=agent)\n",
    "\n",
    "\n",
    "# Navigate to the next page and scrape the data\n",
    "page = session.get('https://birdsoftheworld.org/bow/species/citwar1/cur/introduction?login')\n",
    "\n",
    "session.close()\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dict\n",
    "DataBOW = collections.defaultdict(list)\n",
    "\n",
    "# Reference Remover\n",
    "ReferenceRemover1 = ' \\(\\d+\\n\\t.+?\\n\\t\\n\\t\\tClose\\n\\t\\n\\)'\n",
    "ReferenceRemover2 = ' \\[\\d+\\n\\t.+?\\n\\t\\n\\t\\tClose\\n\\t\\n\\]'\n",
    "# Brackets Remover\n",
    "BracketsRemover = ' \\((.*?)\\)'\n",
    "\n",
    "# Bird name \n",
    "BirdName = soup.title\\\n",
    "                .text\\\n",
    "                .split(' - ')[1]\\\n",
    "                .lstrip()\n",
    "\n",
    "# Find all text data\n",
    "Paragraphs = soup.find_all('p')\n",
    "\n",
    "# Loop over the text\n",
    "for Para in Paragraphs[1:]:\n",
    "    try:\n",
    "        \n",
    "        # Check to which section the text belongs\n",
    "        SubSection = Para.find_previous_siblings()[0]\\\n",
    "                         .text\\\n",
    "                         .rstrip()\\\n",
    "                         .lstrip()\n",
    "        \n",
    "        # Check to which section the text belongs\n",
    "        SubSubSection = Para.find_previous_siblings()[-1]\\\n",
    "                             .text\\\n",
    "                             .rstrip()\\\n",
    "                             .lstrip()\n",
    "        \n",
    "    # If a text does not have a header, skip    \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    if 'Formative' in SubSection:\n",
    "        continue\n",
    "    if 'Juvenile' in SubSection:\n",
    "        continue\n",
    "    \n",
    "    # See which are needed and append with 1\n",
    "    if SubSection == 'Identification' or\\\n",
    "       SubSection == 'Adult' or\\\n",
    "       SubSubSection == 'Plumages' or\\\n",
    "       SubSubSection == 'Bare Parts':\n",
    "          \n",
    "        # Clean the text from references (NEEDS EDIT)    \n",
    "        TextCleaned = re.sub(ReferenceRemover1, '', Para.text, re.DOTALL)\n",
    "        TextCleaned = re.sub(ReferenceRemover2, '', TextCleaned, re.DOTALL)\n",
    "        TextCleaned = re.sub(BracketsRemover, '', TextCleaned)\n",
    "        \n",
    "        # Drop useless short nonsense\n",
    "        if len(TextCleaned.split()) <2:\n",
    "            continue\n",
    "        else:\n",
    "            DataBOW[BirdName] += (1, TextCleaned)\n",
    "            \n",
    "    # Otherwise label 2        \n",
    "    else:\n",
    "        # Clean the text from references (NEEDS EDIT)    \n",
    "        TextCleaned = re.sub(ReferenceRemover1, '', Para.text, re.DOTALL)\n",
    "        TextCleaned = re.sub(ReferenceRemover2, '', TextCleaned, re.DOTALL)\n",
    "        TextCleaned = re.sub(BracketsRemover, '', TextCleaned)\n",
    "        \n",
    "        # Drop useless short nonsense\n",
    "        if len(TextCleaned.split()) <2:\n",
    "            continue\n",
    "        else:\n",
    "            DataBOW[BirdName] += (0, TextCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataBOW[BirdName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Start the session\n",
    "session = requests.Session()\n",
    "\n",
    "# Create the payload\n",
    "payload = {'UserName':'robert.vandevlasakker@wur.nl', \n",
    "           'Password':'Hints4-hung###'\n",
    "         }\n",
    "\n",
    "agent = {\"User-Agent\": 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}\n",
    "\n",
    "# Load the WUR library page\n",
    "#WURpage = session.get('https://login.wur.nl/adfs/ls/?SAMLRequest=lZJBb8IwDIX%2FSpU7TWkZpRFUYnAYEhsIuh12C6mBSGnSxclg%2F35Z2TR2QdrV8ffs95wx8ka1bOrdUW%2FgzQO66Nwojax7mBBvNTMcJTLNG0DmBNtOH5csjRPWWuOMMIpcIbcJjgjWSaNJtJhPyGy1qVaCZ3dpzYf9NC%2BKJMt5XQ%2Bzuj%2FKinq33w1EkXFIBmJIohewGNgJCVJBANHDQqPj2oVSkvZ7Sd5LRlWSszRlWf%2BVRPPgR2ruOuroXIuMUmUOUscnb2OtKK%2F3SBVSEk1%2FdpsZjb4BuwX7LgU8b5a%2FLOjAQoze7oXRcHadRkgPtJOiG0SxpeIi0bvyu%2F4O617qWurD7Zx2lyZkD1W17q1X24qU4694Wefalv%2FcpwHHa%2B74mF6LjC%2FHfwrjF%2FO1UVJ8RFOlzGlmgTuYEGc9EFpeqL%2B%2FpPwE')\n",
    "\n",
    "# Load a WUR login sessions\n",
    "# Post the payload to the site to log in\n",
    "login = session.post(\"https://login.wur.nl/adfs/ls/?SAMLRequest=lZJBb8IwDIX%2FSpV7SRtGKVGLxOAwJDYQdDvsFlIXIqUJi9PB%2Fv26dtPYBWlXx9%2Bz33MyFLU%2B8Vnjj2YLbw2gDy61Nsi7h5w0znArUCE3ogbkXvLd7HHF2SDiJ2e9lVaTK%2BQ2IRDBeWUNCZaLnMzX22KdVHvGhqlIRRRPYpZE8UiyskqTSRWVd%2Fs4TRmbJHJIghdw2LI5aaVaAcQGlga9ML4tRSwOo3EYpUU05mzEWfJKgkXrRxnhO%2Bro%2FQk5pdoelBmcGzcwmoqyQqqRkmD2s9vcGmxqcDtw70rC83b1y4JpWRhg4yppDVx8p9GmB8Yr2Q2ieKKylwiv%2FG6%2Bw7pXplTmcDunfd%2BE%2FKEoNuFmvSvINPuKl3eu3fSf%2B9TgRSm8yOi1SNYf%2F6kdv1xsrFbyI5hpbc9zB8JDTrxrgNBpT%2F39JdNP\", \n",
    "                    data=payload,\n",
    "                    headers=agent)\n",
    "\n",
    "\n",
    "# Navigate to the next page and scrape the data\n",
    "#page = session.get('https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/home')\n",
    "page = session.get('https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/anteup1/cur/appearance')\n",
    "\n",
    "login = session.post(\"https://login.wur.nl/adfs/ls/?SAMLRequest=lZJBb8IwDIX%2FSpV7SRtGKVGLxOAwJDYQdDvsFlIXIqUJi9PB%2Fv26dtPYBWlXx9%2Bz33MyFLU%2B8Vnjj2YLbw2gDy61Nsi7h5w0znArUCE3ogbkXvLd7HHF2SDiJ2e9lVaTK%2BQ2IRDBeWUNCZaLnMzX22KdVHvGhqlIRRRPYpZE8UiyskqTSRWVd%2Fs4TRmbJHJIghdw2LI5aaVaAcQGlga9ML4tRSwOo3EYpUU05mzEWfJKgkXrRxnhO%2Bro%2FQk5pdoelBmcGzcwmoqyQqqRkmD2s9vcGmxqcDtw70rC83b1y4JpWRhg4yppDVx8p9GmB8Yr2Q2ieKKylwiv%2FG6%2Bw7pXplTmcDunfd%2BE%2FKEoNuFmvSvINPuKl3eu3fSf%2B9TgRSm8yOi1SNYf%2F6kdv1xsrFbyI5hpbc9zB8JDTrxrgNBpT%2F39JdNP\", \n",
    "                    data=payload,\n",
    "                    headers=agent)\n",
    "\n",
    "page = session.get('https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/anteup1/cur/appearance')\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
