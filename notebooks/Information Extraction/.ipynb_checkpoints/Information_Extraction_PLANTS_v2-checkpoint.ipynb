{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c838a5-be6b-4ef9-9611-ba3840147e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from matplotlib.colors import is_color_like as color_check\n",
    "import requests\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "from spacy import displacy\n",
    "import collections\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, logging\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as colors\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db09655f-5a96-4b37-8ba0-842e3735ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "URL = 'https://en.wikipedia.org/wiki/Glossary_of_plant_morphology'\n",
    "# Get the page\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, \"lxml\", from_encoding=\"iso-8859-1\")   \n",
    "\n",
    "glossary = collections.defaultdict(list)\n",
    "# Find all H4 \n",
    "for chapter in soup.find_all('h4')[0:]:\n",
    "    # Clean\n",
    "    chapter_text = chapter.text.rstrip('[edit]')\n",
    "    # Find all siblings\n",
    "    for sibling in chapter.find_next_siblings():\n",
    "        # Find the parent\n",
    "        for parent in sibling.find_previous_sibling('h4'):\n",
    "            # Only append if correspond to current chapter\n",
    "            if parent.text == chapter_text:\n",
    "                if 'â' in sibling.text:\n",
    "                    for tag in sibling.find_all('li'):\n",
    "                        candidates = tag.text.split('â')[0]\n",
    "                        candidates = candidates.split('/')\n",
    "                        for candidate in candidates:\n",
    "                            glossary[chapter_text.lower()].append(candidate.strip().lower())  \n",
    "                            \n",
    "glossary['leaves'] += [\n",
    "    'glume', 'surface', 'margin',\n",
    "    'leaves', 'auricles', 'spatheole',\n",
    "    'ovate', 'lanceolate',\n",
    "]\n",
    "\n",
    "glossary['basic flower parts'] += [\n",
    "    'floret', 'awn',\n",
    "    'pod', 'lobe', \n",
    "    'capitulum', 'capitula', # unkown\n",
    "    'legume', 'calyx', \n",
    "]\n",
    "glossary['inflorescences'] += [\n",
    "    'spikelets', 'lemma', 'racemes',\n",
    "    'axis', 'cluster', \n",
    "]\n",
    "glossary['leaves'] += [\n",
    "    'rhachilla',\n",
    "    'needles',\n",
    "]\n",
    "\n",
    "glossary['other parts'] += [\n",
    "    'apex', 'culm', 'tube',\n",
    "    'palea', 'crown', 'canopy',\n",
    "    'base', 'callus', 'hair',\n",
    "    'anther', 'tuberculate', 'cone',\n",
    "    'shoot', 'gland',\n",
    "\n",
    "]\n",
    "\n",
    "glossary['plant property'] += [\n",
    "    'tree', 'shrub', 'plant',\n",
    "    'life-span', 'life', 'span',\n",
    "    'bloom-time', 'species', 'wood', 'timber',\n",
    "    'color', 'colour', \n",
    "    \n",
    "]\n",
    "\n",
    "glossary['stems'] += [\n",
    "    'branchlet', \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f244d93b-d6ba-42ce-b9ff-42d596835f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['morphology', 'roots', 'stems', 'buds', 'leaves', 'basic flower parts', 'inflorescences', 'insertion of floral parts', 'union of flower parts', 'flower sexuality and presence of floral parts', 'flower symmetry', 'terms for fruits', 'fruit types', 'pteridophytes', 'bryophytes', 'other parts', 'plant property'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6534cd43-64ab-45fb-8a95-4135f7879b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = [\n",
    "    'fertile', 'sterile',\n",
    "    'male', 'female', 'bisexual',\n",
    "    'basal', 'developed', \n",
    "    'primary', 'secondary', 'main',\n",
    "    'upper', 'lower', 'greater', 'dorsal', 'alternate', 'lesser', 'apex', 'outer',\n",
    "    'central', 'outermost', 'outer', 'inner', 'uppermost', 'median', 'dorsal', 'central', 'lateral',\n",
    "    'young', 'mature', 'individual', \n",
    "    'opposite', \n",
    "]\n",
    "\n",
    "rubbish = [\n",
    "    '.', ',', '-', '..', '...',\n",
    "]\n",
    "\n",
    "measurements = [\n",
    "    'mm', 'cm', 'm', 'km',\n",
    "    'milimeter', 'centimeter', 'meter', 'kilometer',\n",
    "    'milimetre', 'centimetre', 'metre', 'kilometre',\n",
    "    'inch', 'foot', 'yard', 'mile',\n",
    "    'wide', 'long', 'broad', 'tall',\n",
    "    'length', 'form',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbeebdd-5d79-4ed5-aae8-965d306a5195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_reconstructor(token, doc):\n",
    "    if token.i == 0:\n",
    "        trait = token\n",
    "    elif doc[token.i - 1].pos_ == 'DET':\n",
    "        trait = token\n",
    "    elif doc[token.i - 3].dep_ == 'compound':\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 3].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 2].dep_ == 'compound':\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 2].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 1].dep_ == 'compound':\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    elif doc[token.i - 1].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    else:\n",
    "        trait = token\n",
    "    if ','  in trait.lemma_:\n",
    "        trait = token\n",
    "    return trait.lemma_    \n",
    "\n",
    "def check_existance(t, doc):\n",
    "    \n",
    "    if t.i + 1 < len(doc) and doc[t.i + 1].lemma_ == '-':\n",
    "        return None\n",
    "    # Check prep\n",
    "    single = next((key for key, value in glossary.items() if t.lemma_.lower() in value), None)\n",
    "    multi = next((key for key, value in glossary.items() if t.text.lower() in value), None)\n",
    "    if single:\n",
    "        return single\n",
    "    elif multi:\n",
    "        return multi\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def text_preparation(species, text):\n",
    "    \n",
    "    cleaners = [(r'(?<!\\d)\\.(?!\\d)', ' '),\n",
    "                (r'\\s×\\s', ' times '),\n",
    "                #(r'\\xa0', ' '),\n",
    "                (r'\\s+c\\s+', ' '),\n",
    "                (r'â\\x80\\x93', ' to '),\n",
    "                (r'\\xa0', ' '),\n",
    "                (r'\\x97', ''),]\n",
    "    \n",
    "    species_parts = species.split()\n",
    "    candidates = [' '.join(species_parts[:idx+1]) for idx, _ in enumerate(species_parts)]\n",
    "    candidates += [\n",
    "        f'{species_parts[0][0]}. {species_parts[1]}'\n",
    "    ]\n",
    "    candidates.reverse()\n",
    "    for candidate in candidates:\n",
    "        try:\n",
    "            text = re.sub(candidate, 'the species', text)\n",
    "        except:\n",
    "            continue # Skip species with brackets for now\n",
    "    for (cleaner, replacement) in cleaners:\n",
    "        text = re.sub(cleaner, replacement, text)    \n",
    "    text = f'{text.strip()}.'\n",
    "    \n",
    "    return text.capitalize()\n",
    "\n",
    "\n",
    "def extract_modifiers(t, doc):\n",
    "    if t.text.lower() not in compounds:\n",
    "        if child.dep_ in ['amod', 'nummod', 'appos', 'acl',]:\n",
    "            return doc[child.left_edge.i : child.right_edge.i + 1]\n",
    "    \n",
    "        \n",
    "def create_relation(t):\n",
    "    relation = ''\n",
    "    if t in measurements or list(set(t.split()) & set(measurements)):\n",
    "        relation = 'measures'  #'measurement'\n",
    "    elif t.isdigit():\n",
    "        relation = 'has number'\n",
    "    elif color_check(t) or color_check(t.split()[-1]) or color_check(t.split('-')[-1]):\n",
    "        relation = 'has color'\n",
    "    else:\n",
    "        relation = 'is' # Property\n",
    "        \n",
    "    return relation\n",
    "        \n",
    "def clean_object(t):\n",
    "    objects = ''\n",
    "    if t.root.pos_ == 'NOUN' and t.root.lemma_ not in measurements:\n",
    "        objects = t.root.lemma_\n",
    "    else:\n",
    "        if len(t) > 1:\n",
    "            objects =  t.text\n",
    "        elif t.root.pos_ == 'VERB':\n",
    "            objects = t.text\n",
    "        else:\n",
    "            objects = t.lemma_\n",
    "    \n",
    "    objects =  re.split(',| and | or | with ', objects)\n",
    "    return [obj.strip() for obj in objects if obj if obj not in rubbish]\n",
    "    #print(len(t))\n",
    "\n",
    "def extract_verb(t, doc):\n",
    "    if t.dep_  == 'nsubj':\n",
    "        return next((parent for parent in t.ancestors if parent.pos_ == 'VERB' or parent.pos_ == 'AUX'), None)\n",
    "\n",
    "def extract_verbal_modifier(t, doc):\n",
    "    if t.text.lower() not in compounds:\n",
    "        if child.dep_ in [\"acomp\", \"dobj\", \"prep\",]:\n",
    "            return doc[child.left_edge.i : child.right_edge.i + 1]    \n",
    "    \n",
    "def create_main_triples(part, trait, obj):\n",
    "    triples = []\n",
    "    triples.append(('species', 'has main part', part.lower()))\n",
    "    triples.append((part.lower(), f'has part', trait.lower()))\n",
    "    for o in obj:\n",
    "        rel = create_relation(o)\n",
    "        triples.append((trait.lower(), rel.lower(), o.lower()))\n",
    "    return triples\n",
    "\n",
    "def create_sub_triples(sub, obj):\n",
    "    triples = []\n",
    "    for o in obj:\n",
    "        rel = create_relation(o)\n",
    "        triples.append((sub.lower(), rel.lower(), o.lower()))\n",
    "    return triples\n",
    "\n",
    "def noun_check(t):\n",
    "    if t.root.pos_ == 'NOUN' and t.root.lemma_ not in measurements and not color_check(t.root.lemma_):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba98d50-e81f-4457-a286-5950ecc0f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('../../data/PlantNet/descriptions_raw.pkl', 'rb'))\n",
    "#data = pickle.load(open('../../data/description/04_TRAIN_0000000-0014557_PLANTS.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "699bf423-7d68-449b-8569-ff68053ad129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c982e19b17ff4850a8740de8dec8bb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions = collections.defaultdict(list)\n",
    "\n",
    "for species in tqdm_notebook(list(data.keys())[0:]):\n",
    "    for idx, text in enumerate(data[species][0:]):\n",
    "        triples = []\n",
    "        text = text_preparation(species, text)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # TEMP ESCAPE\n",
    "        try:\n",
    "            if doc[-2].text in rubbish and doc[-3].text in rubbish:\n",
    "                continue\n",
    "            spaces = [t for t in doc if t.pos_ == 'SPACE']\n",
    "            if len(spaces) > 1 and 'species' in doc.text.lower():\n",
    "                continue\n",
    "            if doc[-1].text in ['..', '...']:\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        for t in doc:\n",
    "\n",
    "            if t.pos_ == 'NOUN' or t.pos_ == 'PROPN' or t.pos_ == 'PRON':\n",
    "                if t.dep_ == 'compound':\n",
    "                    continue\n",
    "                part = check_existance(t, doc)\n",
    "                if part: \n",
    "                    # Trait\n",
    "                    trait = compound_reconstructor(t, doc)\n",
    "                    \n",
    "                    ## ADJs and NOUNs\n",
    "                    for child in t.children:\n",
    "                        obj_tok  = extract_modifiers(child, doc)\n",
    "                        if obj_tok:\n",
    "                            obj = clean_object(obj_tok)\n",
    "                            triples += create_main_triples(part, trait, obj)\n",
    "                            # modifiers of NOUNS\n",
    "                            if noun_check(obj_tok):\n",
    "                                for child in obj_tok.root.children:\n",
    "                                    obj_tok = extract_modifiers(child, doc)\n",
    "                                    if obj_tok:\n",
    "                                        obj_new = clean_object(obj_tok)\n",
    "                                        triples += create_sub_triples(obj[0], obj_new)\n",
    "                                        \n",
    "                    ## VERBs\n",
    "                    verb = extract_verb(t, doc)\n",
    "                    if verb:\n",
    "                        for child in verb.children:\n",
    "                            obj_tok  = extract_verbal_modifier(child, doc)\n",
    "                            if obj_tok:\n",
    "                                obj = clean_object(obj_tok)\n",
    "                                triples += create_main_triples(part, trait, obj)\n",
    "                            \n",
    "                        \n",
    "        #print(text)\n",
    "        #print(idx, triples)\n",
    "        #print('\\n')\n",
    "        descriptions[species] += triples              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1f2dd17-0f06-4623-8824-985f9f2bc4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/PlantNet/descriptions_triples_raw_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions, f)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "991ad405-3a16-4789-a598-05f9bbaf8498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4c801068a24912bc9898da63dcf5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions_text = collections.defaultdict(list)\n",
    "descriptions_RDFs = collections.defaultdict(list)\n",
    "\n",
    "for species in tqdm_notebook(descriptions.keys()):\n",
    "    for (sub, rel, obj) in descriptions[species]:\n",
    "        text = f'{sub} {rel} {obj}.'.capitalize()\n",
    "        # Make sure order is the same\n",
    "        if text not in descriptions_text[species]:\n",
    "            descriptions_text[species].append(text)\n",
    "            descriptions_RDFs[species].append((sub, rel, obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b164264c-fb8f-43f6-bf90-ec837c9471d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/PlantNet/descriptions_triples_text_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_text, f)      \n",
    "    \n",
    "with open('../../data/PlantNet/descriptions_triples_rdf_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_RDFs, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bd9f7-f4f3-46f4-a51f-ac6d91bea931",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09cbd99-d2f5-4c01-b3ce-6d4d982815ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp('Stems often with spiny lower branches'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a173ddb-48e4-4b0a-9625-50793c4f8672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
