{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c838a5-be6b-4ef9-9611-ba3840147e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import json\n",
    "from spacy import displacy\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from itertools import islice\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50981800-673a-408a-9bff-4a5c95e292a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary = pickle.load(open('../../data/glossaries/FNA_glossary.pkl', 'rb'))\n",
    "glossary['leaf'] += ['leave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcbd31b-187b-41ad-9589-91c2fa500b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glossary_FNA['Flower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6534cd43-64ab-45fb-8a95-4135f7879b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_list = [\n",
    "    'fertile', 'sterile',\n",
    "    'male', 'female', 'bisexual', 'hermaphroditic', \n",
    "    'basal', 'developed', \n",
    "    'primary', 'secondary', 'main',\n",
    "    'upper', 'lower', 'greater', 'dorsal', 'alternate', 'lesser', 'apex', 'outer',\n",
    "    'central', 'outermost', 'outer', 'inner', 'uppermost', 'median', 'dorsal', 'central', 'lateral',\n",
    "    'young', 'mature', 'individual', \n",
    "    'opposite', 'single', 'paired',\n",
    "]\n",
    "\n",
    "rubbish_list = [\n",
    "    '.', ',', '-', '..', '...', '', \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba98d50-e81f-4457-a286-5950ecc0f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pickle.load(open('../../data/PlantNet/descriptions_raw.pkl', 'rb'))\n",
    "data = pickle.load(open('../../data/description/04_TRAIN_0000000-0014557_PLANTS.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860339e-8d1b-4a93-b5af-60f489798c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "861c7c47-631f-440f-bbda-5d7f4cd92bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_list = [\n",
    "    'mm', 'cm', 'm', 'km',\n",
    "    'milimeter', 'centimeter', 'meter', 'kilometer',\n",
    "    'milimetre', 'centimetre', 'metre', 'kilometre',\n",
    "    'inch', 'foot', 'yard', 'mile',\n",
    "    'wide', 'long', 'broad', 'tall',\n",
    "    'length', 'form',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5fab287-5b56-4fc6-aedb-a31ec3326538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preparation(species, text):\n",
    "    cleaners = [(r'(?<!\\d)\\.(?!\\d)', ' '),\n",
    "                (r'\\s×\\s', ' times '),\n",
    "                (r'\\s+c\\s+', ' '),\n",
    "                (r'â\\x80\\x93', ' to '),\n",
    "                (r'\\xa0', ' '),\n",
    "                (r'\\x97', ''),\n",
    "                (r'\\s{2,}', ' '),\n",
    "                (r'(\\D)(\\.)', r'\\1 '),\n",
    "                (r'(\\d)(\\.)(\\D)', r'\\1 \\3'),\n",
    "                (r'(long,)', r'long and'),\n",
    "                (r'(wide,)', r'wide and'),\n",
    "               ]\n",
    "    \n",
    "    species_parts = species.split()\n",
    "    candidates = [' '.join(species_parts[:idx+1]) for idx, _ in enumerate(species_parts)]\n",
    "    #candidates += [\n",
    "    #        f'{species_parts[0][0]}. {species_parts[1]}'\n",
    "    #    ]\n",
    "    candidates.reverse()\n",
    "    for candidate in candidates:\n",
    "        try:\n",
    "            text = re.sub(candidate, 'the species', text)\n",
    "        except:\n",
    "            continue # Skip species with brackets for now\n",
    "    for (cleaner, replacement) in cleaners:\n",
    "        text = re.sub(cleaner, replacement, text)    \n",
    "    text = f'{text.strip()}.'\n",
    "    return text.capitalize()\n",
    "\n",
    "\n",
    "def dict_sentence(t):\n",
    "    sentence_dict = {}\n",
    "    for child in t.children:\n",
    "        # Exceptions\n",
    "        if (\n",
    "            child.dep_ not in [\n",
    "                'det', 'cc', 'punct', \n",
    "                'poss', \n",
    "                'nmod', 'ccomp', # Gives to many errors\n",
    "                'conj',\n",
    "                'prt', # Verb things\n",
    "            ]\n",
    "            and child.pos_ not in [\n",
    "                'DET', 'PUNCT',\n",
    "                'PART',\n",
    "                'ADV',\n",
    "                'SCONJ',\n",
    "                'PRON',\n",
    "            ]\n",
    "            and child.lemma_ not in [\n",
    "                'on', 'of', 'as', 'from', 'by', 'onto',\n",
    "            ]\n",
    "        ):\n",
    "            items = dict_sentence(child)\n",
    "            sentence_dict[child] = items \n",
    "    return sentence_dict\n",
    "\n",
    "def undict_to_tuples(d, acc = []):\n",
    "    if d == {}:\n",
    "        yield acc\n",
    "    else:\n",
    "        for k, v in d.items():\n",
    "            yield from undict_to_tuples(v, acc + [k,])\n",
    "            \n",
    "def undict_to_pairs(d):\n",
    "    for k,v in d.items():\n",
    "        for subk in v:\n",
    "            yield (k, subk)\n",
    "        yield from undict_to_pairs(v)\n",
    "\n",
    "def dict_sentence_parent(t):\n",
    "    if t.dep_ in [\n",
    "        'nsubj', 'nsubjpass', 'relcl',\n",
    "    ]:\n",
    "        parent = next(tok for tok in t.ancestors)\n",
    "        parent_dict = dict_sentence(parent)\n",
    "        del parent_dict[t]\n",
    "        return parent_dict\n",
    "\n",
    "def update_nested_dict(main_dict, new_dict):\n",
    "    for name, rc_dict in new_dict.items():\n",
    "        main_dict.setdefault(name, {}).update(rc_dict)\n",
    "    return main_dict       \n",
    "\n",
    "def extract_compounds(t, doc):\n",
    "    head = None\n",
    "    if t.dep_ == 'compound':\n",
    "        t = next(t.ancestors)\n",
    "    indices = [child.i for child in t.children\n",
    "               if child.dep_ == 'compound'\n",
    "               or child.lemma_ in compound_list\n",
    "               and child.i < t.i]\n",
    "    indices.append(t.i)\n",
    "    indices.sort(reverse=True)\n",
    "    compounds  = []\n",
    "    for idx in indices:\n",
    "        compounds.append(doc[idx : t.i + 1])\n",
    "    return compounds    \n",
    "            \n",
    "            \n",
    "def check_existance(t):\n",
    "    item = None\n",
    "    for mainpart in glossary.keys():\n",
    "        if t.lemma_ in compound_list:\n",
    "            item = None\n",
    "        #elif t.pos_ != 'NOUN':\n",
    "        elif t.pos_ not in ['NOUN', 'PROPN']:\n",
    "            item = None\n",
    "        elif t.lemma_.lower().strip() in glossary[mainpart]:\n",
    "            item = mainpart            \n",
    "    return item\n",
    "\n",
    "\n",
    "def clean_compounds(item_list, doc):\n",
    "    new_item_list = []\n",
    "    new_item_list.append(item_list[0])\n",
    "    for item in item_list[1:]:\n",
    "        item_orig = item\n",
    "        if type(item) == spacy.tokens.token.Token:\n",
    "            item = item\n",
    "        else:\n",
    "            item = item.root\n",
    "        #if item.dep_ == 'prep':\n",
    "        #    new_item_list += item_list[1:]\n",
    "        #    break\n",
    "        if (\n",
    "            item.lemma_ in compound_list \n",
    "            or item.dep_ == 'compound'\n",
    "        ):\n",
    "            continue\n",
    "        elif item.pos_ == 'NOUN':\n",
    "            compound = extract_compounds(item, doc)[-1]\n",
    "            if len(compound) == 1:\n",
    "                #print(compound)\n",
    "                compound = compound.root\n",
    "            new_item_list.append(item_orig)\n",
    "        else:\n",
    "            new_item_list.append(item_orig)\n",
    "    return new_item_list\n",
    "\n",
    "def clean_measurements(info_list):\n",
    "        \n",
    "    new_item_list = []\n",
    "    # Retok\n",
    "    for item_list in info_list:\n",
    "        nums = [t.dep_ for t in item_list if type(t) == spacy.tokens.token.Token if t.pos_ == 'NUM']\n",
    "        if len(nums) > 1:\n",
    "            temp = []\n",
    "            for item, future in zip(item_list, item_list[1:]):        \n",
    "                if type(item) == spacy.tokens.span.Span:\n",
    "                    temp.append(item)\n",
    "                elif item.pos_ == 'NUM' and future.pos_ == 'NUM':\n",
    "                    new_item_list.append(temp + [item])\n",
    "                    new_item_list.append(temp + [future])\n",
    "                else:\n",
    "                    temp.append(item)        \n",
    "        else:\n",
    "            new_item_list.append(item_list)   \n",
    "    return new_item_list\n",
    "\n",
    "\n",
    "def clean_prepositions_of(info_list):\n",
    "    new_info_lists = []\n",
    "    for info in info_list:\n",
    "        new_item_list = []\n",
    "        used = []\n",
    "        for item in info:\n",
    "            item_orig = item\n",
    "            if type(item) == spacy.tokens.token.Token:\n",
    "                item = item\n",
    "            else:\n",
    "                item = item.root\n",
    "            prep_of = next((t for t in item.children if t.lemma_ == 'of'), None)\n",
    "            if prep_of:\n",
    "                child = next((t for t in prep_of.children if t.dep_ == 'pobj'), None)\n",
    "            if prep_of and child:\n",
    "                #print(item, prep_of, child)\n",
    "                new_item_list.append(doc[item.i : child.i + 1])\n",
    "            else:\n",
    "                new_item_list.append(item_orig)\n",
    "        new_info_lists.append(new_item_list)\n",
    "    return new_info_lists\n",
    "\n",
    "\n",
    "def clean_prepositions_on(info_list):\n",
    "    new_info_lists = []\n",
    "    #print(info_list)\n",
    "    for info in info_list:\n",
    "        new_item_list = []\n",
    "        used = []\n",
    "        for item in info:\n",
    "            item_orig = item\n",
    "            #print(item)\n",
    "            if type(item) == spacy.tokens.token.Token:\n",
    "                item = item\n",
    "            else:\n",
    "                item = item.root\n",
    "            prep = next((t for t in item.children if t.lemma_ == 'on'), None)\n",
    "            #print(prep_on)\n",
    "            if prep:\n",
    "                child = next((t for t in prep.children if t.dep_ == 'pobj'), None)\n",
    "            if prep and child:\n",
    "                #print(item, prep, child)\n",
    "                new_item_list.append(doc[item.i : child.i + 1])\n",
    "            else:\n",
    "                new_item_list.append(item_orig)\n",
    "        new_info_lists.append(new_item_list)\n",
    "    return new_info_lists\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def clean_prepositions_in(info_list):\n",
    "    new_info_lists = []\n",
    "    for info in info_list:\n",
    "        new_item_list = []\n",
    "        used = []\n",
    "        for item in info:\n",
    "            if item.lemma_ == 'in':\n",
    "                break\n",
    "            new_item_list.append(item)\n",
    "        new_info_lists.append(new_item_list)\n",
    "    return new_info_lists\n",
    "\n",
    "\n",
    "def clean_prepositions_with(info_list):\n",
    "    new_info_lists = []\n",
    "    for info in info_list:\n",
    "        new_item_list = []\n",
    "        used = []\n",
    "        for item in info:\n",
    "            if item.lemma_ == 'with':\n",
    "                continue\n",
    "            new_item_list.append(item)\n",
    "        new_info_lists.append(new_item_list)\n",
    "    return new_info_lists\n",
    "\n",
    "        \n",
    "def create_relation(compounds, item_list):    \n",
    "    subjects = item_list[:-1]\n",
    "    objects  = item_list[1:]\n",
    "    relation = None\n",
    "    triples = []\n",
    "    \n",
    "    #print(compounds[-1])\n",
    "    \n",
    "    # Compound\n",
    "    compound = compounds[-1][-1]\n",
    "    # Flatten\n",
    "    compounds = list(sum(compounds, ()))\n",
    "    \n",
    "    for sub, obj in zip(subjects, objects):\n",
    "        relation = f'{compound} temp'\n",
    "        if sub.lemma_ in compounds:\n",
    "            sub = compound\n",
    "        try:\n",
    "            obj_text = obj.lemma_\n",
    "        except:\n",
    "            obj_text = obj\n",
    "        try:\n",
    "            sub_text = sub.lemma_\n",
    "        except:\n",
    "            sub_text = sub\n",
    "        if obj_text in sub_text:\n",
    "            continue\n",
    "        triples.append((sub, relation, obj))\n",
    "    return triples\n",
    "    \n",
    "def extract_triples(doc):\n",
    "    # Speed up the extraction\n",
    "    previous = []\n",
    "    AIKEs_list = []\n",
    "    triples = []\n",
    "    for t in doc:\n",
    "        part = check_existance(t)\n",
    "        #print(part, t)\n",
    "        if part:\n",
    "            # Create temp list for storing compounds\n",
    "            compounds_temp = []\n",
    "            compounds_temp.append(('species', 'has_main_part', part.capitalize()))           \n",
    "            compound = part.capitalize()\n",
    "            #print(compound)\n",
    "            for new_compound in extract_compounds(t, doc):\n",
    "                if type(compound) == str:\n",
    "                    compounds_temp.append((compound, 'has_sub_part', new_compound.lemma_))\n",
    "                else:\n",
    "                    compounds_temp.append((compound.lemma_, 'has_sub_part', new_compound.lemma_))\n",
    "                compound = new_compound\n",
    "                #print(compound)\n",
    "            # Reset T\n",
    "            t = compound.root\n",
    "            # Get child dict\n",
    "            child_dict = {compound: dict_sentence(t)}\n",
    "            # Get parent dict\n",
    "            parent_dict = {compound: dict_sentence_parent(t)}\n",
    "            # Update if exists\n",
    "            if parent_dict[compound]:\n",
    "                # Add dicts together\n",
    "                sentence_dict = update_nested_dict(child_dict, parent_dict)\n",
    "                #print(sentence_dict)\n",
    "            else:\n",
    "                sentence_dict = child_dict\n",
    "            # List dict into tuples\n",
    "            info_lists = list(undict_to_tuples(sentence_dict))\n",
    "            info_lists = clean_measurements(info_lists)\n",
    "            info_lists = clean_prepositions_of(info_lists)\n",
    "            info_lists = clean_prepositions_in(info_lists)\n",
    "            info_lists = clean_prepositions_on(info_lists)\n",
    "            info_lists = clean_prepositions_with(info_lists)\n",
    "            #clean_measurements(info_lists)\n",
    "\n",
    "            #print('end', t, info_lists)\n",
    "            for info in info_lists:\n",
    "                # Skip no info\n",
    "                if len(info) <= 1:\n",
    "                    continue\n",
    "                info = clean_compounds(info, doc)\n",
    "                triples.extend(compounds_temp)\n",
    "                triples.extend(create_relation(compounds_temp, info))\n",
    "            \n",
    "    return list(dict.fromkeys(triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc31c6a9-e25b-4849-9fa8-3ea1bbc51c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('species', 'has_main_part', 'Leaf'),\n",
       " ('Leaf', 'has_sub_part', 'leave'),\n",
       " ('leave', 'leave temp', long),\n",
       " (long, 'leave temp', meters),\n",
       " (meters, 'leave temp', 4),\n",
       " (meters, 'leave temp', 2),\n",
       " (4, 'leave temp', to)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#string = 'Fertile lemma lanceolate in profile, or oblong in profile, 2.5-3.8 milimeter long, membranous, mid-green and grey, 3 -veined.'\n",
    "string = 'The leaves are 2 to 4 meters long.'\n",
    "doc = nlp(string)\n",
    "extract_triples(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d29e1d-0f71-460f-af19-68794b315223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3abff83039444c11978fa306eea0dfab-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">leaves</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">2</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">4</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">meters</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">long.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abff83039444c11978fa306eea0dfab-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abff83039444c11978fa306eea0dfab-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abff83039444c11978fa306eea0dfab-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abff83039444c11978fa306eea0dfab-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abff83039444c11978fa306eea0dfab-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,89.5 920.0,89.5 920.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abff83039444c11978fa306eea0dfab-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abff83039444c11978fa306eea0dfab-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abff83039444c11978fa306eea0dfab-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abff83039444c11978fa306eea0dfab-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abff83039444c11978fa306eea0dfab-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abff83039444c11978fa306eea0dfab-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abff83039444c11978fa306eea0dfab-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abff83039444c11978fa306eea0dfab-0-6\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abff83039444c11978fa306eea0dfab-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46053f-588c-4624-9c8a-72fbee22de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = list(data.keys())\n",
    "species = species_list[535]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993084ab-35c8-4003-a5c1-3a8d4d62d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_data = []\n",
    "for text in tqdm_notebook(data[species][0:16]):\n",
    "    text = text_preparation(species, text)\n",
    "    doc = nlp(text)\n",
    "    #if len(doc) > 20:\n",
    "    #    continue\n",
    "    kn_data.extend(extract_triples(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9e3c8-512d-4f14-b5b0-07d5e769e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6af1c3-bd12-45c1-921e-0570ef9a0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0459083-b539-4eae-92f0-0a3b69206cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa586e-c9d0-42e4-a5aa-818c95acc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = next(t for t in doc if t.dep_ == 'ROOT')\n",
    "print(root, root.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c515994-f057-4de7-b840-7ea6e3ab5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f21075-683e-4065-b7a5-355e22988aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'(long,)', r'long and', doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fd952b-efb6-48ec-9691-b6fd96592b20",
   "metadata": {},
   "source": [
    "# VIZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda96c7-7e1c-4a08-bd5f-07e994625bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from netgraph import Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29154af6-f4ec-4408-8904-207e7a34ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348fc33-e829-403b-b811-e7f6fb7ba9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "source   = []\n",
    "relation = []\n",
    "target   = []\n",
    "\n",
    "\n",
    "for (sub, rel, obj) in kn_data:\n",
    "    if type(sub) != str:\n",
    "        sub = sub.lemma_\n",
    "    if type(obj) != str:\n",
    "        obj = obj.lemma_\n",
    "    if sub ==  obj:\n",
    "        continue\n",
    "    source.append(sub)\n",
    "    relation.append(rel)\n",
    "    target.append(obj)\n",
    "\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4975c-30de-4a84-9a5a-47645f170177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f304656-f6e8-4fd5-8c56-447cde861402",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [(source, target) for source, target in zip(kg_df['source'].values, kg_df['target'].values)]\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.Graph())\n",
    "\n",
    "\n",
    "node_labels = {node : node for idx, node in enumerate(G)}\n",
    "edge_labels = dict(zip(list(zip(kg_df.source, kg_df.target)),\n",
    "                  kg_df['edge'].tolist()))\n",
    "\n",
    "node_size = {}\n",
    "node_color = {}\n",
    "\n",
    "size = 1.5\n",
    "\n",
    "for node in node_labels:\n",
    "    if node == 'species':\n",
    "        node_size[node] = 3.5/size\n",
    "        node_color[node] = 'darkgreen'\n",
    "    elif node[0].isupper():\n",
    "        node_size[node] = 2/size\n",
    "        node_color[node] = 'white'\n",
    "    else:\n",
    "        node_size[node] = 1./size\n",
    "        node_color[node] = 'white'\n",
    "        \n",
    "pos = nx.spring_layout(G, k = 0.08, iterations=5000, seed=3, scale=0.3, center=(0,0), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927df6cf-661d-45e6-8f50-d817a4664c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 25))\n",
    "Graph(nodes, \n",
    "      #node_layout='spring', edge_layout='curved', \n",
    "      #node_layout=pos, edge_layout='straight', \n",
    "      node_layout='spring', edge_layout='straight',\n",
    "      arrows=True, node_zorder=3, #edge_zorder=1,\n",
    "      node_labels=node_labels, \n",
    "      node_label_offset=0.02, \n",
    "      #edge_labels=edge_labels,\n",
    "      node_label_fontdict=dict(size=18, rotation=0, ha='center', clip_on=False), node_edge_width=0.2,\n",
    "      node_size=node_size,  node_color=node_color, #edge_labels=edge_labels,\n",
    "      edge_width=0.2, edge_label_fontdict=dict(size=10,),\n",
    "      #node_layout_kwargs=dict(node_size=1, total_iterations=20),\n",
    "      ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcaf5f-6232-4d70-9e0e-71be53023642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487431ab-1d37-4ee3-bd6d-5f2ed74301ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('legs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f9cf8-4216-41a0-9ae3-51769e6fe9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0b33e7-3cc5-479f-879a-1826835ee65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436de400-3f78-4258-9930-e4b1c949c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821865f-63b6-4e09-95b2-a641ff085389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
