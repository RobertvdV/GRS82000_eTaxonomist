{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50edec8-263d-4711-ba22-4ade8fe40edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import random\n",
    "import string\n",
    "from pathlib import Path\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/models/')\n",
    "sys.path.insert(0, '../src/features/')\n",
    "#sys.path.insert(0, '../src/visualization/')\n",
    "\n",
    "import predict_model\n",
    "from build_features import text_cleaner\n",
    "#import visualize as vis\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a0e5b3-96f4-4039-b2ef-655c1e4bbd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "model = predict_model.loadBERT(\"../models/\", 'saved_weights_inf_FIXED_boot.pt')\n",
    "sim_model = predict_model.load_simBERT()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174c3f4a-8685-471f-895f-b93157ef0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(span, pred_values=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "         \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(**inputs)\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        # Print the prediction values\n",
    "        if pred_values:\n",
    "            return span_class, exps[0]\n",
    "        else:\n",
    "            return span_class\n",
    "        \n",
    "\n",
    "def similarity_matrix(sentence_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a hidden state array per sententence based on a list of\n",
    "    sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dictionary to store tokenized sentences\n",
    "    tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "        # encode each sentence and append to dictionary\n",
    "        new_tokens = tokenizer.encode_plus(sentence, max_length=512,\n",
    "                                           truncation=True, \n",
    "                                           padding='max_length',\n",
    "                                           return_tensors='pt')\n",
    "        # Drop the batch dimension\n",
    "        tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "        tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "    \n",
    "    # Reformat list of tensors into single tensor\n",
    "    tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "    tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "    \n",
    "    # Get vectors\n",
    "    hiddenstates = sim_model(**tokens)\n",
    "    # Sum along first axis\n",
    "    summed_hs = torch.sum(hiddenstates, 1)\n",
    "    # Detach\n",
    "    summed_hs_np = summed_hs.detach().numpy()\n",
    "    # Get the matrix\n",
    "    return cosine_similarity(summed_hs_np, summed_hs_np).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7a2ec-03f7-495e-9bf9-63185240247f",
   "metadata": {},
   "source": [
    "### BIRD OF THE WORLD\n",
    "Structured queries of the BOW website and using the BERT model to scrape the HTML's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad6083-1c55-49b0-9793-d190dfe72bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all html files\n",
    "species_folder = glob.glob('../data/raw/BOW/*')\n",
    "single_list = [Species + '/Introduction.html' for Species in species_folder if len(glob.glob(Species + '/*')) == 1]\n",
    "multi_list = [glob.glob(Species + '/*') for Species in species_folder if len(glob.glob(Species + '/*')) != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4d251-c5b0-47c2-b10a-a9a6251ea783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(list)\n",
    "\n",
    "for html in tqdm(single_list):\n",
    "    try:\n",
    "        with open(html) as f:\n",
    "            # Structure it\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "            # Extract name\n",
    "            species = soup.title.text.strip().split(' - ')[1]\n",
    "            # Find all non Identification Spans\n",
    "            spans = [span for span in soup.find_all('p') \n",
    "                     if not 'fig' in span.text]\n",
    "            for span in spans:\n",
    "                if span.find_previous_sibling() != None:\n",
    "                    if span.find_previous_sibling().find('h2') != None:\n",
    "                        # Locate Identification\n",
    "                        if span.find_previous_sibling().find('h2').text == 'Identification':\n",
    "                            text_id = span.text.strip().replace('\\n', \"\").replace('; ', '. ')\n",
    "                            text_id = re.sub(' +',' ', text_id)\n",
    "                            text_list = text_id.split('. ')\n",
    "                            for sentence in text_list:\n",
    "                                if classify(sentence):\n",
    "                                    #print(URL)\n",
    "                                    data[species].append(sentence)\n",
    "                        else:\n",
    "                            sentences = text_cleaner(span.text)\n",
    "                            # Loop over the individual sentences\n",
    "                            for sentence in sentences:                    \n",
    "                                # Create string object\n",
    "                                sentence_str = str(sentence)\n",
    "                                #print(sentence_str)\n",
    "\n",
    "                                if classify(sentence_str):\n",
    "                                    #print(URL)\n",
    "                                    data[species].append(sentence_str)\n",
    "                    else:\n",
    "                        sentences = text_cleaner(span.text)\n",
    "                        # Loop over the individual sentences\n",
    "                        for sentence in sentences:                    \n",
    "                            # Create string object\n",
    "                            sentence_str = str(sentence)\n",
    "                            #print(sentence_str)\n",
    "\n",
    "                            if classify(sentence_str):\n",
    "                                #print(URL)\n",
    "                                data[species].append(sentence_str)\n",
    "                else:\n",
    "                    continue\n",
    "    except:\n",
    "        print('fail')    # Continue if HTML fails to open\n",
    "        continue\n",
    "\n",
    "for species_list in tqdm(multi_list):\n",
    "    try:\n",
    "        with open(species_list[0]) as f:\n",
    "            # Read only the first html title (there are some inconsistencies)\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "            species = soup.title.text.strip().split(' - ')[2]\n",
    "        for html in species_list:\n",
    "\n",
    "            with open(html) as f:\n",
    "                # Structure it\n",
    "                soup = BeautifulSoup(f, 'html.parser')\n",
    "                # Get all spans\n",
    "                spans = [span for span in soup.find_all('p') \n",
    "                         if not 'fig' in span.text]\n",
    "                for span in spans:\n",
    "\n",
    "                    sentences = text_cleaner(span.text)\n",
    "                    # Loop over the individual sentences\n",
    "                    for sentence in sentences:                    \n",
    "                        # Create string object\n",
    "                        sentence_str = str(sentence)\n",
    "                        #print(sentence_str)\n",
    "\n",
    "                        if classify(sentence_str):\n",
    "                            #print(URL)\n",
    "                            data[species].append(sentence_str)\n",
    "    except:\n",
    "        continue\n",
    "                        \n",
    "# Dump pickle into file\n",
    "with open('../data/processed/scrapeddata_train_species_description_bow.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54073f2e-b92d-47c1-bc5d-983544a3aa16",
   "metadata": {},
   "source": [
    "### AGROFORESTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ce9632-e913-4d3d-9ff8-a91946c7cea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 26/26 [00:09<00:00,  2.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "agro_list = []\n",
    "\n",
    "for letter in tqdm(list(string.ascii_uppercase)):\n",
    "    # Get URL\n",
    "    URL = 'http://apps.worldagroforestry.org/treedb2/index.php?letter={0}'.format(letter)\n",
    "    page = requests.get(URL, timeout=5)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Extract search table\n",
    "    for tree in soup.find_all('table')[2].find_all('a'):\n",
    "        agro_list.append(tree.text)\n",
    "        \n",
    "# Create data list        \n",
    "agro_data = [(tree, 'http://db.worldagroforestry.org//species/properties/' + '_'.join(tree.split(' '))) for tree in agro_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640951ff-721d-4023-8839-b72d6c01e9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 617/617 [27:49<00:00,  2.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# Init dict\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "# Loop over URL\n",
    "for (species, URL) in tqdm(agro_data):\n",
    "    #print(species, URL)\n",
    "    try:\n",
    "        page = requests.get(URL, timeout=5)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        dirty_text = soup.get_text(\". \", strip=True)\n",
    "        sentences = text_cleaner(dirty_text)\n",
    "        \n",
    "        # Loop over sent list\n",
    "        for sentence in sentences:\n",
    "            #print(sentence)\n",
    "            if classify(sentence):\n",
    "                data[species].append((sentence, URL))\n",
    "                #print(sentence, URL)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "# Dump pickle into file\n",
    "with open('../data/processed/descriptions_agroforestry_PLANTS.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911703e4-cf3b-4da9-b5aa-4e6f9c1836fd",
   "metadata": {},
   "source": [
    "### LLIFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f14a943-200c-4a2f-83de-c3b72a2f22c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:05<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# init index list\n",
    "tree_links_index = []\n",
    "# Extract index pages\n",
    "for i in range(1, 8):\n",
    "    tree_links_index.append('http://www.llifle.com/Encyclopedia/TREES/Species/all/{0}/100/'.format(i))\n",
    "\n",
    "# Init empty list\n",
    "tree_links = []\n",
    "\n",
    "for index_pages in tqdm(tree_links_index):\n",
    "    # Extract XML\n",
    "    URL = index_pages\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Extract links incomplete\n",
    "    tree_links_half = soup.find_all('a')\n",
    "\n",
    "    # Complete the links\n",
    "    tree_links_temp = ['http://www.llifle.com' + pages.get('href') for pages in tree_links_half\n",
    "                           if pages.get('href') != None \n",
    "                           if pages.get('href').startswith('/Encyclopedia/TREES/Family/')]\n",
    "    # Add to all trees\n",
    "    tree_links += tree_links_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6861d5d0-530d-4d9e-b9d4-5549b91d2b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 647/647 [18:16<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# Init empty dict\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "for URL in tqdm(tree_links):\n",
    "    try:\n",
    "        # Get Page\n",
    "        page = requests.get(URL)\n",
    "        # Structure page\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        # Name\n",
    "        species = soup.title.text.replace('\\n', '')\n",
    "        # Loop over text\n",
    "        dirty_text = soup.get_text(\". \", strip=True)\n",
    "        sentences = text_cleaner(dirty_text)\n",
    "        \n",
    "        # Loop over sent list\n",
    "        for sentence in sentences:\n",
    "            #print(sentence)\n",
    "            if classify(sentence):\n",
    "                data[species].append((sentence, URL))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "# Dump pickle into file\n",
    "with open('../data/processed/descriptions_llife_PLANTS.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162a071-b38f-4611-a4bf-a5b5093a62b2",
   "metadata": {},
   "source": [
    "### PLANT OF THE WORLD ONLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a2c095-ca2d-4a6c-a5ac-40cc34e3f2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 40187/40187 [18:56:57<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# Init empty dict\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "# Read files\n",
    "powo_HTMLs = glob.glob('../data/raw/POWO/*')\n",
    "\n",
    "for html in tqdm(powo_HTMLs):\n",
    "    try:\n",
    "        # Open HTML file\n",
    "        with open(html) as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "            # Extract title\n",
    "            species = html.lstrip('../data/raw/POWO/').split(' - ')[0]\n",
    "            # Loop over text\n",
    "            dirty_text = soup.get_text(\". \", strip=True)\n",
    "            sentences = text_cleaner(dirty_text)\n",
    "\n",
    "            # Loop over sent list\n",
    "            for sentence in sentences:\n",
    "                #print(sentence)\n",
    "                if classify(sentence):\n",
    "                    data[species].append((sentence, URL))\n",
    "    except:\n",
    "        #print('fail')\n",
    "        continue\n",
    "        \n",
    "# Dump pickle into file\n",
    "with open('../data/processed/descriptions_powo_PLANTS.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a74b45-9ced-4f3a-80a0-dc420c034233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67509384-60c6-481a-a9de-2c1e33f5c4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
