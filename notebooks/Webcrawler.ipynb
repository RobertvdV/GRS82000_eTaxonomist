{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b16434b-0b56-4d3e-aed0-f819fe949ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import urllib.parse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/models/')\n",
    "sys.path.insert(0, '../src/features/')\n",
    "#sys.path.insert(0, '../src/visualization/')\n",
    "import predict_model\n",
    "from build_features import text_cleaner\n",
    "#import visualize as vis\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd21d8e7-545d-4c22-901b-dbd965875fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "model = predict_model.loadBERT(\"../models/\", 'saved_weights_inf_FIXED_boot.pt')\n",
    "sim_model = predict_model.load_simBERT()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "335bc5bc-270f-4eed-9394-066e8c58d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_Duck(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    Queries DuckDuckGo and returns a URL list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get results \n",
    "    page = requests.get('https://duckduckgo.com/html/?q={0}'.format(query), \n",
    "                        headers={'user-agent': 'Descriptor/0.0.1'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    results = soup.find_all('a', attrs={'class':'result__a'}, href=True)\n",
    "    # Init list\n",
    "    links = []\n",
    "    # Clean results\n",
    "    for link in results:\n",
    "        url = link['href']\n",
    "        o = urllib.parse.urlparse(url)\n",
    "        d = urllib.parse.parse_qs(o.query)\n",
    "        links.append(d['uddg'][0])\n",
    "    return links\n",
    "\n",
    "def search_Bing(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    Queries Bing and returns a URL list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get results\n",
    "    page = requests.get('https://www.bing.com/search?form=MOZLBR&pc=MOZI&q={0}'.format(query), \n",
    "                        headers={'user-agent': 'Descriptor/0.0.1'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Init list\n",
    "    links = [] \n",
    "    # Clean results\n",
    "    for i in soup.find_all('a', attrs={'h':re.compile('ID=SERP.+')}, href=True):\n",
    "        link = i['href']\n",
    "        if link.startswith('http') and 'microsoft' not in link and 'bing' not in link:\n",
    "            links.append(link)        \n",
    "    return links\n",
    "\n",
    "def SpanPredictor(span, pred_values=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "         \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(**inputs)\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        # Print the prediction values\n",
    "        if pred_values:\n",
    "            return span_class, exps[0]\n",
    "        else:\n",
    "            return span_class\n",
    "        \n",
    "\n",
    "def similarity_matrix(sentence_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a hidden state array per sententence based on a list of\n",
    "    sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dictionary to store tokenized sentences\n",
    "    tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "        # encode each sentence and append to dictionary\n",
    "        new_tokens = tokenizer.encode_plus(sentence, max_length=512,\n",
    "                                           truncation=True, \n",
    "                                           padding='max_length',\n",
    "                                           return_tensors='pt')\n",
    "        # Drop the batch dimension\n",
    "        tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "        tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "    \n",
    "    # Reformat list of tensors into single tensor\n",
    "    tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "    tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "    \n",
    "    # Get vectors\n",
    "    hiddenstates = sim_model(**tokens)\n",
    "    # Sum along first axis\n",
    "    summed_hs = torch.sum(hiddenstates, 1)\n",
    "    # Detach\n",
    "    summed_hs_np = summed_hs.detach().numpy()\n",
    "    # Get the matrix\n",
    "    return cosine_similarity(summed_hs_np, summed_hs_np).round(5)\n",
    "        \n",
    "        \n",
    "    \n",
    "def VisualizeDoc(text, per_sentence=False, save=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates and HTML file (can be rendered in a notebook) by using the SpaCy \n",
    "    Displacy.\n",
    "    \n",
    "    per_sentence : Returns the visualization per sentence instead of a whole doc.\n",
    "    save         : If True returns the html string to save.\n",
    "    \"\"\"\n",
    "    \n",
    "    # nlp the text\n",
    "    doc = nlp(text)\n",
    "    # Extract the sents\n",
    "    sentences = [i for i in doc.sents]\n",
    "    # Init color map\n",
    "    cmap = cm.get_cmap('Spectral')\n",
    "    # Init color dict\n",
    "    colors = {}\n",
    "    # Init option dict\n",
    "    options = {\"ents\": [],\n",
    "               \"colors\": colors,\n",
    "               \"distance\": 75}\n",
    "    # Init matcher\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    # Loop over the sentences\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        \n",
    "        # Get the prediction values    \n",
    "        prediction = SpanPredictor(str(sentence), pred_values=True)[1][1].numpy().item()\n",
    "        \n",
    "        # String ID            \n",
    "        #text = '#{0} - {1:.2f}'.format(idx, prediction)\n",
    "        text = f'{prediction:.3f}'\n",
    "        # Add the patterns        \n",
    "        pattern = nlp(str(sentence))\n",
    "        matcher.add(text, None, pattern)\n",
    "\n",
    "        # Colorize the strings\n",
    "        if prediction > .5:\n",
    "            colors[text] = matplotlib.colors.rgb2hex(cmap(prediction))\n",
    "        else:\n",
    "            colors[text] = matplotlib.colors.rgb2hex(cmap(prediction)) #+ '90'\n",
    "        # Add the new ENTS to the doc\n",
    "        options[\"ents\"].append(text)\n",
    "\n",
    "    # Match the enitities in the doc\n",
    "    matches = matcher(doc)\n",
    "    # Reset the current ENTS\n",
    "    doc.ents = ()\n",
    "    # Loop over the matches\n",
    "    for match_id, start, end in matches:\n",
    "        # Add the sentencen as a ENT\n",
    "        span = Span(doc, start, end, label=match_id)\n",
    "        #doc.ents = filter_spans(doc.ents)\n",
    "        try:\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Set title\n",
    "    #doc.user_data[\"title\"] = \"Description Predictor\"\n",
    "    sentence_spans = list(doc.sents)\n",
    "    \n",
    "    if save and per_sentence:\n",
    "        html = displacy.render(sentence_spans, style='ent', options=options, page=True, jupyter=False, minify=False)\n",
    "        return html\n",
    "    elif save and not per_sentence:\n",
    "        html = displacy.render(doc, style='ent', options=options, page=True, jupyter=False, minify=False)\n",
    "        return html\n",
    "    elif not save and per_sentence:\n",
    "        displacy.render(sentence_spans, style='ent', options=options)\n",
    "    elif not save and not per_sentence:\n",
    "        displacy.render(doc, style='ent', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6de9f95-b9d8-43cb-bd11-3fbe568f4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists\n",
    "plants_dict = pickle.load(open('../data/processed/train_dataPOWO.pkl', 'rb'))\n",
    "birds_dict = pickle.load(open('../data/processed/descriptions_web_birds_bow.pkl', 'rb'))\n",
    "\n",
    "plants_list = [keys for keys, values in plants_dict.items()]\n",
    "birds_list = [keys for keys, values in birds_dict.items()]\n",
    "\n",
    "plants_list_r = random.sample(plants_list, len(birds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e6eb56-80b3-4287-98dd-d6f1f335358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = plants_list_r + birds_list\n",
    "random.shuffle(species_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf881be9-fcd7-49dd-b171-7ecf6c7082f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [03:52<00:00, 23.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# Init dict\n",
    "data = collections.defaultdict(list)\n",
    "# Init dict\n",
    "\n",
    "# DEBUGGING\n",
    "data_link = collections.defaultdict(list)\n",
    "data_with_source = collections.defaultdict(list)\n",
    "\n",
    "query = 'description'\n",
    "\n",
    "for count, species in enumerate(tqdm(species_list[0:10])):\n",
    "#for count, species in enumerate(tqdm(plants_list[0:4])):\n",
    "    \n",
    "    # Empty list\n",
    "    search_links = []\n",
    "    # create q\n",
    "    species_q = species.replace(' ', '+')\n",
    "    species_q = f'\"{species_q}\"+{query}'\n",
    "    # species_q = f'\"{species_q}\"+{query}'\n",
    "    try:\n",
    "        search_links += search_Duck(species_q)\n",
    "        search_links += search_Bing(species_q)\n",
    "    except:\n",
    "        # Skip connection timeout\n",
    "        continue\n",
    "    # Drop duplicates\n",
    "    search_links = list(set(search_links))\n",
    "    # DEBUGGING\n",
    "    data_link[species] += search_links\n",
    "    \n",
    "    # Loop over the URLs\n",
    "    for URL in search_links:\n",
    "        # Skip google archives\n",
    "        if 'google' in URL:\n",
    "            continue\n",
    "        # PDF and TXT\n",
    "        if URL.endswith('txt') or URL.endswith('pdf'):\n",
    "            \n",
    "            \"\"\"\n",
    "            Continue for now, insert the text/pdf processor here\n",
    "            \"\"\"\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            #print(URL)\n",
    "            page = requests.get(URL, timeout=5)\n",
    "            # Skip PDF files for now\n",
    "            if page.headers['Content-Type'].startswith('application/pdf'):\n",
    "                \n",
    "                \"\"\"\n",
    "                Continue for now, insert the pdf processor here\n",
    "                \"\"\"\n",
    "                continue\n",
    "                \n",
    "            # Soup the result\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                \n",
    "            # Skip Embedded PDF's\n",
    "            if 'pdf' in soup.title.text.lower():\n",
    "                continue\n",
    "            \n",
    "            #print(soup.title.text, species)\n",
    "            # Check if species exists somewhere within title\n",
    "            if bool(set(species.split()).intersection(soup.title.text.split())):\n",
    "                # Get text\n",
    "                dirty_text = soup.get_text(\". \", strip=True)\n",
    "                # Clean the soup and break into sents\n",
    "                sentences = text_cleaner(dirty_text)\n",
    "                # Loop over the individual sentences\n",
    "                for sentence in sentences:                    \n",
    "                    # Create string object\n",
    "                    sentence_str = str(sentence)\n",
    "                    # Check if description\n",
    "                    if SpanPredictor(sentence_str):\n",
    "                        data[species].append(sentence_str)\n",
    "                        data_with_source[species].append(tuple([sentence_str, URL]))\n",
    "                            \n",
    "        except: \n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cb67752-7a8e-4347-9271-3e380ec17ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:56<00:00, 14.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# Drop double sentences\n",
    "for key, values in tqdm(data.items()):\n",
    "    # Get similarity matrix\n",
    "    matrix = similarity_matrix(values)\n",
    "    # Extract indices with threshold\n",
    "    indices = np.transpose((matrix>0.98).nonzero())\n",
    "    # Get doubles\n",
    "    if len(indices) > 1:\n",
    "        doubles = [values[idx_y] for (idx_x, idx_y) in indices if idx_x != idx_y]\n",
    "        # drop last half of list\n",
    "        doubles = doubles[len(doubles)//2:]\n",
    "        # sentences non double\n",
    "        sents_nodouble = [sent for sent in values if sent not in doubles]\n",
    "        # Replace data\n",
    "        data[key] = sents_nodouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538fbe3-b75c-4c28-adb5-606632f88ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_species_descriptions.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a35d2d-6b7e-46d9-b0f4-a909f053d00b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e893bd-72cb-4c31-a34d-b8a40a75bcf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fcc38f-ec5b-4852-aa7d-ff1ee6cf91c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af744062-bff6-4383-9928-c86b4d1d190f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2416f75-2baa-4bfa-bb0b-9449f716eaa9",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a570a79-ad33-45e7-ac6e-3e00ae93550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://db.worldagroforestry.org//species/properties/Enterolobium_cyclocarpum' \n",
    "#URL = 'http://powo.science.kew.org/taxon/757855-1'\n",
    "#URL = 'https://birdsoftheworld.org/bow/species/gargan/cur/introduction'\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "dirty_text = soup.get_text(\". \", strip=True)\n",
    "text = str(text_cleaner(dirty_text, per_sent=False))\n",
    "VisualizeDoc(text, per_sentence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c83af-eaa8-4541-9150-2cdfa3e0d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://www.llifle.com/Encyclopedia/TREES/Family/Apocynaceae/12217/Pachypodium_lealii' \n",
    "#URL = 'http://powo.science.kew.org/taxon/757855-1'\n",
    "#URL = 'https://birdsoftheworld.org/bow/species/gargan/cur/introduction'\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "text = str(text_cleaner(soup, per_sent=False))\n",
    "VisualizeDoc(text, per_sentence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe57034f-5c76-49fd-adcb-3b8db2642566",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "Ceiba pentandra, is a lofty tropical deciduous tree with a very straight buttressed trunk up to 3 m in diameter that usually grows to an average of 18-20 meters, with old trees up to 65-70 meters in very favourable wet tropical weather and is said to be the largest tree of the West African region and occurs throughout. It produces rose-coloured or white flowers followed by a capsule which, when ripe, contains white fibres like cotton. Its trunk bears spikes to deter attacks by animals. Kapok is the most used common name for the tree and may also refer to the cotton-like fluff obtained from its seed pods. The tree is also known as the Java cotton, Java kapok, silk-cotton or ceiba both of this names may also refer to Bombax ceiba.\n",
    "Ceiba pentandra, is quite easily grown from seed and is planted in parks and on roadsides as an avenue and shade tree. In built-up areas it will prove to be a troublesome one as the roots effect forceful entry into cracks in buildings, roads, drains, etc., and pass through or under and disturb foundations. It grows best in subtropical climate and heavy rainfalls but fairly drought-resistant too. \n",
    "Trunk, very straight, bole up to 35 m tall, usually cylindrical, 2-3 m in diameter, usually with large plank-like buttresses up to 3(-8) m high extending 1-2 m from the bole and with more or less horizontal main branches and often bracketed below to the stem. Branches usually in whorls of 3. The trunk and many of the larger branches are often crowded with large conical thorns 1-1.5 cm long, at least when young; bark smooth pale grey; young branches glabrous or pubescent.\n",
    "\"\"\"\n",
    "sents = ' '.join(text_cleaner(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed15471d-ff66-4cd7-a340-8b8fb545ac0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #a40844; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    In built-up areas it will prove to be a troublesome one as the roots effect forceful entry into cracks in buildings, roads, drains, etc., and pass through or under and disturb foundations.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0.013</span>\n",
       "</mark>\n",
       " </div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #bc2249; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    It grows best in subtropical climate and heavy rainfalls but fairly drought-resistant too.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0.055</span>\n",
       "</mark>\n",
       " </div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #97d5a4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ceiba pentandra, is a lofty tropical deciduous tree with a very straight buttressed trunk up to 3 m in diameter that usually grows to an average of 18-20 meters, with old trees up to 65-70 meters in very favourable wet tropical weather and is said to be the largest tree of the West African region and occurs throughout.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0.730</span>\n",
       "</mark>\n",
       " </div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #fdad60; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Its trunk bears spikes to deter attacks by animals.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0.299</span>\n",
       "</mark>\n",
       " </div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #e2514a; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The tree is also known as the Java cotton, Java kapok, silk-cotton or ceiba both of this names may also refer to Bombax ceiba.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0.143</span>\n",
       "</mark>\n",
       " </div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #5c51a3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The trunk and many of the larger branches are often crowded with large conical thorns 1-1.5 cm long, at least when young, bark smooth pale grey, young branches glabrous or pubescent.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0.994</span>\n",
       "</mark>\n",
       " </div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #b11747; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ceiba pentandra, is quite easily grown from seed and is planted in parks and on roadsides as an avenue and shade tree.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0.038</span>\n",
       "</mark>\n",
       " </div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aadca4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    It produces rose-coloured or white flowers followed by a capsule which, when ripe, contains white fibres like cotton.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0.702</span>\n",
       "</mark>\n",
       " </div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #5e4fa2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trunk, very straight, bole up to 35 m tall, usually cylindrical, 2-3 m in diameter, usually with large plank-like buttresses up to 3 m high extending 1-2 m from the bole and with more or less horizontal main branches and often bracketed below to the stemeters Branches usually in whorls of 3.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0.998</span>\n",
       "</mark>\n",
       " </div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #fdc574; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kapok is the most used common name for the tree and may also refer to the cotton-like fluff obtained from its seed pods.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0.347</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html = VisualizeDoc(sents, per_sentence=True, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e76caf-f308-455c-821f-486423d4c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"europeanrobin.html\")\n",
    "output_path.open(\"w\", encoding=\"utf-8\").write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7c10b-fabb-4333-b179-e91a154885a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96632308-f208-4baa-9f07-19e1497cd232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
