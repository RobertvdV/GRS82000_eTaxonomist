{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f08f15-0109-46d5-8587-f6ea8ab7baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "import collections\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as colors\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/models/')\n",
    "sys.path.insert(0, '../src/features/')\n",
    "\n",
    "import predict_model\n",
    "from predict_model import loadBERT\n",
    "from predict_model import SpanPredictor as classify\n",
    "from build_features import text_cleaner\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e10cb9-11ea-4ad8-90ee-dac32e9d3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "model = loadBERT(\"../models/\", 'saved_weights_inf_FIXED_boot_beta80.pt')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65833bf8-9868-40b0-adde-909477f762df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_reconstructor(token, doc):\n",
    "    if token.i == 0:\n",
    "        trait = token\n",
    "    elif doc[token.i - 3].dep_ == 'compound':\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 3].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 3: token.i + 1]\n",
    "    elif doc[token.i - 2].dep_ == 'compound':\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 2].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 2: token.i + 1]\n",
    "    elif doc[token.i - 1].dep_ == 'compound':\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    elif doc[token.i - 1].text.lower() in compounds or doc[token.i - 3].lemma_.lower() in compounds:\n",
    "        trait = doc[token.i - 1: token.i + 1]\n",
    "    else:\n",
    "        trait = token   \n",
    "    return trait.lemma_\n",
    "\n",
    "def check_existance(t, doc, glossary):\n",
    "    if t.lemma_ in glossary:\n",
    "        return t\n",
    "    elif t.text in glossary:\n",
    "        return t\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_advmod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'advmod':\n",
    "            return child\n",
    "        \n",
    "def extract_nummod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'nummod':\n",
    "            return child\n",
    "\n",
    "def extract_conjunction(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    if t.dep_ == 'conj' and t.pos_ == 'ADJ':\n",
    "        return t \n",
    "\n",
    "def extract_amod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'amod':\n",
    "            return child\n",
    "        \n",
    "def extract_measurements(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    obj = None\n",
    "    measurements = ['wide', 'long']\n",
    "    if t.text in measurements or t.lemma_ in measurements:\n",
    "        obj = doc[t.left_edge.i : t.right_edge.i + 1]\n",
    "    return obj\n",
    "\n",
    "def extract_prepositions(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'prep':\n",
    "            return doc[child.left_edge.i : child.right_edge.i + 1]\n",
    "\n",
    "def define_position(x, y, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    if len(x.text.split()) > 1:\n",
    "        return f'{y.text} {x.text}'\n",
    "    else:\n",
    "        try:\n",
    "            if x.i > y.i:\n",
    "                return doc[y.i : x.i + 1]\n",
    "            else:\n",
    "                return doc[x.i : y.i + 1]\n",
    "        except:\n",
    "            return f'{y.text} {x.text}'\n",
    "\n",
    "def extract_noun_verbs(t, doc):\n",
    "    relations = []\n",
    "    objects = []\n",
    "    if t.dep_ not in ['ROOT', 'nsubj', 'nsubjpass', 'csubj', 'csubjpass']:\n",
    "        return '', ''\n",
    "    parent = next((parent for parent in t.ancestors), None)\n",
    "    if parent and parent.pos_ == 'VERB':\n",
    "        prep = extract_verb_preps(parent, doc)\n",
    "        dobj = extract_verb_dobj(parent, doc)\n",
    "        oprd = extract_verb_orpd(parent, doc)\n",
    "        agnt = extract_verb_agnt(parent, doc)\n",
    "        nmod = extract_verb_nmod(parent, doc)\n",
    "        if prep:\n",
    "            obj = extract_verb_pobj(prep, doc)\n",
    "            if obj:\n",
    "                relations.append(f'{parent.text} {prep}')\n",
    "                objects.append(obj.lemma_)\n",
    "        if dobj:\n",
    "            pass\n",
    "            #print(dobj)\n",
    "        if oprd:\n",
    "            pass\n",
    "            #print(oprd)\n",
    "        if agnt:\n",
    "            pass\n",
    "           # print(agnt)\n",
    "        if nmod:\n",
    "            pass\n",
    "           # print(nmod)\n",
    "        grandparent = next((grandparent for grandparent in parent.ancestors), None)\n",
    "        if not grandparent:\n",
    "            relations.append('is')\n",
    "            numbers = extract_nounandverb_nummods(parent, doc)\n",
    "            if numbers:\n",
    "                objects.append(f'{numbers.text} {parent.text}')\n",
    "            else:\n",
    "                objects.append(parent.text)\n",
    "    if not parent:\n",
    "        for child in t.children:\n",
    "            if child.pos_ == 'VERB' and child.dep_ != 'amod':\n",
    "                                \n",
    "                prep = extract_verb_preps(child, doc)\n",
    "                dobj = extract_verb_dobj(child, doc)\n",
    "                oprd = extract_verb_orpd(child, doc)\n",
    "                agnt = extract_verb_agnt(child, doc)\n",
    "                nmod = extract_verb_nmod(child, doc)\n",
    "                if prep:\n",
    "                    noun = extract_verb_pobj(prep, doc)\n",
    "                    if noun:\n",
    "                        relations.append(f'{child.text} {prep}')\n",
    "                    \n",
    "                        objects.append(doc[noun.left_edge.i : noun.right_edge.i + 1].text)\n",
    "                if dobj:\n",
    "                    relations.append(child.text)\n",
    "                    objects.append(doc[dobj.left_edge.i : dobj.right_edge.i + 1].text)     \n",
    "                if oprd:\n",
    "                    oprd_prep = extract_verb_preps(oprd, doc)\n",
    "                    if oprd_prep:\n",
    "                        relations.append(f'{child.text} {oprd.text}')\n",
    "                        objects.append(doc[oprd_prep.left_edge.i : oprd_prep.right_edge.i + 1].text)  \n",
    "                if agnt:\n",
    "                    continue\n",
    "                    #print(agnt)\n",
    "                if nmod:\n",
    "                    relations.append('is')\n",
    "                    objects.append(f'{nmod.text} {child}') \n",
    "\n",
    "    rel = []\n",
    "    obj = []\n",
    "    for relation, object_ in zip(relations, objects):\n",
    "        rel.append(relation)\n",
    "        obj.append(object_.split(',')[0])\n",
    "            \n",
    "    return rel, obj\n",
    "\n",
    "def extract_verb_nmod(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'nummod':\n",
    "            return doc[child.left_edge.i : child.right_edge.i + 1] \n",
    "        \n",
    "def extract_verb_preps(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'prep':\n",
    "            return child    \n",
    "\n",
    "def extract_verb_pobj(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'pobj' or child.dep_ == 'pcomp' or child.dep_ == 'prep':\n",
    "            return child\n",
    "        \n",
    "def extract_verb_dobj(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'dobj':\n",
    "            return child\n",
    "        \n",
    "def extract_verb_orpd(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'oprd':\n",
    "            return child    \n",
    "        \n",
    "def extract_verb_agnt(t, doc):\n",
    "    \"\"\"HELPER\"\"\"\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'agent':\n",
    "            return child \n",
    "        \n",
    "def extract_nounandverb_nummods(t, doc):\n",
    "    obj = None\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'nummod':\n",
    "            obj = doc[child.left_edge.i : child.right_edge.i + 1]\n",
    "            return obj   \n",
    "\n",
    "def extract_dnummod(t, doc):\n",
    "    obj = extract_nounandverb_nummods(t, doc)\n",
    "    if obj:\n",
    "        return obj.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_noun_adjectives(t, doc):\n",
    "    adjs = []\n",
    "    adjectives = []\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'appos':\n",
    "            continue\n",
    "        if child.pos_ =='ADJ' or child.tag_ == 'VBN' and child.dep_ in ['conj', 'amod']:\n",
    "            \n",
    "            \n",
    "            advmod = extract_advmod(child, doc)\n",
    "            measurement = extract_measurements(child, doc)\n",
    "            prep = extract_prepositions(child, doc)\n",
    "            nummod = extract_nummod(child, doc)\n",
    "            amod = extract_amod(child, doc)\n",
    "           \n",
    "            \n",
    "            if child.lemma_.lower() in compounds:\n",
    "                continue\n",
    "            if child.text.lower() in compounds:\n",
    "                continue\n",
    "            elif amod:\n",
    "                obj = define_position(amod, child, doc)\n",
    "                adjs.append(obj)\n",
    "            elif advmod:\n",
    "                #obj = doc[advmod.i : child.i + 1]\n",
    "                obj = define_position(advmod, child, doc)\n",
    "                adjs.append(obj)\n",
    "            elif measurement:\n",
    "                obj = measurement\n",
    "                adjs.append(obj)\n",
    "            elif prep:\n",
    "                obj = define_position(prep, child, doc)\n",
    "                adjs.append(obj)\n",
    "            elif nummod:\n",
    "                obj = define_position(nummod, child, doc)\n",
    "                adjs.append(obj)                \n",
    "            else:\n",
    "                obj = child\n",
    "                adjs.append(obj)\n",
    "            for grandchild in child.subtree:\n",
    "                conj = extract_conjunction(grandchild, doc)\n",
    "                if conj:\n",
    "                    advmod = extract_advmod(conj, doc)\n",
    "                    prep = extract_prepositions(conj, doc)\n",
    "                    nummod = extract_nummod(conj, doc)\n",
    "                    if advmod:\n",
    "                        obj = define_position(advmod, conj, doc)\n",
    "                        adjs.append(obj)\n",
    "                    elif prep:\n",
    "                        obj = define_position(prep, conj, doc)\n",
    "                        adjs.append(obj)\n",
    "                    elif nummod:\n",
    "                        obj = define_position(nummod, conj, doc)\n",
    "                        adjs.append(obj)\n",
    "                    else:\n",
    "                        obj = conj\n",
    "                        adjs.append(obj)            \n",
    "    for adj in adjs:\n",
    "        try:\n",
    "            if adj.pos_ == 'VERB':\n",
    "                adj_text = adj.text.lower()\n",
    "            elif adj.root.pos_ == 'VERB':\n",
    "                adj_text = adj.text.lower()\n",
    "            else:\n",
    "                adj_text = adj.lemma_.lower()\n",
    "        except:\n",
    "                if type(adj) == str:\n",
    "                    adj_text = adj.lower()\n",
    "                else:\n",
    "                    adj_text = adj.text.lower()\n",
    "        for adj_split in adj_text.split(','):\n",
    "            adjectives.append(adj_split.strip())\n",
    "    return adjectives\n",
    "\n",
    "def extract_noun_appos(t, doc):\n",
    "    appos = []\n",
    "    for child in t.children:\n",
    "        if child.dep_ == 'appos':\n",
    "            obj = doc[child.left_edge.i : child.right_edge.i + 1].text.lower()\n",
    "            for obj_split in obj.split(','):\n",
    "                appos.append(obj_split.strip())\n",
    "    return appos\n",
    "\n",
    "def check_species(t, species, doc):\n",
    "    if t.text in species.split():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def extract_auxillary(t, doc):\n",
    "     parent = next((parent for parent in t.ancestors if parent.pos_ == 'AUX'), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14e43d58-ebcc-4440-8855-5cf858b2da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "URL = 'https://en.wikipedia.org/wiki/Glossary_of_bird_terms'\n",
    "# Get the page\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\", from_encoding=\"iso-8859-1\")   \n",
    "# Find embedded glossary\n",
    "glossaries = soup.find_all('dt', {'class': 'glossary'})\n",
    "parts = [part.text.lower().strip() for part in glossaries]\n",
    "# Get additional anchors (\"also know as...\")\n",
    "glossaries_other = soup.find_all('span', {'class': 'anchor'})\n",
    "parts_other = [part['id'].lower().strip() for part in glossaries_other]\n",
    "# Append and drop duplicates\n",
    "parts = list(set((parts + parts_other)))\n",
    "# Replace underscore with space\n",
    "glossary = [part.replace('_', ' ') for part in parts]\n",
    "\n",
    "\n",
    "# A few helpers\n",
    "additions = [\n",
    "    'legs',\n",
    "    'beak',\n",
    "    'head',\n",
    "    'wingspan',\n",
    "    'eye',\n",
    "    'forecrown'\n",
    "]\n",
    "\n",
    "glossary += additions\n",
    "\n",
    "# Compound helpers\n",
    "compounds = [\n",
    "    'upper', 'lower', 'greater', 'dorsal', 'alternate', 'lesser',\n",
    "    'central', 'outermost', 'outer', 'inner', 'uppermost', 'median',  \n",
    "    'upperwing', 'underwing', 'tail', 'rump',\n",
    "    'secondary', 'primary', 'definitive', 'basic', 'tertial', 'preformative', 'prebasic', 'tertials',\n",
    "    'posterior', 'fresh', \n",
    "    'sexual', 'juvenile', 'male', 'female', 'adult', 'juvenal',\n",
    "    'breeding', 'predefinitive', 'non-breeding',\n",
    "    'raised', 'closed', 'open', \n",
    "    'entire', 'partial',\n",
    "    \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b071c3d9-11f2-42d5-8db3-a7f0a72c399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/List_of_birds_of_the_Netherlands'\n",
    "# Get the page\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\", from_encoding=\"iso-8859-1\") \n",
    "\n",
    "species_NL = []\n",
    "for candidate in soup.find_all('li'):\n",
    "    species = candidate.text.split(',')[0]\n",
    "    species_NL.append(species.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d56605d5-94e0-4c5d-a201-bfdf66813381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all html files\n",
    "species_folder = glob.glob('../data/raw/BOW/*')\n",
    "species_ALL = [link[16:].lower() for link in species_folder]\n",
    "\n",
    "species_NL = list(set(species_ALL) & set(species_NL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83051632-066e-423a-8338-f53eaaec72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('../data/processed/Dutch_birds_descriptions.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df628460-4391-4418-bd36-b58247d521b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1828ca71a3184d1b8bd981cb59d4e16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions = collections.defaultdict(list)\n",
    "descriptions_cleaned = collections.defaultdict(list)\n",
    "\n",
    "for dutch_bird in tqdm_notebook(species_NL[0:20]):\n",
    "    idx = species_ALL.index(dutch_bird)\n",
    "    files = glob.glob(species_folder[idx] + '/*')\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "            dirty_text = soup.get_text(\" \", strip=False).replace('\\n', '.')\n",
    "            # Clean and break into sents\n",
    "            sentences = text_cleaner(dirty_text)\n",
    "\n",
    "            for idx, sentence in enumerate(sentences):\n",
    "                if classify(sentence, model=model):\n",
    "                    doc = nlp(sentence)\n",
    "                    # Init to check empty values\n",
    "                    descriptions[dutch_bird, idx] = []\n",
    "                    triples = []\n",
    "                    # Loop over tokens\n",
    "                    for t in doc:\n",
    "                        if t.dep_ == 'compound':\n",
    "                            continue\n",
    "                        ### SUBJECTS ###    \n",
    "                        if t.pos_ == 'NOUN' or t.pos_ == 'PROPN':\n",
    "                            # Check existance of parts\n",
    "                            part = check_existance(t, doc, glossary=glossary)\n",
    "                            if part:\n",
    "                                # Reconstruct Compounds & Append\n",
    "                                trait = compound_reconstructor(t, doc)\n",
    "                                triples.append(('bird', 'has part', trait))\n",
    "                                # NOUN ADJECTIVES\n",
    "                                adjectives = extract_noun_adjectives(t, doc)\n",
    "                                for adjective in adjectives:\n",
    "                                    triples.append((trait, 'is', adjective))\n",
    "                                # NOUN VERBS\n",
    "                                verbs_rel, verbs_obj = extract_noun_verbs(t, doc)\n",
    "                                for rel, obj in zip(verbs_rel, verbs_obj):\n",
    "                                    triples.append((trait, rel, obj))\n",
    "                                # NOUN APPOSITIONAL MODIFIER\n",
    "                                adjectives = extract_noun_appos(t, doc)\n",
    "                                for adjective in adjectives:\n",
    "                                    triples.append((trait, 'is', adjective))\n",
    "                                # NOUN NUMMODS\n",
    "                                nummod = extract_dnummod(t, doc)\n",
    "                                triples.append((trait, 'is', nummod))\n",
    "\n",
    "                        #if check_species(t, species, doc):\n",
    "\n",
    "                    # APPEND\n",
    "                    descriptions[dutch_bird, idx] = [triple for triple in triples if all(triple)]     \n",
    "                            \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6aaf7c7e-55cf-42d4-aa80-3883fda6ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/dutch_birdv2_unclean.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2687de08-3cb4-43a9-b890-0895feb5a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_cleaned = collections.defaultdict(list)\n",
    "\n",
    "for (bird, idx) in descriptions.keys():\n",
    "    for (sub, rel, obj) in descriptions[(bird, idx)]:\n",
    "        if rel != 'has part':\n",
    "            descriptions_cleaned[bird].append(f'{sub} {obj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "144fa44a-76ce-43fd-a7b6-78a442f14484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptions_cleaned['sooty tern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7290a09c-3ff4-4126-9147-c5239996d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/dutch_birdv2_clean.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptions_cleaned, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410992f-25c5-4c31-a7a8-d7344c717e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
