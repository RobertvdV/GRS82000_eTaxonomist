{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7433918c-9e5b-4055-8d8d-5b7258af2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "from itertools import chain\n",
    "import collections\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4bbcad-10ff-438b-9dcd-cd5b584972e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "URLs = ['https://en.wikipedia.org/wiki/List_of_animal_names',\n",
    "      'https://en.wikipedia.org/wiki/List_of_fish_common_names',\n",
    "      'https://en.wikipedia.org/wiki/List_of_trees_and_shrubs_by_taxonomic_family',\n",
    "      'https://en.wikipedia.org/wiki/List_of_birds_by_common_name']\n",
    "\n",
    "WikiLinks = []\n",
    "\n",
    "for URL in tqdm(URLs):\n",
    "\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Find all wikiparts\n",
    "    WikiRefs = soup.find_all('a')\n",
    "    # Create links \n",
    "    WikiLinks = ['https://en.wikipedia.org' + pages.get('href') for pages in WikiRefs \n",
    "                           if pages.get('href') != None \n",
    "                           if pages.get('href').startswith('/wiki/')]\n",
    "            \n",
    "# Remove duplicates            \n",
    "WikiLinks = list(set(WikiLinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a736bd-3891-4b99-81c6-ff049e792172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11005/11005 [1:16:14<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "127985"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WikiSubLinks = []\n",
    "\n",
    "for WikiLink in tqdm(WikiLinks):\n",
    "\n",
    "    URL = WikiLink\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Find all wikiparts\n",
    "    WikiRefs = soup.find_all('a')\n",
    "    # Create links \n",
    "    WikiSubLinks += ['https://en.wikipedia.org' + pages.get('href') for pages in WikiRefs \n",
    "                           if pages.get('href') != None \n",
    "                           if pages.get('href').startswith('/wiki/')]\n",
    "\n",
    "# Drop doubles\n",
    "WikiSubLinks = list(set(WikiSubLinks))\n",
    "\n",
    "WikiSubLinks += WikiLinks\n",
    "# Remove duplicates            \n",
    "WikiSubLinks = list(set(WikiSubLinks))\n",
    "\n",
    "\n",
    "with open('../data/processed/intermediate_wiki_links.pkl', 'wb') as f:\n",
    "    pickle.dump(WikiLinks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45c46bab-bd9b-4329-ab39-c8927bad2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127985/127985 [11:33:52<00:00,  3.07it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Init Dict\n",
    "Data = collections.defaultdict(list)\n",
    "\n",
    "# Known chapter titles\n",
    "KnownDescriptionList = ['Physical characteristics', 'Description and morphology',\n",
    "                        'Description', 'General description',\n",
    "                        'Appearance',\n",
    "                        'Characteristics']\n",
    "\n",
    "# Lower them\n",
    "KnownDescriptionListLower =[i.lower() for i in KnownDescriptionList]\n",
    "\n",
    "# Removes references in text\n",
    "ReferenceRemover = '\\[\\d*\\]'\n",
    "\n",
    "for WikiLink in tqdm(WikiSubLinks):\n",
    "    \n",
    "    URL = WikiLink\n",
    "\n",
    "    try:\n",
    "        Response = requests.get(URL, timeout=5)\n",
    "        Soup = BeautifulSoup(Response.content, \"html.parser\")\n",
    "\n",
    "        # Check if the link contains description data\n",
    "        Contents = [i.text.replace('[edit]', '') for i in Soup.find_all('h2')]\n",
    "        if all(s not in Contents for s in KnownDescriptionList) == False:\n",
    "\n",
    "            # Find Chapters\n",
    "            for Tag in Soup.find_all('h2')[1:]:\n",
    "\n",
    "\n",
    "                # Get species name\n",
    "                Species = Soup.title.string\\\n",
    "                                    .split(' - ')[0]\\\n",
    "                                    .rstrip(' ')\n",
    "\n",
    "                # Clean Chapter\n",
    "                Chapter = Tag.text.strip().replace('[edit]', '')\n",
    "\n",
    "                # Find all Text\n",
    "                for Item in Tag.find_next_siblings('p'):\n",
    "                    # Check if Chapter is Description or similar\n",
    "                    if Chapter.lower() in KnownDescriptionListLower:\n",
    "                        # Check if text belongs to current chapter\n",
    "                        if Chapter in Item.find_previous_siblings('h2')[0].text.strip():\n",
    "                            # Clean Text\n",
    "                            CleanText = re.sub(ReferenceRemover, '', Item.text)\n",
    "                            # Add to dict\n",
    "                            Data[Species].append(tuple([1, CleanText]))\n",
    "\n",
    "                    else:\n",
    "                        # Check if text belongs to current chapter\n",
    "                        if Chapter in Item.find_previous_siblings('h2')[0].text.strip():\n",
    "                            # Clean Text\n",
    "                            CleanText = re.sub(ReferenceRemover, '', Item.text)\n",
    "                            # Add to dict\n",
    "                            Data[Species].append(tuple([0, CleanText]))\n",
    "        \n",
    "        # Non species part\n",
    "        else:\n",
    "            try:\n",
    "                # Find Chapters\n",
    "                for Tag in Soup.find_all('h2')[1:]:\n",
    "                    # Clean Chapter\n",
    "                    Chapter = Tag.text.strip().replace('[edit]', '')\n",
    "                    # Find all Text\n",
    "                    for Item in Tag.find_next_siblings('p'):\n",
    "                        # Check if text belongs to current chapter\n",
    "                        if Chapter in Item.find_previous_siblings('h2')[0].text.strip():\n",
    "                            #print(Chapter)\n",
    "                            # Clean Text\n",
    "                            CleanText = re.sub(ReferenceRemover, '', Item.text)\n",
    "                            # Add to dict\n",
    "                            Data['no_data'].append(tuple([0, CleanText]))\n",
    "            # Continue on wrong links                \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "    except:\n",
    "        # Continue if timeout\n",
    "        continue\n",
    "\n",
    "with open('../data/processed/train_dataWIKI_all.pkl', 'wb') as f:\n",
    "    pickle.dump(Data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5745e-dde5-47f9-8d6d-801acdf29d1a",
   "metadata": {},
   "source": [
    "## Reduce memory for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a62202-3b19-4a26-b2cf-d21b031780af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b5be1b-a2d6-4328-b7e5-84ee5b0bcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pickle.load(open('../data/processed/train_dataWIKI_all.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad65f5c9-f313-4fd8-b2cf-e1b6edbef26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_values = (list(chain.from_iterable(Data.values())))\n",
    "data_values = list(set(data_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c567bc5-7d71-46ed-ae95-fd0c63941451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687064"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56e7487-a3bd-456f-9c6f-dd495b5199a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce RAM on Colab\n",
    "no_data = Data['no_data']\n",
    "no_data_slim = list(set(random.sample(no_data, 200000)))\n",
    "\n",
    "data_slimmed = copy.deepcopy(Data)\n",
    "data_slimmed['no_data'] = no_data_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391b82cb-7c47-4e91-809a-b100d8287652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687064"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_values_slim = (list(chain.from_iterable(data_slimmed.values())))\n",
    "data_values_slim = list(set(data_values))\n",
    "len(data_values_slim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd999a2-3cae-43d4-96f4-87b2b0b40582",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/COLAB_train_dataWIKI_all.pkl', 'wb') as f:\n",
    "    pickle.dump(data_slimmed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43f7a5bf-328b-4290-b5c6-0dcaa33dfb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8666"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_slimmed.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27e7d1bf-c276-4241-9e6a-4afb114d9f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  'On June 6, 1956, Bingham died at his Washington, D.C. home. He was interred at Arlington National Cemetery in Virginia.\\n'),\n",
       " (0,\n",
       "  \"The opportunity to edit without linking to a person's real world identity provides a degree of confidentiality to those who could be placed at risk if they edited using their legal names. This confidentiality is not guaranteed, however, and is largely dependent on editors withholding personal information about themselves.\\n\"),\n",
       " (0,\n",
       "  \"Very low sounds are also produced in various species of Coleoptera, Hymenoptera, Lepidoptera, Mantodea and Neuroptera. These low sounds are simply the sounds made by the insect's movement. Through microscopic stridulatory structures located on the insect's muscles and joints, the normal sounds of the insect moving are amplified and can be used to warn or communicate with other insects. Most sound-making insects also have tympanal organs that can perceive airborne sounds. Some species in Hemiptera, such as the corixids (water boatmen), are known to communicate via underwater sounds. Most insects are also able to sense vibrations transmitted through surfaces.\\n\"),\n",
       " (0,\n",
       "  \"Notes: (a) The 2000 Census populations for Parigi Moutong Regency and Sigi Regency are included in the figure for Donggala Regency. (b) the province's BPS figure is 257,585 as shown, but the \\nregency's BPS figure is only 239,421. (c) The 2000 Census population for Tojo Una-Una Regency is included in the figure for Poso Regency. (d) The 2000 Census population for Banggai Laut Regency is included in the figure for Banggai Kepulauan Regency. (e) The 2000 Census population for Morowali Utara Regency is included in the figure for Morowali Regency.\\n\"),\n",
       " (0,\n",
       "  '(See also Atlanta in the Civil War, Charleston, South Carolina, in the Civil War, Nashville in the Civil War, New Orleans in the Civil War, Wilmington, North Carolina, in the American Civil War, and Richmond in the Civil War).\\n'),\n",
       " (0,\n",
       "  'In the 1950s–1960s, as the Siamese was increasing in popularity, many breeders and cat show judges began to favor the more slender look. As a result of generations of selective breeding, they created increasingly long, fine-boned, narrow-headed cats. Eventually, the modern show Siamese was bred to be extremely elongated, with a lean, tubular body, long, slender legs, a very long, very thin tail that tapers gradually into a point and a long, wedge-shaped head topped by extremely large, wide-set ears.\\n'),\n",
       " (0,\n",
       "  'Wikipedia breathes new life into one of the initial dreams of the World Wide Web: hyperlinks. Hyperlinks allow Wikipedia authors to link any word or phrase to another Wikipedia article, often providing annotations of great value. Background information to an  article no longer needs to be limited or even produced by the author of the article. This method has proved to have major limitations on the Internet as a whole, because for a variety of reasons links are prone to quickly become obsolete. However, internal links within Wikipedia can be made with confidence, and so Wikipedia serves a web of mutually supporting information.\\n'),\n",
       " (0,\n",
       "  'The main crops for which Bahawalpur is recognised are cotton, sugarcane, wheat, sunflower seeds, rape/mustard seed and rice. Bahawalpur mangoes, citrus, dates and guavas are some of the fruits exported out of the country. Vegetables include onions, tomatoes, cauliflower, potatoes and carrots. Being an expanding industrial city, the government has revolutionised and liberalised various markets allowing the caustic soda, cotton ginning and pressing, flour mills, fruit juices, general engineering, iron and steel re-rolling mills, looms, oil mills, poultry feed, sugar, textile spinning, textile weaving, vegetable ghee and cooking oil industries to flourish.\\n'),\n",
       " (0,\n",
       "  \"Palo Alto's retail and restaurant trade includes Stanford Shopping Center, an upscale open air shopping center established in 1955, and downtown Palo Alto, centered on University Avenue.\\n\"),\n",
       " (0,\n",
       "  'Amphibians are common in the capital, including smooth newts living by the Tate Modern, and common frogs, common toads, palmate newts and great crested newts. On the other hand, native reptiles such as slowworms, common lizards, barred grass snakes and adders, are mostly only seen in Outer London.\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_slimmed['no_data'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d2315-a199-4888-91b9-12a449601492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eTaxonomist]",
   "language": "python",
   "name": "conda-env-eTaxonomist-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
