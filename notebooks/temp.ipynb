{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9a12586-594f-4408-aadf-6eaa00bedfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import transformers\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from selenium import webdriver\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../src/models/')\n",
    "import predict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ae25068-6d80-4352-8965-b1433d42df68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "# Load BERT\n",
    "model = predict_model.loadBERT(\"../models/\", 'model_weights_splitted_reducednegatives.pt')\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a950c35-fcd8-4b1e-9bf0-5b6fa4ef6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpanPredictor(span, pred_values=False, threshold=False):\n",
    "      \n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean text\n",
    "    TextCleaner = [\n",
    "        '\\(\\d+.+?Close\\n\\t\\n\\)',\n",
    "        '\\[\\d*\\]',\n",
    "        '\\([^)]*\\)',\n",
    "        '<[^<]+>',\n",
    "    ]\n",
    "    \n",
    "    for Cleaner in TextCleaner:\n",
    "        span = re.sub(Cleaner, '', span, flags=re.DOTALL)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        # Print the prediction values\n",
    "        if pred_values:\n",
    "            return span_class, exps[0]\n",
    "        else:\n",
    "            return span_class    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "619c293a-e43f-4edc-888b-b30cd01cda3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gp/hp50s5114x52591qbdhn43xm0000gn/T/ipykernel_1044/1291267063.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mipni_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/processed/intermediate_ipni_links_trees.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Extract the indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mipni_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.ipni.org/n/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mipni_links\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create powo links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "ipni_links = pickle.load(open('../data/processed/intermediate_ipni_links_trees.pkl', 'rb'))\n",
    "\n",
    "# Extract the indices\n",
    "ipni_index = [link.strip('https://www.ipni.org/n/') for link in ipni_links]\n",
    "# Create powo links\n",
    "powo_links = ['http://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:' + index for index in ipni_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b5fcbe8-5483-4a8b-8d8a-3e88c21b45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.ipni.org/?perPage=500&page=1&q=species%3A*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f694762-49f3-4ab3-bcce-57d7152e0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "161124bc-535b-4deb-bc86-ccd3540ae141",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for span in soup.find_all('dd'):\n",
    "    span = span.text.strip()\n",
    "    preds.append((span, SpanPredictor(span, pred_values=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f58abb7-4ee5-408e-82c9-67bbc410f5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ceiba pentandra (L.) Gaertn.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.text.split(' | ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6fa66e4-493e-421f-9e8d-982383e0bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [ref for ref in soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb94186-cd3a-47c5-b853-8a586897977b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
