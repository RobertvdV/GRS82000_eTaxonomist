{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e425c0-997e-4b73-9e55-bebf062c7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "import transformers\n",
    "import torch\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../src/models/')\n",
    "import predict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7dd4b7-fa55-4769-bdbd-a5ac5696d151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "# Load BERT\n",
    "model = predict_model.loadBERT(\"../models/\", 'model_weights_splitted_reducednegatives.pt')\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037d3509-d54f-49a1-b223-18a956fd3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpanPredictor(span, pred_values=False, threshold=False):\n",
    "      \n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean text\n",
    "    TextCleaner = [\n",
    "        '\\(\\d+.+?Close\\n\\t\\n\\)',\n",
    "        '\\[\\d*\\]',\n",
    "        '\\([^)]*\\)',\n",
    "        '<[^<]+>',\n",
    "    ]\n",
    "    \n",
    "    for Cleaner in TextCleaner:\n",
    "        span = re.sub(Cleaner, '', span, flags=re.DOTALL)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        # Print the prediction values\n",
    "        if pred_values:\n",
    "            return span_class, exps[0]\n",
    "        else:\n",
    "            return span_class    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2197a673-7150-4fa3-973c-42cde4726356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor([0.9602, 0.0398]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\"\n",
    "The European has a black bill and orange breast.\n",
    "\"\"\"\n",
    "SpanPredictor(string, pred_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff001b56-9def-4981-bc7e-cda071203f1a",
   "metadata": {},
   "source": [
    "## Gentry Wood Plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c489377-d9c2-4474-970d-039db7788411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init regex pattern\n",
    "pattern = '\\(\\d+\\sspp[^)]*\\)'\n",
    "# Init dict\n",
    "data_pdf = collections.defaultdict(list)\n",
    "\n",
    "# Open a PDF file\n",
    "with pdfplumber.open(\"../data/external/Gentry_woodyplants.pdf\") as pdf:\n",
    "    \n",
    "    # Skip first pages\n",
    "    #for i in tqdm(range(4, len(pdf.pages) -1)):\n",
    "    for i in tqdm(range(4, 200)):\n",
    "        # Get page\n",
    "        page = pdf.pages[i]\n",
    "\n",
    "        # Clip top and split page\n",
    "        left = page.crop((0, 0.0 * float(page.height), 0.5 * float(page.width), 1.0 * float(page.height)))\n",
    "        right = page.crop((0.5 * float(page.width), 0.0 * float(page.height), page.width, 1.0 * float(page.height)))\n",
    "\n",
    "        combined = [left, right]\n",
    "        for part in combined:\n",
    "\n",
    "            # Extract text\n",
    "            text = page.extract_text()\n",
    "            # Split on \\n\n",
    "            text_list = text.split('\\n')\n",
    "            # Join text\n",
    "            text_page = ''.join(text_list)\n",
    "            # Search for the patterns and index\n",
    "            split_index = [m.start(0) for m in re.finditer(pattern, text_page)]\n",
    "            # Insert zero for species\n",
    "            split_index.insert(0, 0)\n",
    "            # Split on the found indices\n",
    "            text_page_indices = [text_page[i:j].strip() for i,j in zip(split_index, split_index[1:] + [None])]\n",
    "            # Extract species\n",
    "            species_list = []\n",
    "            for text_blocks in text_page_indices[:-1]:\n",
    "                _, *_, species = text_blocks.split()\n",
    "                species_list.append(species)\n",
    "            # Clean the text\n",
    "            text_cleaned = [re.sub(pattern, '', span) for span in text_page_indices[1:]]\n",
    "            text_cleaned = [span.lstrip('- ').lstrip(' - ') for span in text_cleaned]\n",
    "            try:\n",
    "                # Remove species\n",
    "                text_cleaned_last = text_cleaned[-1]\n",
    "                text_cleaned = [span.replace(species, '') for span, species in zip(text_cleaned[:-1], species_list[1:])]\n",
    "                text_cleaned += [text_cleaned_last]\n",
    "\n",
    "                for span, species in zip(text_cleaned, species_list):\n",
    "                    # Get predictions\n",
    "                    if SpanPredictor(span):\n",
    "                        data_pdf[species].append(span)\n",
    "                    else:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "with open('../data/processed/description_pdf_Gentry_woodyplants_trees.pkl', 'wb') as f:\n",
    "    pickle.dump(data_pdf, f)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9b21146-5ac6-4b98-90ac-376daa79fcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Init list\n",
    "book_list = []\n",
    "\n",
    "# Open a PDF file\n",
    "with pdfplumber.open(\"../data/external/Gentry_woodyplants.pdf\") as pdf:\n",
    "    \n",
    "    # Skip first pages\n",
    "    #for i in tqdm(range(4, len(pdf.pages) -1)):\n",
    "    for i in tqdm(range(19, 21)):\n",
    "        # Get page\n",
    "        page = pdf.pages[i]\n",
    "\n",
    "        # Clip top and split page\n",
    "        left = page.crop((0, 0.1 * float(page.height), 0.5 * float(page.width), 1.0 * float(page.height)))\n",
    "        right = page.crop((0.5 * float(page.width), 0.1 * float(page.height), page.width, 1.0 * float(page.height)))\n",
    "        \n",
    "        # Extract text\n",
    "        text_list = [left.extract_text(), right.extract_text()]\n",
    "        page_list = [left, right]\n",
    "        if not text_list[0]:\n",
    "            text_list[0] = ''\n",
    "        if not text_list[1]:\n",
    "            text_list[1] = ''\n",
    "        \n",
    "        # Loop over left and right.\n",
    "        for text_part, page_part in zip(text_list, page_list):\n",
    "            # Read the characters\n",
    "            char_list = [(each_char[\"text\"], int(each_char[\"top\"])) for each_char in page_part.chars]\n",
    "            \n",
    "            # Init list\n",
    "            pos_list = []            \n",
    "            pos_list_with_char = []\n",
    "            # Loop over characters\n",
    "            for i, pos in enumerate(char_list):\n",
    "                # Check if values is already there\n",
    "                if pos[1] not in pos_list:\n",
    "                    # Append new high values\n",
    "                    pos_list.append(pos[1])\n",
    "                    pos_list_with_char.append(pos)\n",
    "                # Continue on known values\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            # Get the difference\n",
    "            pos_diff = [x[1] - pos_list_with_char[i - 1][1] for i, x in enumerate(pos_list_with_char) \n",
    "                        if (x[1] - pos_list_with_char[i - 1][1]) > 5]\n",
    "            \n",
    "            # Check enters\n",
    "            # Init empty list\n",
    "            span_list = [''] * 40\n",
    "            # Add end number\n",
    "            pos_diff.append(5)\n",
    "            # Init list\n",
    "            test_text_list = []\n",
    "            # Init counter\n",
    "            count = 0\n",
    "            # Loop over values\n",
    "            for position, text in zip(pos_diff, text_part.split('\\n')):\n",
    "                # If lower no enter\n",
    "                if position < 20:\n",
    "                    span_list[count] = span_list[count] + text\n",
    "                # Else enter\n",
    "                else:\n",
    "                    span_list[count] = span_list[count] + text\n",
    "                    # Update count\n",
    "                    count += 1\n",
    "                    \n",
    "            # Remove empties\n",
    "            span_list = [span.strip() for span in span_list if len(span.split()) > 1]\n",
    "        \n",
    "            # Append to booklist\n",
    "            book_list += span_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20004eb1-2437-4019-b00c-7d3daa18609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbefaa4-bd00-4598-85c3-5d96dfc61174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gentry_woodyplants = pickle.load(open('../data/processed/description_pdf_Gentry_woodyplants_trees.pkl', 'rb'))\n",
    "\n",
    "#len(Gentry_woodyplants.keys())\n",
    "#Gentry_woodyplants.keys()\n",
    "#Gentry_woodyplants['Agave'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b57ca-ce30-4ed1-a619-db497d7197e6",
   "metadata": {},
   "source": [
    "## Trees of Peru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4226453c-2877-4ceb-821e-a14b2e136ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.02it/s]\n",
      "100%|█████████████████████████████████████████| 166/166 [00:10<00:00, 15.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Init list\n",
    "book_list_cleaned = []\n",
    "# Init counter\n",
    "contents_counter = 0\n",
    "# Init patterns\n",
    "TextCleaner = [\n",
    "    '\\(\\d+.+?Close\\n\\t\\n\\)',\n",
    "    '\\[\\d*\\]',\n",
    "    '\\([^)]*\\)',\n",
    "    '<[^<]+>',\n",
    "]\n",
    "\n",
    "with pdfplumber.open(\"../data/external/Trees of Peru.pdf\") as pdf:\n",
    "    \n",
    "    #print(len(pdf.pages))\n",
    "    \n",
    "    book_list = []\n",
    "    \n",
    "    for i in tqdm(range(29, 31)):\n",
    "        page = pdf.pages[i]\n",
    "        \n",
    "    \n",
    "        left = page.crop((0, 0.05 * float(page.height), 0.6 * float(page.width), 0.85 * float(page.height)))\n",
    "        right = page.crop((0.6 * float(page.width), 0.05 * float(page.height), page.width, 0.85 * float(page.height)))\n",
    "\n",
    "\n",
    "        # Extract text\n",
    "        text_left = left.extract_text()\n",
    "        text_right = right.extract_text()\n",
    "        text = text_left + '\\n' + text_right\n",
    "        text_list = text.split('\\n')\n",
    "        \n",
    "        # Append to booklist\n",
    "        book_list += text_list\n",
    "        \n",
    "for sentence in book_list:\n",
    "    # Clean the text\n",
    "    for Cleaner in TextCleaner:\n",
    "        sentence = re.sub(Cleaner, '', sentence, flags=re.DOTALL)\n",
    "    \n",
    "    # Drop useless figures and content pages\n",
    "    if sentence.startswith('Fig'):\n",
    "        book_list_cleaned.append('-_-')\n",
    "    elif re.match(r'Key to the genera', sentence):\n",
    "        # Update counter\n",
    "        contents_counter = 4\n",
    "        book_list_cleaned.append('-_-')\n",
    "    elif re.match(r'1\\.', sentence) and contents_counter != 3:\n",
    "        contents_counter = 0\n",
    "        book_list_cleaned.append(sentence)     \n",
    "    elif contents_counter > 0:\n",
    "        # Check if still in contents page\n",
    "        if re.match(r'\\d+\\.', sentence):\n",
    "            # Update counter\n",
    "            contents_counter += 4\n",
    "        book_list_cleaned.append('')       \n",
    "    else:\n",
    "        book_list_cleaned.append(sentence)\n",
    "    # Update counter   \n",
    "    contents_counter -= 1\n",
    "    \n",
    "# Init dict\n",
    "data_pdf = collections.defaultdict(list)\n",
    "\n",
    "# Index the found families\n",
    "name_index = [count for count, span in enumerate(book_list_cleaned) \n",
    "                if re.match(r'\\d+\\.\\s*[A-z]+', span) \n",
    "                if span[0].isdigit()]\n",
    "\n",
    "# counter\n",
    "current = 0 \n",
    "# Loop over the spans\n",
    "for idx, span in enumerate(tqdm(book_list_cleaned)):\n",
    "    # Skip no family/genus\n",
    "    if idx < name_index[0]:\n",
    "        continue\n",
    "    try:\n",
    "        # Check the index\n",
    "        if idx == name_index[current]:\n",
    "            # Clean the family/genus name\n",
    "            name = re.findall(\"[[A-z]+\", book_list_cleaned[idx])[0].lower().capitalize()\n",
    "            \n",
    "            # Update counter\n",
    "            current += 1\n",
    "            \n",
    "        # Add if descriptions\n",
    "        if SpanPredictor(span):\n",
    "            data_pdf[name].append(span)\n",
    "        else:\n",
    "            #continue\n",
    "            ### DEBUGGINg\n",
    "            data_pdf[name].append(SpanPredictor(span, pred_values=True)[1])\n",
    "        \n",
    "\n",
    "    # Catch the end exeption\n",
    "    except:\n",
    "        if SpanPredictor(span):\n",
    "            data_pdf[name].append(span)\n",
    "        else:\n",
    "            #continue\n",
    "            ### DEBUGGINg\n",
    "            data_pdf[name].append(SpanPredictor(span, pred_values=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca4a17d8-3b10-4eca-abd4-ac0ca1642186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Palmae', 'Chelyocarpus', 'Itaya', 'Mauritia'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57fb135f-0aa7-4dab-8c9a-dc11ae7814cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.9835, 0.0165]),\n",
       " tensor([0.9969, 0.0031]),\n",
       " tensor([0.9919, 0.0081]),\n",
       " tensor([0.9942, 0.0058]),\n",
       " 'Unarmed trees, stems solitary; leaf sheath densely woolly and longitudinally split, peti-',\n",
       " 'ole long, with smooth margins, leaves palmate, 11-25, blade orbicular in outline and split',\n",
       " 'into wedge-shaped induplicate segments with serrate tips; inflorescence axillary, pendulous,',\n",
       " 'branched to 2orders, subtended by woolly bracts, inflorescence branches numerous; flowers',\n",
       " 'solitary, bisexual; sepals 3, fused; petals 3, fused to halfway, valvate; stamens 18-24; ovary',\n",
       " tensor([0.5749, 0.4251]),\n",
       " tensor([0.9932, 0.0068]),\n",
       " tensor([0.9975, 0.0025]),\n",
       " 'The genus isclose to Chelyocarpus but differs inthe split leaf sheath, partly fused sepals',\n",
       " tensor([0.6710, 0.3290]),\n",
       " tensor([0.9601, 0.0399]),\n",
       " 'orbicular in outline, c2m dia., split to the base into 10-15 segments, undersurface whitish,',\n",
       " 'flowers creamish white, fruit 2-2,5 cm long, greenish yellow; known only from near Iquitos,',\n",
       " tensor([0.9918, 0.0082])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pdf['Itaya'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af829899-1a6c-40f6-9e84-a3d9443c68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def replace_ending(sentence):\n",
    "    \n",
    "    \"\"\"\n",
    "    Replace the ending of a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if sentence.endswith('.'):\n",
    "        return sentence[:-len('.')] + ' KAAAAS'\n",
    "    return sentence\n",
    "\n",
    "pattern = r\"[A-Z][A-Z]+AE\"\n",
    "\n",
    "# Init\n",
    "data_pdf = collections.defaultdict(list)\n",
    "\n",
    "with pdfplumber.open(\"../data/external/Trees of Peru.pdf\") as pdf:\n",
    "    \n",
    "    #print(len(pdf.pages))\n",
    "    \n",
    "    book_list_cleaned = []\n",
    "    \n",
    "    for i in tqdm(range(27, 40)):\n",
    "        page = pdf.pages[i]\n",
    "        \n",
    "    \n",
    "        left = page.crop((0, 0.05 * float(page.height), 0.6 * float(page.width), 0.85 * float(page.height)))\n",
    "        right = page.crop((0.6 * float(page.width), 0.05 * float(page.height), page.width, 0.85 * float(page.height)))\n",
    "\n",
    "\n",
    "        # Extract text\n",
    "        text_left = left.extract_text()\n",
    "        text_right = right.extract_text()\n",
    "        text = text_left + '\\n' + text_right\n",
    "        \n",
    "        # Clean the text\n",
    "        text_newline_replaced = text.replace('\\n', ' \\n ')\n",
    "        # Rejoin multilines\n",
    "        text_joined_multilines = text_newline_replaced.replace('- \\n ', '')\n",
    "        # Split into list\n",
    "        text_list = text_joined_multilines.split(' \\n ')\n",
    "        # Clean list\n",
    "        text_list_cleaned = [line for line in text_list \n",
    "                             if not line.startswith('Fig.')]\n",
    "                             #if not len(re.split('- |, ', line)) == 2\n",
    "                             #if len(line.split(' ')) > 1]\n",
    "\n",
    "        # Replace the ending to get spans\n",
    "        text_list_endings_replaced = [replace_ending(line) for line in text_list_cleaned]\n",
    "        # Rejoin for spans\n",
    "        text_cleaned = ' '.join(text_list_endings_replaced).split(' KAAAAS')\n",
    "    \n",
    "        # Remove contents\n",
    "        book_list_cleaned += [span.strip() for span in text_cleaned if span.count('.') < 10]\n",
    "        \n",
    "    # Index the found families\n",
    "    family_index = [count for count, span in enumerate(book_list_cleaned) if re.findall(pattern, span)]\n",
    "\n",
    "    # counter\n",
    "    current = 0 \n",
    "    # Loop over the spans\n",
    "    for idx, span in enumerate(tqdm(book_list_cleaned)):\n",
    "        # Skip no family\n",
    "        if idx < family_index[0]:\n",
    "            continue\n",
    "        try:\n",
    "            # Check the index\n",
    "            if idx == family_index[current]:\n",
    "                # Clean the family name\n",
    "                family_name = re.findall(pattern, book_list_cleaned[idx])[0].lower().capitalize()\n",
    "                # Update counter\n",
    "                current += 1\n",
    "            # Add if descriptions\n",
    "            #if SpanPredictor(span):\n",
    "            #    data_pdf[family_name].append(span)\n",
    "\n",
    "            ##### TESTING\n",
    "            data_pdf[family_name].append(tuple([span, SpanPredictor(span, pred_values=True)]))\n",
    "        # Catch the end exeption\n",
    "        except:\n",
    "            #if SpanPredictor(span):\n",
    "            #    data_pdf[family_name].append(span)\n",
    "            \n",
    "            ##### TESTING\n",
    "            data_pdf[family_name].append(tuple([span, SpanPredictor(span, pred_values=True)]))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
