{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c7a0cb57-c660-4414-9bf0-ca7b42d7a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import DistilBertModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertConfig, DistilBertTokenizer\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/models/')\n",
    "import predict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "840beda8-fad4-47f5-9650-00e0a4d76434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = predict_model.loadBERT(\"../models/\", 'saved_weights_inf_FIXED_boot.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a8e0bd8b-08da-47e1-ae2c-9f9bd1a03292",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "74bd8319-6399-4990-a353-22df12f496fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizeTextAttention(Dataset):\n",
    "    def __init__(self, sentences, tokenizer):\n",
    "        \n",
    "        \"\"\"\n",
    "        Dataload for small setence dataset, to visualize the words\n",
    "        used for making the prediction\n",
    "        \n",
    "        text_list : List of sentences to be visualized.\n",
    "        tokenizer : Tokenizer to embed to sentence.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.text_list = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self._init_dataset()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #input_ids = self.text_list[idx][\"input_ids\"].squeeze()\n",
    "        #masks = self.text_list[idx][\"attention_mask\"].squeeze()\n",
    "        #return {\"input_ids\": input_ids, \"attention_mask\": masks}\n",
    "        \n",
    "        return self.text_list[idx]\n",
    "    \n",
    "    def _init_dataset(self):\n",
    "        \n",
    "        for sentence in self.sentences:\n",
    "            tokens = self.tokenizer(\n",
    "                           sentence,\n",
    "                           max_length=512,\n",
    "                           truncation=True, \n",
    "                           padding='max_length',\n",
    "                           return_tensors='pt')\n",
    "            \n",
    "            # Remove batch dimension\n",
    "            #tokens['input_ids'] = tokens['input_ids'].squeeze()\n",
    "            #tokens['attention_mask'] = tokens['attention_mask'].squeeze()\n",
    "            \n",
    "            self.text_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d1ffdb98-0d96-4a7a-834d-970486458129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SaliencyInterpreter:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 criterion,\n",
    "                 tokenizer,\n",
    "                 show_progress=True,\n",
    "                 **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        :param model: nn.Module object - can be HuggingFace's model or custom one.\n",
    "        :param criterion: torch criterion used to train your model.\n",
    "        :param tokenizer: HuggingFace's tokenizer.\n",
    "        :param show_progress: bool flag to show tqdm progress bar.\n",
    "        :param kwargs:\n",
    "            encoder: string indicates the HuggingFace's encoder, that has 'embeddings' attribute. Used\n",
    "                if your model doesn't have 'get_input_embeddings' method to get access to encoder embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.criterion = criterion\n",
    "        self.tokenizer = tokenizer\n",
    "        self.show_progress = show_progress\n",
    "        self.kwargs = kwargs\n",
    "        # To save outputs in saliency_interpret\n",
    "        self.batch_output = None\n",
    "\n",
    "    def _get_gradients(self, batch):\n",
    "        # Set requires_grad to true for all parameters, but save original values to restore them later\n",
    "        original_param_name_to_requires_grad_dict = {}\n",
    "        for param_name, param in self.model.named_parameters():\n",
    "            original_param_name_to_requires_grad_dict[param_name] = param.requires_grad\n",
    "            param.requires_grad = True\n",
    "        embedding_gradients = []\n",
    "        hooks = self._register_embedding_gradient_hooks(embedding_gradients)\n",
    "\n",
    "        loss = self.forward_step(batch)\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # Restore the original requires_grad values of the parameters\n",
    "        for param_name, param in self.model.named_parameters():\n",
    "            param.requires_grad = original_param_name_to_requires_grad_dict[param_name]\n",
    "\n",
    "        return embedding_gradients[0]\n",
    "\n",
    "    def _register_embedding_gradient_hooks(self, embedding_gradients):\n",
    "        \"\"\"\n",
    "        Registers a backward hook on the\n",
    "        Used to save the gradients of the embeddings for use in get_gradients()\n",
    "        When there are multiple inputs (e.g., a passage and question), the hook\n",
    "        will be called multiple times. We append all the embeddings gradients\n",
    "        to a list.\n",
    "        \"\"\"\n",
    "\n",
    "        def hook_layers(module, grad_in, grad_out):\n",
    "            embedding_gradients.append(grad_out[0])\n",
    "\n",
    "        backward_hooks = []\n",
    "        embedding_layer = self.get_embeddings_layer()\n",
    "        backward_hooks.append(embedding_layer.register_backward_hook(hook_layers))\n",
    "        return backward_hooks\n",
    "\n",
    "    def get_embeddings_layer(self):\n",
    "        if hasattr(self.model, \"get_input_embeddings\"):\n",
    "            embedding_layer = self.model.get_input_embeddings()\n",
    "        else:\n",
    "            encoder_attribute = self.kwargs.get(\"encoder\")\n",
    "            assert encoder_attribute, \"Your model doesn't have 'get_input_embeddings' method, thus you \" \\\n",
    "                \"have provide 'encoder' key argument while initializing SaliencyInterpreter object\"\n",
    "            embedding_layer = getattr(self.model, encoder_attribute).embeddings\n",
    "        return embedding_layer\n",
    "\n",
    "    def colorize(self, instance, skip_special_tokens=False):\n",
    "\n",
    "        special_tokens = self.special_tokens\n",
    "\n",
    "        word_cmap = matplotlib.cm.Blues\n",
    "        prob_cmap = matplotlib.cm.Greens\n",
    "        template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
    "        colored_string = ''\n",
    "        # Use a matplotlib normalizer in order to make clearer the difference between values\n",
    "        normalized_and_mapped = matplotlib.cm.ScalarMappable(cmap=word_cmap).to_rgba(instance['grad'])\n",
    "        for word, color in zip(instance['tokens'], normalized_and_mapped):\n",
    "            if word in special_tokens and skip_special_tokens:\n",
    "                continue\n",
    "            # Handle wordpieces\n",
    "            word = word.replace(\"##\", \"\") if \"##\" in word else ' ' + word\n",
    "            color = matplotlib.colors.rgb2hex(color[:3])\n",
    "            colored_string += template.format(color, word)\n",
    "        #colored_string += template.format(0, \"    Label: {} |\".format(instance['label']))\n",
    "        prob = instance['prob']\n",
    "        color = matplotlib.colors.rgb2hex(prob_cmap(prob)[:3])\n",
    "        #colored_string += template.format(color, \"{:.2f}%\".format(instance['prob']*100)) + '|'\n",
    "        return colored_string\n",
    "\n",
    "    @property\n",
    "    def special_tokens(self):\n",
    "        \"\"\"\n",
    "        Some tokenizers don't have 'eos_token' and 'bos_token' attributes.\n",
    "        So needed we some trick to get them.\n",
    "        \"\"\"\n",
    "        if self.tokenizer.bos_token is None or self.tokenizer.eos_token is None:\n",
    "            special_tokens = self.tokenizer.build_inputs_with_special_tokens([])\n",
    "            special_tokens_ids = self.tokenizer.convert_ids_to_tokens(special_tokens)\n",
    "            self.tokenizer.bos_token, self.tokenizer.eos_token = special_tokens_ids\n",
    "\n",
    "        special_tokens = self.tokenizer.eos_token, self.tokenizer.bos_token\n",
    "        return special_tokens\n",
    "\n",
    "    def forward_step(self, batch):\n",
    "        \"\"\"\n",
    "        If your model receive inputs in another way or you computing not\n",
    "         like in this example simply override this method. It should return the batch loss\n",
    "        :param batch: batch returned by dataloader\n",
    "        :return: torch.Tensor: batch loss\n",
    "        \"\"\"\n",
    "        input_ids = batch.get('input_ids').to(self.device)\n",
    "        attention_mask = batch.get(\"attention_mask\").to(self.device)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #print(outputs.shape)\n",
    "        #print(outputs)\n",
    "        \n",
    "        label = torch.argmax(outputs, dim=1)\n",
    "        batch_losses = self.criterion(outputs, label)\n",
    "        loss = torch.mean(batch_losses)\n",
    "\n",
    "        self.batch_output = [input_ids, outputs]\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_output(self):\n",
    "        \"\"\"\n",
    "        You can also override this method if you want to change the format\n",
    "         of outputs. (e.g. store just gradients)\n",
    "        :return: batch_output\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids, outputs, grads = self.batch_output\n",
    "\n",
    "        probs = softmax(outputs, dim=-1)\n",
    "        probs, labels = torch.max(probs, dim=-1)\n",
    "\n",
    "        tokens = [\n",
    "            self.tokenizer.convert_ids_to_tokens(input_ids_)\n",
    "            for input_ids_ in input_ids\n",
    "        ]\n",
    "\n",
    "        embedding_grads = grads.sum(dim=2)\n",
    "        # norm for each sequence\n",
    "        norms = torch.norm(embedding_grads, dim=1, p=1)\n",
    "        # normalizing\n",
    "        for i, norm in enumerate(norms):\n",
    "            embedding_grads[i] = torch.abs(embedding_grads[i]) / norm\n",
    "\n",
    "        batch_output = []\n",
    "\n",
    "        iterator = zip(tokens, probs, embedding_grads, labels)\n",
    "\n",
    "        for example_tokens, example_prob, example_grad, example_label in iterator:\n",
    "            example_dict = dict()\n",
    "            # as we do it by batches we has a padding so we need to remove it\n",
    "            example_tokens = [t for t in example_tokens if t != self.tokenizer.pad_token]\n",
    "            example_dict['tokens'] = example_tokens\n",
    "            example_dict['grad'] = example_grad.cpu().tolist()[:len(example_tokens)]\n",
    "            example_dict['label'] = example_label.item()\n",
    "            example_dict['prob'] = example_prob.item()\n",
    "            batch_output.append(example_dict)\n",
    "        return batch_output\n",
    "    \n",
    "class IntegratedGradient(SaliencyInterpreter):\n",
    "    \"\"\"\n",
    "    Interprets the prediction using Integrated Gradients (https://arxiv.org/abs/1703.01365)\n",
    "    Registered as a `SaliencyInterpreter` with name \"integrated-gradient\".\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 criterion,\n",
    "                 tokenizer,\n",
    "                 num_steps=20,\n",
    "                 show_progress=True,\n",
    "                 **kwargs):\n",
    "        super().__init__(model, criterion, tokenizer, show_progress, **kwargs)\n",
    "        # Hyperparameters\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def saliency_interpret(self, test_dataloader):\n",
    "\n",
    "        instances_with_grads = []\n",
    "        iterator = tqdm(test_dataloader) if self.show_progress else test_dataloader\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            # we will store there batch outputs such as gradients, probability, tokens\n",
    "            # so as each of them are used in different places, for convenience we will create\n",
    "            # it as attribute:\n",
    "            self.batch_output = []\n",
    "            self._integrate_gradients(batch)\n",
    "            batch_output = self.update_output()\n",
    "            instances_with_grads.extend(batch_output)\n",
    "\n",
    "        return instances_with_grads\n",
    "\n",
    "    def _register_forward_hook(self, alpha, embeddings_list):\n",
    "        \"\"\"\n",
    "        Register a forward hook on the embedding layer which scales the embeddings by alpha. Used\n",
    "        for one term in the Integrated Gradients sum.\n",
    "        We store the embedding output into the embeddings_list when alpha is zero.  This is used\n",
    "        later to element-wise multiply the input by the averaged gradients.\n",
    "        \"\"\"\n",
    "\n",
    "        def forward_hook(module, inputs, output):\n",
    "            # Save the input for later use. Only do so on first call.\n",
    "            if alpha == 0:\n",
    "                embeddings_list.append(output.squeeze(0).clone().detach())\n",
    "\n",
    "            # Scale the embedding by alpha\n",
    "            output.mul_(alpha)\n",
    "\n",
    "        embedding_layer = self.get_embeddings_layer()\n",
    "        handle = embedding_layer.register_forward_hook(forward_hook)\n",
    "        return handle\n",
    "\n",
    "    def _integrate_gradients(self, batch):\n",
    "\n",
    "        ig_grads = None\n",
    "\n",
    "        # List of Embedding inputs\n",
    "        embeddings_list = []\n",
    "\n",
    "        # Exclude the endpoint because we do a left point integral approximation\n",
    "        for alpha in np.linspace(0, 1.0, num=self.num_steps, endpoint=False):\n",
    "            # Hook for modifying embedding value\n",
    "            handle = self._register_forward_hook(alpha, embeddings_list)\n",
    "\n",
    "            grads = self._get_gradients(batch)\n",
    "            handle.remove()\n",
    "\n",
    "            # Running sum of gradients\n",
    "            if ig_grads is None:\n",
    "                ig_grads = grads\n",
    "            else:\n",
    "                ig_grads = ig_grads + grads\n",
    "\n",
    "        # Average of each gradient term\n",
    "        ig_grads /= self.num_steps\n",
    "\n",
    "        # Gradients come back in the reverse order that they were sent into the network\n",
    "        embeddings_list.reverse()\n",
    "\n",
    "        # Element-wise multiply average gradient by the input\n",
    "        ig_grads *= embeddings_list[0]\n",
    "\n",
    "        self.batch_output.append(ig_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9eae19d8-adbc-4158-9fbe-cf37c7caeea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['this is a no test', 'hello no']\n",
    "data = VisualizeTextAttention(sents, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5bf98f40-4a8c-43c9-9a6b-5ad820dd0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f4a5fabd-ccee-42e1-b59a-7bdd59ac3a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gp/hp50s5114x52591qbdhn43xm0000gn/T/ipykernel_22278/2295903331.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintegrated_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaliency_interpret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/gp/hp50s5114x52591qbdhn43xm0000gn/T/ipykernel_22278/459770332.py\u001b[0m in \u001b[0;36msaliency_interpret\u001b[0;34m(self, test_dataloader)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_progress\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;31m# we will store there batch outputs such as gradients, probability, tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/DL/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/DL/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/DL/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/DL/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/DL/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/DL/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             raise KeyError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0;34m\"Indexing with integers (to access backend Encoding for a given batch index) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;34m\"is not available when using Python based tokenizers\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'"
     ]
    }
   ],
   "source": [
    "integrated_grad = IntegratedGradient(\n",
    "    model, \n",
    "    criterion, \n",
    "    tokenizer, \n",
    "    show_progress=True,\n",
    "    encoder=\"bert\"\n",
    ")\n",
    "instances = integrated_grad.saliency_interpret(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6efc1977-d5a3-436f-bf3c-a5aab6ba8a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0350e-03, -5.2939e+00]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9a57d8af-6d44-46f7-999d-d12d39864df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_interpret(test_dataloader):\n",
    "\n",
    "    instances_with_grads = []\n",
    "    iterator = tqdm(test_dataloader)\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        # we will store there batch outputs such as gradients, probability, tokens\n",
    "        # so as each of them are used in different places, for convenience we will create\n",
    "        # it as attribute:\n",
    "        batch_output = []\n",
    "        self._integrate_gradients(batch)\n",
    "        batch_output = self.update_output()\n",
    "        instances_with_grads.extend(batch_output)\n",
    "\n",
    "    return instances_with_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c079d385-2010-4792-8da5-d91eeca08eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gp/hp50s5114x52591qbdhn43xm0000gn/T/ipykernel_22278/918652275.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaliency_interpret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/gp/hp50s5114x52591qbdhn43xm0000gn/T/ipykernel_22278/97212813.py\u001b[0m in \u001b[0;36msaliency_interpret\u001b[0;34m(test_dataloader)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# it as attribute:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_integrate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minstances_with_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "saliency_interpret(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92608b2-5de8-4b0a-92f3-feaefcfbb2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
