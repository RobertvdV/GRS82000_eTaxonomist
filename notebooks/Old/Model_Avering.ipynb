{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "15475b1d-4b68-4c08-addf-6db692768a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import transformers\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from statistics import mean\n",
    "from selenium import webdriver\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\", exclude=[\"tagger\", \"ner\", \"lemmatizer\", ])\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/models/')\n",
    "sys.path.insert(0, '../src/features/')\n",
    "import predict_model\n",
    "#from build_features import random_text_splitter as split_text\n",
    "\n",
    "# Load BERT\n",
    "model = predict_model.loadBERT(\"../models/\", 'saved_weights_inf_FIXED.pt')\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e87e3a1-c803-4614-a3ef-9078cc5d5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter(text, split_value=20):\n",
    "    \n",
    "    # Split text\n",
    "    words = text.split()\n",
    "    if len(words) < 10:\n",
    "        return [text]\n",
    "    # Create counter\n",
    "    remaining_word_amount = len(words)\n",
    "    # Init list\n",
    "    parts = []\n",
    "    # While words remaining\n",
    "    while remaining_word_amount > 0:\n",
    "        if len(words) < 20:\n",
    "            # Add last part if less then 10\n",
    "            parts.append(' '.join(words))\n",
    "            # exit\n",
    "            break\n",
    " \n",
    "        # Append to list \n",
    "        parts.append(' '.join(words[:split_value]))\n",
    "        # Delete previous selection\n",
    "        words = words[split_value:]\n",
    "        # Update counter\n",
    "        remaining_word_amount -= split_value\n",
    "        \n",
    "        #print(parts)\n",
    "        \n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813956f1-d9cf-47a5-82a4-74d2eb0f829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpanPredictor(span, pred_values=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "         \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        # Print the prediction values\n",
    "        if pred_values:\n",
    "            return span_class, exps[0]\n",
    "        else:\n",
    "            return span_class    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0af7514e-6960-4507-b1ee-088901265f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_text(predictions, save=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Highlight text based on the predicted value by the \n",
    "    description deep learning BERT model.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "    \n",
    "    # Call figure and add axis\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    ax = fig.add_subplot()\n",
    "    # Set axis\n",
    "    ax.axis([0, 10, 0, 10])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    y_axis_start = 10\n",
    "    x_axis_start = 0.0\n",
    "    # Loop over the text\n",
    "    for sent, pred in predictions:\n",
    "        \n",
    "        #pred = pred[1][1].numpy().item()\n",
    "\n",
    "        # Set axis with correct color and align right\n",
    "        if len(sent.split()) <= 5:\n",
    "            ax.text(.5, y_axis_start,  sent, ha='left', wrap=True, fontsize=13,\n",
    "                    bbox={'facecolor': 'grey', 'alpha': 0.6, 'pad': 2})\n",
    "        elif pred > 0.5:\n",
    "            ax.text(.25, y_axis_start, sent, ha='left', weight=\"bold\", wrap=True,\n",
    "                    bbox={'facecolor': cmap(pred), 'alpha': 0.6, 'pad': 4})\n",
    "        else :\n",
    "            ax.text(.25, y_axis_start, sent, ha='left', wrap=True,\n",
    "                    bbox={'facecolor': cmap(pred), 'alpha': 0.6, 'pad': 4})\n",
    "        \n",
    "        #if x_axis_start \n",
    "        # Update axis start\n",
    "        y_axis_start -= .22\n",
    "        x_axis_start += 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fcf119-cfa3-4aef-8131-7a192e15f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://db.worldagroforestry.org//species/properties/Enterolobium_cyclocarpum'\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4cae8d9-efbc-4bd3-8c97-e74001249901",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.get_text(\" \", strip=True)\n",
    "text_list = [text for text in soup.stripped_strings if len(text.split()) > 5]\n",
    "\n",
    "' '.join(text_list)\n",
    "\n",
    "text_list_5 = text_splitter(text, split_value=5)\n",
    "text_list_10 = text_splitter(text, split_value=10)\n",
    "text_list_20 = text_splitter(text, split_value=20)\n",
    "\n",
    "\n",
    "text_list_5_preds = [tuple([text, SpanPredictor(text, pred_values=True)]) for text in text_list_5]\n",
    "text_list_10_preds = [tuple([text, SpanPredictor(text, pred_values=True)]) for text in text_list_10]\n",
    "text_list_20_preds = [tuple([text, SpanPredictor(text, pred_values=True)]) for text in text_list_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ca8780e-cd93-4dd1-aecd-d3d3f1e1c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_preds = []\n",
    "\n",
    "list10 = -1\n",
    "list20 = -1\n",
    "\n",
    "for count, (text, (label, pred)) in enumerate(text_list_5_preds):\n",
    "    #print(count)\n",
    "    if count % 2 == 0:\n",
    "        list10 += 1\n",
    "        #print(count, list10)\n",
    "    if count % 4 == 0:\n",
    "        list20 += 1\n",
    "        \n",
    "    pred5 = pred[1].numpy().item()\n",
    "    pred10 = text_list_10_preds[list10][1][1][1].numpy().item()\n",
    "    pred20 = text_list_20_preds[list20][1][1][1].numpy().item()\n",
    "    \n",
    "    newpred = mean([pred5, pred10, pred20])\n",
    "    \n",
    "    text_list_preds.append(tuple([text, newpred]))\n",
    "        \n",
    "    #print(count, list10, list20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a675b937-0c10-4aa2-a1ae-0e22bf2813ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#highlight_text(text_list_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd250b-b2e1-4e69-bdf2-dbf471c41736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7646b39e-e695-46bc-82d0-6575d0655995",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://db.worldagroforestry.org//species/properties/Enterolobium_cyclocarpum'\n",
    "page = requests.get(URL, timeout=5)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "927a2226-62ba-4ba7-900a-5204fca0ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = soup.get_text(\" \", strip=True)\n",
    "text_list = [text for text in soup.stripped_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6ec7c0c6-a9be-438d-99aa-61efe633e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 43/43 [00:03<00:00, 13.49it/s]\n"
     ]
    }
   ],
   "source": [
    "text_list_splitted_20 = [text_splitter(text) for text in text_list]\n",
    "\n",
    "span_list_20_predictions = [tuple([span, SpanPredictor(span, pred_values=True)[1][1].numpy().item()])\n",
    "                            for span_list in tqdm(text_list_splitted_20) \n",
    "                            for span in span_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e1292dc-7da8-4fab-ac7c-2d19b7b8202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3468b401-2e94-4347-b1ad-9c93c718bbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The small, white flowers are borne in clusters or heads at the base of the leaves. 1\n",
      "Flowering takes place in March and April during the regrowth of new leaves after the leafless dry season. 0\n",
      "There is no indication in the literature as to what age flowers 1st appear. 0\n",
      "Seed dissemination is mainly by cattle, horses and wild ungulates, attracted by the syrupy pulp of the fruits. 0\n"
     ]
    }
   ],
   "source": [
    "for sent in docs[19].sents:\n",
    "    print(sent, SpanPredictor(str(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3453385-08be-49bb-a379-8adce12e63e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, tensor([0.1581, 0.8419]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SpanPredictor(text_list[0], pred_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693323b-812c-4a18-b734-49cac24bcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def highlight_text(predictions, save=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Highlight text based on the predicted value by the \n",
    "    description deep learning BERT model.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "    \n",
    "    # Call figure and add axis\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    ax = fig.add_subplot()\n",
    "    # Set axis\n",
    "    ax.axis([0, 10, 0, 10])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    y_axis_start = 10\n",
    "    x_axis_start = 0.0\n",
    "    # Loop over the text\n",
    "    \n",
    "    for sent, pred in predictions:\n",
    "                \n",
    "        # Set axis with correct color and align right\n",
    "        if pred > 0.5:\n",
    "            ax.text(x_axis_start, y_axis_start, sent, ha='left', weight=\"bold\", wrap=True,\n",
    "                    bbox={'facecolor': cmap(pred), 'alpha': 0.6, 'pad': 2})\n",
    "        else :\n",
    "            ax.text(x_axis_start, y_axis_start, sent, ha='left', wrap=True,\n",
    "                    bbox={'facecolor': cmap(pred), 'alpha': 0.6, 'pad': 2})\n",
    "        \n",
    "\n",
    "        #if x_axis_start \n",
    "        # Update axis start\n",
    "        if x_axis_start > 10:\n",
    "            x_axis_start = 0\n",
    "            y_axis_start -= .25\n",
    "        else:\n",
    "            x_axis_start += len(sent.lower()) / 14\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
