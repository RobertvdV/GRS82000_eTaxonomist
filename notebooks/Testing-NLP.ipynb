{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d19423-061e-4cc9-b37d-2ff2ce09710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch import nn\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fbfe3cd-dc36-4c65-8074-c47cdc70d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "pickle_in = open(\"data_withMeasurements.pkl\", \"rb\")\n",
    "dataWIKI = pickle.load(pickle_in)\n",
    "\n",
    "# Load data\n",
    "pickle_in = open(\"dataBOW_withMeasurements.pkl\", \"rb\")\n",
    "dataBOW = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b4bc10-ba1e-4cff-be77-442d952f576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data together\n",
    "data = dataWIKI | dataBOW\n",
    "#data = dataWIKI\n",
    "#data = dataBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0135fbc-34ec-4b11-8de3-8e34c8ad5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47997 values.\n",
      "9639 labels with 1 (true).\n",
      "38358 labels with 0 (false).\n"
     ]
    }
   ],
   "source": [
    "TotalValues = list(chain.from_iterable(data.values()))\n",
    "\n",
    "ones = Counter(ones[0] for ones in TotalValues if ones[0] == 1)\n",
    "zeros = Counter(ones[0] for ones in TotalValues if ones[0] == 0)\n",
    "\n",
    "print('{0} values.'. format(len(TotalValues)))\n",
    "print('{0} labels with 1 (true).'.format(ones[1]))\n",
    "print('{0} labels with 0 (false).'.format(zeros[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c398b-9358-48bc-87a5-40676264a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Species\n",
    "len(data)\n",
    "# Total values\n",
    "Total = sum(len(v) for v in list(data.values()))\n",
    "print(Total)\n",
    "# Total Truths\n",
    "TotalValues = [v for v in list(data.values())]\n",
    "Truths = [[ones[0] for ones in Values if ones[0] == 1] for Values in TotalValues]\n",
    "AmountofTruths = sum(len(x) for x in Truths)\n",
    "# Total False\n",
    "AmountofFalse = Total - AmountofTruths\n",
    "print(AmountofTruths)\n",
    "print(AmountofFalse)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c52af21-0669-46c4-ba4a-d5f9e67a291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values from the dict\n",
    "data = list(chain.from_iterable(data.values()))\n",
    "# Train test sequence\n",
    "train = int(len(data) * 0.8)\n",
    "test = len(data) - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc90c63-95b6-4a81-aa04-d6c94c369d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "trainset = data[0:train]\n",
    "testset = data[train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ada087-f784-4140-85e8-4ef7b8cbb817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38397"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "708d5b7b-bc04-4e2a-a60d-7d772aeae649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic English\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def yield_tokens(data_iter):\n",
    "    # Drop the label (label, text)\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Build a vocabulary        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(trainset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a6ce8fa-9f29-4127-a754-a9e35b14c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "# -1 could be removed if data is loaded differently\n",
    "label_pipeline = lambda x: int(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4b51979-1c1c-4e6f-9b6c-936eff41b454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   1,    8,  441,  114,    2,    1,  318,    9,   37, 2708,    6])\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "proc = torch.tensor(text_pipeline('the a bird birds, the bear are not equal.'), dtype=torch.int64)\n",
    "print(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa2995c1-cb7d-4c68-bcd0-b06e70622b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (CPU for macs)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \n",
    "    '''\n",
    "    Convert sentences or paragrahs to integers by using\n",
    "    the PyTorch Vocab(). The data is converted and \n",
    "    returned as a tensor. The offset of the words is \n",
    "    compared to the start of the sentence/paragraph.\n",
    "    '''\n",
    "\n",
    "    # Init lists\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    \n",
    "    # Loop over the data\n",
    "    for (_label, _text) in batch:\n",
    "        # Append the labels to list\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        # Process the text (singed 64), and convert to tensor\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        # Append the text to list \n",
    "        text_list.append(processed_text)\n",
    "        # Append the offset (tensor size)   \n",
    "        offsets.append(processed_text.size(0))\n",
    "    \n",
    "    # Convert the label list to a tensor\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    # Cummulative sum the offsets (dim=0 == rowwise)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    # Concatenate the text list\n",
    "    text_list = torch.cat(text_list)\n",
    "    \n",
    "    # Return the values\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "533c99c8-bb8c-41f0-91da-a64be4fb783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = trainset\n",
    "dataloader = DataLoader(train_iter, batch_size=16, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0583f2f-dcda-4c29-a4a5-562259e7a623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0]),\n",
       " tensor([   14,     1,  1305,  5478,    41,     2,    11,   634,    97,    43,\n",
       "            17,   541,    68,  1344,     3,  4489,    16,     8,  2197,     5,\n",
       "            46,  1488,     3,  2894,     1,   132,  8285,     5,     1,   543,\n",
       "             9,   100,  8777,    24,   383,   973,     3,   316,   162,     2,\n",
       "            31,    75,     9,    70,   841,    11,    22,   150,     4,   247,\n",
       "             6,     1,   631,     5,     1,   173,   111,   258,  1177,    10,\n",
       "          5766,   900,     3,  1459,     7,  1217,  5924,     1,  1131,   847,\n",
       "            17,  1573,   868,    44,   171,    51,     7,     1,    78,     6,\n",
       "             1,    53,    29,   949,   290,    10,  6486,    36,  1060,     9,\n",
       "          2740,     5,   195,    16,  3174,   902,     3,  7884,    30,   350,\n",
       "             5,  2121,    10,    26,   435,  2319,     4,    14, 13808,    52,\n",
       "             6,   690,    11,     9,     1,    53,  1527,   389,     5,     1,\n",
       "          9750,  1923,     6,    80,    11,  5583,  4430,     4,     1,    29,\n",
       "           389,     5,     1,  3242,   335,     2,    11,     9,  2401,    24,\n",
       "            12,    55,     2,  1591,     3,  3069,  1906,     6,    20, 13878,\n",
       "             9,   261,     4,   735,     1,   154,   796,    15,     1,    63,\n",
       "          1041,    13,    19,  1347,    17,     6,    12,  1906,    22,    26,\n",
       "           606,  1760,     3,  2888,  1584,     2,    33,   346,    32,   773,\n",
       "           914,     6,   140,  1560,    74,    22,   210,    41,     4,   171,\n",
       "           109,     7,   209,    80,    72,   205,   222,     4,   324,   109,\n",
       "             6,    26,     2,     1,    74,     3,    72,     5,    46,    21,\n",
       "            18,   128,   917,  1064,    69,     2,   105,     4,   141,     3,\n",
       "            86,   175,  1253,    18,  1336,   484,     5,    12,   129,   404,\n",
       "           103,    99,    16,     1,   498,     5,    78,   184,    50,     7,\n",
       "             8,   143,     5,  1987,    60,   513,  1030,     6,   155,     9,\n",
       "           406,    14,     8,   300,   180,    24,    70,    29,    21,   112,\n",
       "            84,  1253,     3,   211,    33,     9,   447,   685,     5,  3868,\n",
       "          4340,     8,   232,     4,   589,     3,  1354,    19,   286,     6,\n",
       "             1,   111,   238,   273,    10,     1,    45,     5,     1,   236,\n",
       "            56,   258,   285,    17,  1275,     2,    16,    82,   111,   238,\n",
       "           211,  1077,   372,   241,   512,     7,   954,     3,     1,    64,\n",
       "           111,   238,   211,    58,   214,     6,   182,   512,   294,   543,\n",
       "            18,   222,   919,    13,     9,  1092,    17,   130,   553,  2832,\n",
       "             3,   324,   919,   122,     1,   603,   231,     2,   117,   130,\n",
       "           878,    10,  2809,    16,  3128,     6,     1,  2202,     5,    20,\n",
       "           543,     9,  3664,     3,     9,    85,    16,     8,   977,  4659,\n",
       "             6,     1,    74,    18,  3706,  3619,    13,     9,   100,    85,\n",
       "            25,   195,     2,    31,     1,    72,     9,    70,  8521,     2,\n",
       "          5322,     8,   125,  1318,   449,     6,    72,    26,   160,     4,\n",
       "            27,   127,    49,    12,    82,  1026,     6,     1,   352,   318,\n",
       "            10,     8,   305,    76,    13,    22,    37,    53,   838,    35,\n",
       "          1477,     5,    41,     4,  5513,    31,    28,   291,   380,     4,\n",
       "           478,    35,  5523,   346,    19,     8,  1886,  1446,   290,   267,\n",
       "            28,   249,   174,     2,     1,  1170,    10,   562,   406,     7,\n",
       "             1,    78,    16,   243,     8,  3372,     5,  3162,   393,    65,\n",
       "          1206,   116,     1,   869,  3258,  1325,   619,     9,    50,     7,\n",
       "            84,     1,  6077,     3,  3563,  1027,     2,    98,    11,   675,\n",
       "          1752,   930,   517,    16,  1273,  3519,   212,     4,    27,     1,\n",
       "         11912,    86,    15,    32,   636,    85,   252,     9,   139,     4,\n",
       "           284,    40,    11,     9,    62,   329,     3,   182,    51,   136,\n",
       "             3,   110,   104,    83,     1,  1108,   371,    94,     5,  1537,\n",
       "             3,  2191]),\n",
       " tensor([  0,  24,  51,  67,  80,  87,  98, 182, 214, 248, 280, 318, 397, 432,\n",
       "         455, 484]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a49b6e0-7eb0-46b7-b743-c8528401b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1144cfc2-43c0-44d0-9f5c-493cc449e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = trainset\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ede95da0-ce09-4593-b1dd-0a0c0da61d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12ca46ca-7f2a-43e6-bbb5-e1781dd0f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 4560 batches | accuracy    0.840\n",
      "| epoch   1 |  1000/ 4560 batches | accuracy    0.875\n",
      "| epoch   1 |  1500/ 4560 batches | accuracy    0.904\n",
      "| epoch   1 |  2000/ 4560 batches | accuracy    0.893\n",
      "| epoch   1 |  2500/ 4560 batches | accuracy    0.905\n",
      "| epoch   1 |  3000/ 4560 batches | accuracy    0.903\n",
      "| epoch   1 |  3500/ 4560 batches | accuracy    0.906\n",
      "| epoch   1 |  4000/ 4560 batches | accuracy    0.921\n",
      "| epoch   1 |  4500/ 4560 batches | accuracy    0.912\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  4.80s | valid accuracy    0.918 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 4560 batches | accuracy    0.926\n",
      "| epoch   2 |  1000/ 4560 batches | accuracy    0.919\n",
      "| epoch   2 |  1500/ 4560 batches | accuracy    0.926\n",
      "| epoch   2 |  2000/ 4560 batches | accuracy    0.932\n",
      "| epoch   2 |  2500/ 4560 batches | accuracy    0.929\n",
      "| epoch   2 |  3000/ 4560 batches | accuracy    0.926\n",
      "| epoch   2 |  3500/ 4560 batches | accuracy    0.929\n",
      "| epoch   2 |  4000/ 4560 batches | accuracy    0.936\n",
      "| epoch   2 |  4500/ 4560 batches | accuracy    0.936\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  4.74s | valid accuracy    0.924 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 4560 batches | accuracy    0.943\n",
      "| epoch   3 |  1000/ 4560 batches | accuracy    0.940\n",
      "| epoch   3 |  1500/ 4560 batches | accuracy    0.938\n",
      "| epoch   3 |  2000/ 4560 batches | accuracy    0.943\n",
      "| epoch   3 |  2500/ 4560 batches | accuracy    0.939\n",
      "| epoch   3 |  3000/ 4560 batches | accuracy    0.939\n",
      "| epoch   3 |  3500/ 4560 batches | accuracy    0.946\n",
      "| epoch   3 |  4000/ 4560 batches | accuracy    0.940\n",
      "| epoch   3 |  4500/ 4560 batches | accuracy    0.941\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  4.75s | valid accuracy    0.934 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 4560 batches | accuracy    0.948\n",
      "| epoch   4 |  1000/ 4560 batches | accuracy    0.954\n",
      "| epoch   4 |  1500/ 4560 batches | accuracy    0.956\n",
      "| epoch   4 |  2000/ 4560 batches | accuracy    0.951\n",
      "| epoch   4 |  2500/ 4560 batches | accuracy    0.944\n",
      "| epoch   4 |  3000/ 4560 batches | accuracy    0.950\n",
      "| epoch   4 |  3500/ 4560 batches | accuracy    0.953\n",
      "| epoch   4 |  4000/ 4560 batches | accuracy    0.945\n",
      "| epoch   4 |  4500/ 4560 batches | accuracy    0.954\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  4.72s | valid accuracy    0.935 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 4560 batches | accuracy    0.959\n",
      "| epoch   5 |  1000/ 4560 batches | accuracy    0.952\n",
      "| epoch   5 |  1500/ 4560 batches | accuracy    0.956\n",
      "| epoch   5 |  2000/ 4560 batches | accuracy    0.952\n",
      "| epoch   5 |  2500/ 4560 batches | accuracy    0.956\n",
      "| epoch   5 |  3000/ 4560 batches | accuracy    0.951\n",
      "| epoch   5 |  3500/ 4560 batches | accuracy    0.955\n",
      "| epoch   5 |  4000/ 4560 batches | accuracy    0.956\n",
      "| epoch   5 |  4500/ 4560 batches | accuracy    0.959\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  4.83s | valid accuracy    0.932 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 4560 batches | accuracy    0.969\n",
      "| epoch   6 |  1000/ 4560 batches | accuracy    0.966\n",
      "| epoch   6 |  1500/ 4560 batches | accuracy    0.966\n",
      "| epoch   6 |  2000/ 4560 batches | accuracy    0.967\n",
      "| epoch   6 |  2500/ 4560 batches | accuracy    0.966\n",
      "| epoch   6 |  3000/ 4560 batches | accuracy    0.971\n",
      "| epoch   6 |  3500/ 4560 batches | accuracy    0.967\n",
      "| epoch   6 |  4000/ 4560 batches | accuracy    0.969\n",
      "| epoch   6 |  4500/ 4560 batches | accuracy    0.964\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  4.90s | valid accuracy    0.942 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 4560 batches | accuracy    0.972\n",
      "| epoch   7 |  1000/ 4560 batches | accuracy    0.967\n",
      "| epoch   7 |  1500/ 4560 batches | accuracy    0.967\n",
      "| epoch   7 |  2000/ 4560 batches | accuracy    0.970\n",
      "| epoch   7 |  2500/ 4560 batches | accuracy    0.970\n",
      "| epoch   7 |  3000/ 4560 batches | accuracy    0.971\n",
      "| epoch   7 |  3500/ 4560 batches | accuracy    0.966\n",
      "| epoch   7 |  4000/ 4560 batches | accuracy    0.968\n",
      "| epoch   7 |  4500/ 4560 batches | accuracy    0.970\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  4.81s | valid accuracy    0.941 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 4560 batches | accuracy    0.973\n",
      "| epoch   8 |  1000/ 4560 batches | accuracy    0.969\n",
      "| epoch   8 |  1500/ 4560 batches | accuracy    0.969\n",
      "| epoch   8 |  2000/ 4560 batches | accuracy    0.966\n",
      "| epoch   8 |  2500/ 4560 batches | accuracy    0.968\n",
      "| epoch   8 |  3000/ 4560 batches | accuracy    0.979\n",
      "| epoch   8 |  3500/ 4560 batches | accuracy    0.970\n",
      "| epoch   8 |  4000/ 4560 batches | accuracy    0.966\n",
      "| epoch   8 |  4500/ 4560 batches | accuracy    0.972\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  4.83s | valid accuracy    0.943 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 4560 batches | accuracy    0.970\n",
      "| epoch   9 |  1000/ 4560 batches | accuracy    0.972\n",
      "| epoch   9 |  1500/ 4560 batches | accuracy    0.972\n",
      "| epoch   9 |  2000/ 4560 batches | accuracy    0.973\n",
      "| epoch   9 |  2500/ 4560 batches | accuracy    0.973\n",
      "| epoch   9 |  3000/ 4560 batches | accuracy    0.974\n",
      "| epoch   9 |  3500/ 4560 batches | accuracy    0.968\n",
      "| epoch   9 |  4000/ 4560 batches | accuracy    0.965\n",
      "| epoch   9 |  4500/ 4560 batches | accuracy    0.968\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  4.71s | valid accuracy    0.943 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 4560 batches | accuracy    0.973\n",
      "| epoch  10 |  1000/ 4560 batches | accuracy    0.973\n",
      "| epoch  10 |  1500/ 4560 batches | accuracy    0.970\n",
      "| epoch  10 |  2000/ 4560 batches | accuracy    0.962\n",
      "| epoch  10 |  2500/ 4560 batches | accuracy    0.972\n",
      "| epoch  10 |  3000/ 4560 batches | accuracy    0.974\n",
      "| epoch  10 |  3500/ 4560 batches | accuracy    0.971\n",
      "| epoch  10 |  4000/ 4560 batches | accuracy    0.972\n",
      "| epoch  10 |  4500/ 4560 batches | accuracy    0.969\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  4.81s | valid accuracy    0.943 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = trainset, testset\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c85ffde5-d8d3-477a-9e7b-621066b9f5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.855\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ebfcb-a5a5-4bce-93bd-dec92c8d7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {1: \"a description or similar.\",\n",
    "         0: \"something else.\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() \n",
    "\n",
    "# Measurements\n",
    "ex_text_str = 'They are 90 to 150 cm tall at the shoulder and can tower at an intimidating height of 8 feet when standing upright on their hind legs.'\n",
    "model = model.to(\"cpu\")\n",
    "print(\"This is %s\" %label[predict(ex_text_str, text_pipeline)])\n",
    "\n",
    "# Random\n",
    "ex_text_str = 'Hi I am GIS student, this is a random sentence!'\n",
    "model = model.to(\"cpu\")\n",
    "print(\"This is %s\" %label[predict(ex_text_str, text_pipeline)])\n",
    "\n",
    "# Bird stuff\n",
    "ex_text_str = 'The bill is long and orange.'\n",
    "model = model.to(\"cpu\")\n",
    "print(\"This is %s\" %label[predict(ex_text_str, text_pipeline)])\n",
    "\n",
    "# Something about bears\n",
    "ex_text_str = '''Brown bears are often not fully brown. \n",
    "                They have long, thick fur, with a moderately long mane at the back of the neck which varies somewhat across the types. \n",
    "                In India, brown bears can be reddish with silver-tipped hairs, while in China brown bears are bicolored, \n",
    "                with a yellowish-brown or whitish collar across the neck, chest and shoulders.'''\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "print(\"This is %s\" %label[predict(ex_text_str, text_pipeline)])\n",
    "\n",
    "# Somehting about Robins\n",
    "ex_text_str = '''The upperparts are brownish, or olive-tinged in British birds, and the belly whitish, while the legs and feet are brown. \n",
    "                The bill and eyes are black.'''\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "print(\"This is %s\" %label[predict(ex_text_str, text_pipeline)])\n",
    "\n",
    "# Random Difficult sentence\n",
    "ex_text_str = '''While I am very tan from the sun, I can be pale within a few days.\n",
    "                I have blonde hair'''\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "print(\"This is %s\" %label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c71d33-b09c-43f9-8a3b-3b6fc75a3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init dicts\n",
    "ExtraTesting = []\n",
    "\n",
    "# Removes references in text\n",
    "ReferenceRemover = '\\[\\d*\\]'\n",
    "\n",
    "URL = 'https://en.wikipedia.org/wiki/Eurasian_wigeon'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "for Tags in soup.find_all('h2'):\n",
    "\n",
    "    # Skip useless/empty stuff\n",
    "    if Tags.span == None:\n",
    "        continue\n",
    "\n",
    "    # Set chapter variable    \n",
    "    Chapter = Tags.span.attrs['id']\n",
    "\n",
    "    # Check if the chapter is description (or similar)\n",
    "    if Chapter == 'Characteristics'or \\\n",
    "       Chapter == 'Description' or \\\n",
    "       Chapter == 'Appearance':\n",
    "\n",
    "        # Get the next sibling (text)\n",
    "        for Text in Tags.find_next_siblings('p'):\n",
    "\n",
    "            # Add description data to dict\n",
    "            if Chapter in Text.find_previous_siblings('h2')[0].text.strip():\n",
    "                # Remove source\n",
    "                Paragraph = re.sub(ReferenceRemover, '', Text.text)\n",
    "                # Split into Sentences\n",
    "                SentenceList = Paragraph.split('. ')\n",
    "                # Add to the dict\n",
    "                ExtraTesting += [(1, Sentence) for Sentence in SentenceList]\n",
    "\n",
    "            # Add non description data to dict\n",
    "            elif Chapter not in Text.find_previous_siblings('h2')[0].text.strip():\n",
    "                # Remove source\n",
    "                Paragraph = re.sub(ReferenceRemover, '', Text.text)\n",
    "                # Split into Sentences\n",
    "                SentenceList = Paragraph.split('. ')\n",
    "                # Add to the dict\n",
    "                ExtraTesting += [(0, Sentence) for Sentence in SentenceList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597ada4-0238-4f00-9a77-35fe42c0881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {1: \"a description or similar.\",\n",
    "         0: \"something else.\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() \n",
    "\n",
    "for tests in ExtraTesting:\n",
    "    ex_text_str = tests[1]\n",
    "    \n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    print(\"This is %s\" %label[predict(ex_text_str, text_pipeline)])\n",
    "    print(\"Real value was {0}\".format(tests[0]))\n",
    "    print(tests[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25eb47-da5e-4001-929b-962a7a885bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
