{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d19423-061e-4cc9-b37d-2ff2ce09710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch import nn\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fbfe3cd-dc36-4c65-8074-c47cdc70d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "pickle_in = open(\"dataBOW.pkl\", \"rb\")\n",
    "data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0135fbc-34ec-4b11-8de3-8e34c8ad5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 values\n",
      "Counter({1: 32})\n",
      "Counter({0: 92})\n"
     ]
    }
   ],
   "source": [
    "TotalValues = list(chain.from_iterable(data.values()))\n",
    "print('{0} values'. format(len(TotalValues)))\n",
    "\n",
    "print(Counter(ones[0] for ones in TotalValues if ones[0] == 1))\n",
    "print(Counter(ones[0] for ones in TotalValues if ones[0] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c398b-9358-48bc-87a5-40676264a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Species\n",
    "len(data)\n",
    "# Total values\n",
    "Total = sum(len(v) for v in list(data.values()))\n",
    "print(Total)\n",
    "# Total Truths\n",
    "TotalValues = [v for v in list(data.values())]\n",
    "Truths = [[ones[0] for ones in Values if ones[0] == 1] for Values in TotalValues]\n",
    "AmountofTruths = sum(len(x) for x in Truths)\n",
    "# Total False\n",
    "AmountofFalse = Total - AmountofTruths\n",
    "print(AmountofTruths)\n",
    "print(AmountofFalse)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c52af21-0669-46c4-ba4a-d5f9e67a291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values from the dict\n",
    "data = list(chain.from_iterable(data.values()))\n",
    "# Train test sequence\n",
    "train = int(len(data) * 0.8)\n",
    "test = len(data) - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc90c63-95b6-4a81-aa04-d6c94c369d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "trainset = data[0:train]\n",
    "testset = data[train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ada087-f784-4140-85e8-4ef7b8cbb817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "708d5b7b-bc04-4e2a-a60d-7d772aeae649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic English\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def yield_tokens(data_iter):\n",
    "    # Drop the label (label, text)\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Build a vocabulary        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(trainset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a6ce8fa-9f29-4127-a754-a9e35b14c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "# -1 could be removed if data is loaded differently\n",
    "label_pipeline = lambda x: int(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b51979-1c1c-4e6f-9b6c-936eff41b454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6,  7,  0,  3,  2,  6,  0, 20, 51,  0,  1])\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "proc = torch.tensor(text_pipeline('the a whale and, the bear are not equal.'), dtype=torch.int64)\n",
    "print(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa2995c1-cb7d-4c68-bcd0-b06e70622b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (CPU for macs)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \n",
    "    '''\n",
    "    Convert sentences or paragrahs to integers by using\n",
    "    the PyTorch Vocab(). The data is converted and \n",
    "    returned as a tensor. The offset of the words is \n",
    "    compared to the start of the sentence/paragraph.\n",
    "    '''\n",
    "    \n",
    "    # Init lists\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    \n",
    "    # Loop over the data\n",
    "    for (_label, _text) in batch:\n",
    "        # Append the labels to list\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        # Process the text (singed 64), and convert to tensor\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        # Append the text to list \n",
    "        text_list.append(processed_text)\n",
    "        # Append the offset (tensor size)   \n",
    "        offsets.append(processed_text.size(0))\n",
    "    \n",
    "    # Convert the label list to a tensor\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    # Cummulative sum the offsets (dim=0 == rowwise)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    # Concatenate the text list\n",
    "    text_list = torch.cat(text_list)\n",
    "    \n",
    "    # Return the values\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "533c99c8-bb8c-41f0-91da-a64be4fb783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = trainset\n",
    "dataloader = DataLoader(train_iter, batch_size=16, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0583f2f-dcda-4c29-a4a5-562259e7a623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
       " tensor([530,   1,   7,  ..., 774,   1, 915]),\n",
       " tensor([   0,    7,  318,  333,  434,  476,  625,  739,  772,  798,  909, 1102,\n",
       "         1109, 1157, 1191, 1215]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a49b6e0-7eb0-46b7-b743-c8528401b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1144cfc2-43c0-44d0-9f5c-493cc449e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = trainset\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede95da0-ce09-4593-b1dd-0a0c0da61d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12ca46ca-7f2a-43e6-bbb5-e1781dd0f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.06s | valid accuracy    0.800 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.06s | valid accuracy    0.800 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.06s | valid accuracy    0.800 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.04s | valid accuracy    0.800 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.05s | valid accuracy    1.000 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.05s | valid accuracy    1.000 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.05s | valid accuracy    1.000 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.04s | valid accuracy    1.000 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 8 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 2 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = trainset, testset\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c85ffde5-d8d3-477a-9e7b-621066b9f5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.560\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b64ebfcb-a5a5-4bce-93bd-dec92c8d7ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is something else.\n"
     ]
    }
   ],
   "source": [
    "label = {1: \"a description or similar.\",\n",
    "         0: \"something else.\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() \n",
    "\n",
    "ex_text_str = \"\"\"\n",
    "One of the largest of living carnivores, grizzly bears are 1 to 2.8 meters in length from head to rump and\\\n",
    "their tails are 65 to 210 mm long. They are 90 to 150 cm tall at the shoulder and can tower at an intimidating\\\n",
    "height of 8 feet when standing upright on their hind legs. They range in weight from 80 to more than 600 kg. \\\n",
    "On average, adult males are 8 to 10% larger than females. Ursus arctos is largest along the the coast of southern\\\n",
    "Alaska and on nearby islands where males average 389 kg and females average 207 kg, though some males have been weighed \\\n",
    "at as much as 780 kg. Distance between the canines is from 6 to 8 cm. Size rapidly declines to the north and east, with\\\n",
    "individuals in southwestern Yukon weighing only 140 kg on average. Fur is usually dark brown, but varies from cream to almost black.\\\n",
    "Individuals in the Rocky Mountains have long hairs along the shoulders and back which are frosted with white, giving a grizzled appearance,\\\n",
    "hence the common name grizzly bear in that region. Brown bears are extremely strong and have good endurance; they\\\n",
    "can kill a cow with one blow, outrun a horse, outswim an Olympian, and drag a dead elk uphill. (Wilson and Ruff, 1999)\"\"\"    \n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is %s\" %label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c71d33-b09c-43f9-8a3b-3b6fc75a3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init dicts\n",
    "ExtraTesting = []\n",
    "\n",
    "# Removes references in text\n",
    "ReferenceRemover = '\\[\\d*\\]'\n",
    "\n",
    "URL = 'https://en.wikipedia.org/wiki/Eurasian_wigeon'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "for Tags in soup.find_all('h2'):\n",
    "\n",
    "    # Skip useless/empty stuff\n",
    "    if Tags.span == None:\n",
    "        continue\n",
    "\n",
    "    # Set chapter variable    \n",
    "    Chapter = Tags.span.attrs['id']\n",
    "\n",
    "    # Check if the chapter is description (or similar)\n",
    "    if Chapter == 'Characteristics'or \\\n",
    "       Chapter == 'Description' or \\\n",
    "       Chapter == 'Appearance':\n",
    "\n",
    "        # Get the next sibling (text)\n",
    "        for Text in Tags.find_next_siblings('p'):\n",
    "\n",
    "            # Add description data to dict\n",
    "            if Chapter in Text.find_previous_siblings('h2')[0].text.strip():\n",
    "                # Remove source\n",
    "                Paragraph = re.sub(ReferenceRemover, '', Text.text)\n",
    "                # Split into Sentences\n",
    "                SentenceList = Paragraph.split('. ')\n",
    "                # Add to the dict\n",
    "                ExtraTesting += [(1, Sentence) for Sentence in SentenceList]\n",
    "\n",
    "            # Add non description data to dict\n",
    "            elif Chapter not in Text.find_previous_siblings('h2')[0].text.strip():\n",
    "                # Remove source\n",
    "                Paragraph = re.sub(ReferenceRemover, '', Text.text)\n",
    "                # Split into Sentences\n",
    "                SentenceList = Paragraph.split('. ')\n",
    "                # Add to the dict\n",
    "                ExtraTesting += [(0, Sentence) for Sentence in SentenceList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597ada4-0238-4f00-9a77-35fe42c0881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {1: \"a description or similar.\",\n",
    "         0: \"something else.\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() \n",
    "\n",
    "for tests in ExtraTesting:\n",
    "    ex_text_str = tests[1]\n",
    "    \n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    print(\"This is %s\" %label[predict(ex_text_str, text_pipeline)])\n",
    "    print(\"Real value was {0}\".format(tests[0]))\n",
    "    print(tests[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25eb47-da5e-4001-929b-962a7a885bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
