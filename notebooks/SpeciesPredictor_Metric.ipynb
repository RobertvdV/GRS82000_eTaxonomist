{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37de2a1-3596-46fe-92a7-0921b3178245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pytorch-metric-learning testing module requires faiss. You can install the GPU version with the command 'conda install faiss-gpu -c pytorch'\n",
      "                        or the CPU version with 'conda install faiss-cpu -c pytorch'. Learn more at https://github.com/facebookresearch/faiss/blob/master/INSTALL.md\n"
     ]
    }
   ],
   "source": [
    "from pytorch_metric_learning import losses, miners, samplers, trainers, testers\n",
    "from pytorch_metric_learning.utils import common_functions\n",
    "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_metric_learning\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1b3139-0091-4410-a2c9-c153c57564df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# specify device\n",
    "from torch import cuda\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Bert mode\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b46e24-006a-408d-9d1c-e939673b3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeciesDescriptions(Dataset):\n",
    "    \n",
    "    \"\"\"Description dataset without species names.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.samples = []\n",
    "        self._init_dataset()\n",
    "        #self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "     \n",
    "    def _init_dataset(self):\n",
    "        \n",
    "        # Encoder \n",
    "        label_encoder = LabelEncoder()\n",
    "        # Init dict\n",
    "        datadict = {}\n",
    "        # Load the pickle list\n",
    "        data_files = glob.glob(self.root_dir + 'subset*.pkl')\n",
    "        for data_file in data_files:\n",
    "            # Open the dict and update\n",
    "            datadict.update(pickle.load(open(data_file, 'rb')))\n",
    "            \n",
    "        # Get keys and encode them\n",
    "        keys = np.array([key for key in datadict.keys()])\n",
    "        print(len(keys))\n",
    "        keys_encoded = label_encoder.fit_transform(keys)\n",
    "        # Extract the values with the encoded keys\n",
    "        self.samples += [(key_label, value[0]) for key_label, (key, value_list) in zip(keys_encoded, datadict.items()) for value in value_list]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ca6d32-9551-416e-b1fe-15262c243a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted @Local\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Colab\n",
    "    from google.colab import drive\n",
    "    root = '/content/gdrive/My Drive/'\n",
    "    drive.mount('/content/gdrive')\n",
    "    print('Mounted @Google')\n",
    "except:\n",
    "    # Local\n",
    "    root = \"../data/processed/\"\n",
    "    print('Mounted @Local')\n",
    "\n",
    "# Load data\n",
    "data = SpeciesDescriptions(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7721f304-dbb5-4217-8952-de126e3ad110",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(data)\n",
    "train_count = int(1.0 * total_count)\n",
    "valid_count = int(0.0 * total_count)\n",
    "test_count = total_count - train_count - valid_count\n",
    "train_dataset, valid_dataset, test_dataset = random_split(data, (train_count, valid_count, test_count), \n",
    "                                                       generator=torch.Generator().manual_seed(33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff70d405-db24-4f89-916a-2d5b6169b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# Random sample (skewed set)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "# DataLoader for train set\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Random sample\n",
    "val_sampler = SequentialSampler(valid_dataset)\n",
    "# DataLoader for validation set\n",
    "val_dataloader = DataLoader(valid_dataset, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ec707b3-30a0-4704-ad0b-39e0f4a15de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8fbcc6f-cd2a-40ab-be68-ad7c9de7bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        # Distil Bert model\n",
    "        self.bert = bert\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, **kwargs):\n",
    "\n",
    "        # Pass data trough bert and extract \n",
    "        cls_hs = self.bert(**kwargs)\n",
    "        # Extract hidden state\n",
    "        hidden_state = cls_hs.last_hidden_state\n",
    "        # Only first is needed for classification\n",
    "        x = hidden_state[:, 0]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16a0cc9b-b59a-4ecb-ae54-665c4a80692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found.\n"
     ]
    }
   ],
   "source": [
    "# Load the entire model\n",
    "model = BERT(bert)\n",
    "\n",
    "# Load trained model (colab)\n",
    "try:\n",
    "    try:\n",
    "        model_save_name = 'saved_weights_NLP_test.pt'\n",
    "        path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        print('Google Success')\n",
    "\n",
    "    except:\n",
    "        model_save_name = 'saved_weights_NLP_subset.pt'\n",
    "        path = \"../models/\" + model_save_name\n",
    "        model.load_state_dict(torch.load(path, \n",
    "                                         map_location=torch.device('cpu')))\n",
    "        print('Local Success')\n",
    "except:\n",
    "    print('No pretrained model found.')\n",
    "\n",
    "# Push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f2a6782-878f-49c4-afc2-cdbe529b6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimizer (Adam best for bert)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=3e-5)\n",
    "# Define loss function\n",
    "softmax = nn.Softmax(1)\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "\n",
    "TripletLoss = losses.TripletMarginLoss(margin=0.1)\n",
    "\n",
    "def tokenize_batch(batch_set):\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenize a pytorch dataset using the hugging face tokenizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the labels and text\n",
    "    y = batch_set[0]\n",
    "    text = batch_set[1]\n",
    "    \n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.batch_encode_plus(text,\n",
    "                max_length = 512,\n",
    "                padding=True,\n",
    "                truncation=True)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    seq = torch.tensor(tokens['input_ids'])\n",
    "    mask = torch.tensor(tokens['attention_mask'])\n",
    "    \n",
    "    return seq, mask, y\n",
    "\n",
    "def train():\n",
    "  \n",
    "    \"\"\"\n",
    "    Function to train classification Bert model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        \n",
    "        # Tokenize batch\n",
    "        train_seq, train_mask, train_y = tokenize_batch(batch)\n",
    "        # Push to device\n",
    "        sent_id, mask, labels = [t.to(device) for t in [train_seq, train_mask, train_y]]\n",
    "        # Clear gradients \n",
    "        model.zero_grad()        \n",
    "        # Get predictions\n",
    "        preds = model(sent_id, mask)\n",
    "        # Compute loss\n",
    "        loss =  TripletLoss(preds, labels) \n",
    "        #loss = cross_entropy(preds, labels)\n",
    "        # Update total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # Backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # Clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to test classification Bert model.\n",
    "    \"\"\"\n",
    "  \n",
    "    # Deactivate dropout layers\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate over batches\n",
    "    for batch in tqdm(val_dataloader):   \n",
    "        # Tokenize batch\n",
    "        val_seq, val_mask, val_y = tokenize_batch(batch)\n",
    "        # Push to device\n",
    "        sent_id, mask, labels = [t.to(device) for t in [val_seq, val_mask, val_y]]\n",
    "        # Deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # Model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            # Compute the validation loss between actual and predicted values\n",
    "            loss =  TripletLoss(preds, labels) \n",
    "            #loss = cross_entropy(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "    # Compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d703069a-7b3b-4eb0-880c-19bc663593d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 4190/4190 [55:10<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Epochs\n",
    "epochs = 1\n",
    "\n",
    "# Init loss\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# data lists\n",
    "train_losses=[]\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    # Train model\n",
    "    train_loss = train() \n",
    "        \n",
    "    # Append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f99353e6-e1d9-41c2-b6f3-6f46aedc656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'This is a test.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79243e1a-5c53-44fc-a8cf-ca3694b52b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer(string, return_tensors=\"pt\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77fc36c3-af42-4ca9-89e1-d9a6152677f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f2b0423-6253-4f0f-9dba-193fd4a4f65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5201,  0.5049, -0.8210,  0.1483,  0.4277, -0.8137,  1.4250,  0.3003,\n",
       "         -0.5727,  0.5645, -0.0306, -0.4534,  0.3267,  0.8140, -0.7719,  0.8386,\n",
       "          0.2793, -0.7581,  0.7456,  1.1674,  0.1890, -0.2233, -0.0487, -0.1938,\n",
       "          0.1842, -0.0789, -0.1713, -0.5290, -0.2594,  0.0734,  0.3469,  0.1272,\n",
       "          0.4685, -0.1654,  1.4168, -0.0442, -0.7272,  0.4253, -0.8182, -0.2440,\n",
       "          0.1256, -0.3787, -0.1420, -0.4979, -0.3011, -0.0638, -0.2782, -0.1950,\n",
       "         -0.7990,  0.5789, -0.8186,  0.1311,  0.3382, -0.3325, -0.5793,  0.0871,\n",
       "         -0.7164, -0.8506, -0.7472, -0.6172,  0.6725, -0.1354,  1.4744, -0.6154,\n",
       "         -0.4263, -0.8802, -0.8408, -0.1092, -0.6150, -0.5463, -0.0990,  0.6968,\n",
       "          0.5898, -0.0431, -0.0584,  0.3107,  0.3548, -0.6508,  0.8356, -1.0803,\n",
       "          1.0688, -0.0106, -0.3511,  0.1426,  1.4892, -1.3131, -0.3424,  0.1309,\n",
       "         -0.9179, -0.0051,  0.1006, -0.5325, -0.8342, -0.2906, -0.4913, -0.1670,\n",
       "         -0.1618,  0.2526,  0.3876,  0.3634,  0.0074, -0.4504, -0.4332,  1.4394,\n",
       "          0.5185, -0.3345,  0.6826,  1.9537,  0.6428, -0.1780, -0.2749,  0.1400,\n",
       "         -1.1098, -0.7386, -0.7403,  1.1272, -0.2092, -0.1859,  0.1766, -0.2315,\n",
       "         -0.5153,  0.7716, -0.9678,  0.3252,  0.0111,  1.0561, -0.7451,  1.2972,\n",
       "          0.3950,  0.3412, -1.2304,  0.2837,  1.5371, -1.5054,  0.8923, -0.0200,\n",
       "         -0.3253, -0.2786, -0.8342,  0.1020,  0.6009, -0.7398, -0.2764,  0.1426,\n",
       "          0.3404, -0.3583, -0.5371,  0.1060, -0.8986,  0.7984, -0.6448,  0.4724,\n",
       "          0.2976,  0.2870, -0.9618,  0.5449,  0.3112, -0.2131, -1.0526,  0.8871,\n",
       "          0.4687,  0.2076, -0.5228,  0.2613,  0.8250,  0.1548, -0.8313,  2.0659,\n",
       "         -1.2453, -0.0121, -0.0249,  0.3732,  0.0622,  0.2594,  0.2169, -0.5621,\n",
       "          0.5439,  0.0248, -0.2278, -1.1251,  0.5852,  1.1903, -0.4432,  0.9370,\n",
       "          0.3139, -0.9549, -0.1468, -0.6411, -0.5492, -0.0864,  0.4384, -1.3192,\n",
       "         -0.9134, -0.2646,  0.5003, -0.7849, -0.0502, -0.0986,  0.2890,  0.2038,\n",
       "          0.2123,  0.2240,  0.6630,  0.6460, -0.4467,  0.4258,  0.9861,  0.3060,\n",
       "          0.3184,  0.1749, -0.9468,  0.5923, -1.3083,  0.0947,  0.2112,  0.2147,\n",
       "         -0.5002,  0.6004,  0.2886, -0.0513,  0.3973, -0.5377, -0.7838,  0.5039,\n",
       "         -0.3068,  0.5558,  0.2700,  0.4875, -0.6347, -1.0132, -1.4033, -1.5332,\n",
       "          1.1346,  0.5030, -0.7853,  1.4081, -0.7935,  1.2016,  0.3245, -0.3281,\n",
       "         -1.0840,  1.1503, -0.3637,  0.4090, -0.2661,  0.2113, -1.1235, -0.7576,\n",
       "         -0.0204, -0.5958,  0.5912, -0.6037, -0.0332, -0.4431,  0.3037, -0.2410,\n",
       "          0.0325,  0.3505, -0.0130, -0.3958,  0.6949,  0.1455,  0.6541,  0.9780,\n",
       "         -0.9004, -1.4764,  0.4400,  0.6144,  0.5982, -0.1469,  0.9173,  0.2056,\n",
       "         -0.3796,  0.2757, -0.5098, -0.4167,  0.4971,  1.1793,  0.0442,  1.3030,\n",
       "          0.4331,  0.9077, -1.0343,  1.1927,  1.7450, -0.5569, -0.4084, -0.3900,\n",
       "         -0.1822, -0.8462,  0.1394,  1.6646, -0.0320, -0.0784, -0.0286,  0.2154,\n",
       "          0.4512,  0.4176,  0.8350, -0.2341, -0.2804,  1.8740, -0.8618, -0.6430,\n",
       "         -1.4876, -0.7843, -0.7076,  0.1201,  0.0701, -0.8837,  0.8227,  0.1705,\n",
       "          0.5365, -0.0632,  0.8187,  0.6280, -0.6167,  0.2267, -0.5179, -0.5788,\n",
       "          0.5959, -0.4100,  0.3344,  0.1276,  1.4051, -0.6458, -0.3214,  0.5338,\n",
       "          0.0990, -0.4834, -0.0232,  0.1706, -0.6943, -0.4361,  0.4131,  0.5932,\n",
       "          0.2415,  0.0146,  0.0357, -0.5012,  0.0845, -0.1729, -0.0852,  1.3773,\n",
       "          0.4592, -0.5150,  0.5403,  0.0081,  0.1483, -0.4231,  0.8951,  0.1457,\n",
       "          0.4665,  0.4700,  0.0954, -0.7359,  0.6642, -0.5673,  0.1329, -0.8189,\n",
       "         -0.2285, -0.9791, -0.5914,  0.3674,  0.4328,  1.5278,  0.2284, -0.6719,\n",
       "         -1.0989, -1.5830, -0.4640, -0.5993, -0.6822, -0.4662, -0.4958, -0.6366,\n",
       "         -0.1559, -1.0676, -0.3097, -0.1498, -0.3637, -0.2999, -0.0419, -0.4878,\n",
       "         -0.4342, -1.2059,  0.5435, -0.5854, -0.4110,  0.3921, -0.8461, -0.2550,\n",
       "         -1.3665,  1.0055, -0.1405,  0.5832,  0.2225, -0.4485,  0.5360,  0.8451,\n",
       "          0.3590,  0.5197, -0.5218, -0.3844,  0.8817, -1.2445,  0.4189, -0.5372,\n",
       "          1.4721,  0.6804,  0.3130, -0.1768, -1.3235,  1.6592, -0.1745,  0.7700,\n",
       "         -0.8836, -0.0837,  0.8272, -1.8938, -0.4085,  0.7954, -0.6596,  0.4053,\n",
       "          0.3141,  0.7567,  0.7491, -0.0549,  0.1823,  0.3098, -0.9795, -1.8373,\n",
       "          0.1651, -0.1332,  0.4922,  0.5729, -1.0083,  0.7760, -1.1558,  0.8166,\n",
       "         -0.1025,  1.5043, -0.4729, -0.1293,  0.3217, -0.5492, -0.0872, -0.6335,\n",
       "          0.5999, -1.6720, -0.6647, -0.4010,  0.7673,  0.2263,  0.0118, -1.4677,\n",
       "          0.5261, -0.6781,  0.4688, -1.1140,  0.2846,  0.8725, -1.0638, -0.5843,\n",
       "         -0.4810, -0.3265,  0.4932, -0.4303,  0.3791, -0.5884, -0.0098,  0.6502,\n",
       "         -1.3011,  1.0536, -0.0884,  0.8569, -0.1726,  0.8569, -1.1045, -1.4437,\n",
       "         -0.5317,  0.6372,  0.7376, -0.9050, -0.3175, -0.4069,  1.1954, -0.4258,\n",
       "         -0.2054, -1.1802, -0.6085,  0.4659,  0.0132, -0.6002,  0.8456, -0.0636,\n",
       "          0.5309,  0.3825, -0.3987,  0.2513,  0.2989,  0.0046, -1.3331, -0.8047,\n",
       "          1.2333,  1.1200, -0.4229,  1.6298,  0.8285, -0.0873,  0.1355, -0.5177,\n",
       "         -1.3474,  0.2610, -0.8573,  0.5857, -0.3905, -0.3012, -0.2769,  0.0199,\n",
       "          1.0932, -1.6927, -0.9019, -0.1929, -0.3912, -1.2244,  0.4509,  1.1351,\n",
       "          0.1540,  0.1671,  0.7003,  0.3664, -0.8990,  0.2399,  0.3208, -0.1045,\n",
       "         -0.4969, -0.2447, -0.2894,  0.2110,  0.2235, -0.8048, -0.5138, -0.0504,\n",
       "         -1.1517,  0.3126,  0.2771,  0.3571, -0.6988,  0.1165,  0.1556, -0.9203,\n",
       "         -1.1580, -0.3452,  0.7315, -0.0340,  0.0960,  0.3785,  0.9735,  0.5976,\n",
       "         -0.9951, -0.3236,  0.0300,  0.6803, -0.1004, -0.4432, -0.6868, -0.4749,\n",
       "          0.7752,  0.5818, -0.8376,  0.5593,  0.5039, -0.3429,  0.4276,  0.4727,\n",
       "          0.5095,  0.2651, -0.4510, -0.1542, -0.7608,  0.1374, -0.9839,  1.0162,\n",
       "         -0.4505,  0.1598,  0.0746, -0.1468,  0.2243,  1.0757,  0.5148,  0.1571,\n",
       "         -0.1792, -0.0320, -0.8122, -0.3481, -0.6920, -0.3605, -0.6403,  0.7929,\n",
       "         -0.2094, -0.2704, -0.3447, -0.7783,  1.7784,  1.0809, -0.5369,  0.4156,\n",
       "         -0.4272,  0.0093, -0.2609,  0.1980, -0.2265, -1.0730,  0.1654, -0.1093,\n",
       "         -0.3035, -0.4677,  0.1798, -0.1511,  0.6797,  0.1218,  0.6333, -1.7022,\n",
       "          0.1367, -1.4613, -0.4856, -0.7921,  0.2302, -0.5249,  1.5686, -0.0463,\n",
       "         -0.2351, -0.4991, -0.9116,  0.1133,  0.6997, -0.5375,  0.3551,  0.5270,\n",
       "          0.2058,  0.6532,  0.8229,  1.2356, -0.0489,  0.4090, -0.3776,  0.4273,\n",
       "          0.4300,  0.1896,  0.2403,  0.6434,  0.1075, -0.7153, -1.0242, -0.2131,\n",
       "          0.8260,  0.2882, -0.0312,  0.7768, -0.0453, -0.7621,  0.2222,  0.2342,\n",
       "          1.1659, -0.0939,  1.0419,  0.4213,  0.3677, -0.6783, -0.1464, -1.0510,\n",
       "         -0.1387, -1.0443,  0.2421, -0.4233, -0.0555,  0.7903,  0.1647, -0.7125,\n",
       "         -0.4397,  1.1124, -1.0457,  1.2143,  1.3028, -0.0841, -1.2659, -0.9789,\n",
       "          0.0768,  0.8251, -0.8826, -0.4129, -0.2972, -0.0833, -0.9758,  0.6187,\n",
       "          0.0671, -1.1850, -0.5271,  0.5476, -0.1593,  0.7966,  0.2591,  0.4163,\n",
       "          0.5374, -0.0155,  1.4279, -0.8424,  0.3008, -0.3388, -0.9332,  0.7381,\n",
       "         -0.2084,  1.4174, -0.1988,  0.6878,  0.3298,  0.3152,  1.1070,  1.8098,\n",
       "          0.4630, -0.3395,  0.0070, -1.2493, -0.5925, -0.8878, -0.7387, -0.2434,\n",
       "         -0.7113, -0.1044, -0.1199,  1.3339, -0.7269,  0.8937,  1.1491, -0.3878,\n",
       "         -0.0353,  0.8813, -0.3209, -0.0735, -0.3738, -0.0290, -0.4742, -1.0477,\n",
       "          0.2299,  0.8793,  0.3117, -0.8204,  0.6449,  0.4382,  0.3168, -0.4081,\n",
       "         -0.0219,  0.0735,  0.4332,  0.6034, -0.0488, -0.6729, -0.0694, -0.2997,\n",
       "         -0.2097, -1.2984, -1.2112, -0.5222,  0.0125,  0.4465, -0.3004,  0.4535]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40351060-b33a-4ac9-99c4-01b20195f8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
