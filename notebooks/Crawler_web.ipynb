{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d364341-b3ec-41c7-84a1-5c696fde676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import transformers\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from selenium import webdriver\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3221abf-8e46-4279-b6ef-37a2d78fb5a4",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32002984-2337-49bf-9de7-03cd408307f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify device\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96880bb-a2c6-4504-a0f8-b43944aa5939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7859069b-063f-43d4-8fd8-20cb684d003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        # Distil Bert model\n",
    "        self.bert = bert\n",
    "        ## Additional layers\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # Relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "        # Dense layer 1\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        # Dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        # Softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model BERT  \n",
    "        cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "        hidden_state = cls_hs[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        \n",
    "        # dense layer 1        \n",
    "        x = self.fc1(pooler)\n",
    "        # ReLU activation\n",
    "        x = self.relu(x)\n",
    "        # Drop out\n",
    "        x = self.dropout(x)\n",
    "        # dense layer 2\n",
    "        x = self.fc2(x)\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654cc9ff-6669-4c43-ae75-fff5562132b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT(bert)\n",
    "# push the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Load trained model (colab)\n",
    "try:\n",
    "    try:\n",
    "        model_save_name = 'model_weights.pt'\n",
    "        path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        print('Google Success')\n",
    "\n",
    "    except:\n",
    "        model_save_name = 'model_weights.pt'\n",
    "        path = \"../models/\" + model_save_name\n",
    "        model.load_state_dict(torch.load(path, \n",
    "                                         map_location=torch.device('cpu')))\n",
    "        print('Local Success')\n",
    "except:\n",
    "    print('No pretrained model found.')\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6305dd2-6842-414e-a58c-d08f35e068c9",
   "metadata": {},
   "source": [
    "## Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0887b644-ff10-4ed5-b0ff-e616d863816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpanPredictor(span, pred_values=False, threshold=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean text\n",
    "    TextCleaner = [\n",
    "        '\\(\\d+.+?Close\\n\\t\\n\\)',\n",
    "        '\\[\\d*\\]',\n",
    "        '\\([^)]*\\)',\n",
    "        '<[^<]+>',\n",
    "    ]\n",
    "    \n",
    "    for Cleaner in TextCleaner:\n",
    "        span = re.sub(Cleaner, '', span, flags=re.DOTALL)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        # Print the prediction values\n",
    "        if pred_values:\n",
    "            return span_class, exps[0]\n",
    "        else:\n",
    "            return span_class    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30fd94be-b54e-41a1-b96a-5b09af1e9871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor([0.8258, 0.1742]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\"The European robin has an orange bill and black wings.\n",
    "\"\"\"\n",
    "\n",
    "SpanPredictor(string, pred_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3b22e-ebc7-4626-a728-95c7a7d6e218",
   "metadata": {},
   "source": [
    "### Web part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bdab5a-b062-4e3b-bad3-1f6fda9cd18b",
   "metadata": {},
   "source": [
    "#### LLIFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e04d92-e44e-4630-a9a4-4d6a32d696a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init index list\n",
    "tree_links_index = []\n",
    "# Extract index pages\n",
    "for i in range(1, 8):\n",
    "    tree_links_index.append('http://www.llifle.com/Encyclopedia/TREES/Species/all/{0}/100/'.format(i))\n",
    "\n",
    "# Init empty list\n",
    "tree_links = []\n",
    "\n",
    "for index_pages in tqdm(tree_links_index):\n",
    "    # Extract XML\n",
    "    URL = index_pages\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Extract links incomplete\n",
    "    tree_links_half = soup.find_all('a')\n",
    "\n",
    "    # Complete the links\n",
    "    tree_links_temp = ['http://www.llifle.com' + pages.get('href') for pages in tree_links_half\n",
    "                           if pages.get('href') != None \n",
    "                           if pages.get('href').startswith('/Encyclopedia/TREES/Family/')]\n",
    "    # Add to all trees\n",
    "    tree_links += tree_links_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e267628-e948-4c63-8b97-453a74509c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty dict\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "# Loop over URLS\n",
    "for tree_link in tqdm(tree_links):\n",
    "    # Get URL\n",
    "    URL = tree_link\n",
    "    # Get Page\n",
    "    page = requests.get(URL)\n",
    "    # Structure page\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Get species name\n",
    "    species = soup.title.text.replace('\\n', '')\n",
    "    # List page \n",
    "    page_list = soup.getText().split('\\n')\n",
    "    # Clean the page\n",
    "    page_list = [spans for spans in page_list if spans != '']\n",
    "    # Get prediction with BERT\n",
    "    predictions = [SpanPredictor(span) for span in page_list]\n",
    "    # Extract data that match description\n",
    "    descriptions = [span for span, pred in zip(page_list, predictions) if pred == 1]\n",
    "    # If data found add to dict\n",
    "    if descriptions:\n",
    "        # Add data\n",
    "        data[species] = descriptions\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "with open('../data/processed/descriptions_trees_llifle.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76c545-22f2-4104-b3a5-e95748e8deaa",
   "metadata": {},
   "source": [
    "#### POWO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26164c22-acb7-4e25-b7e3-8515f466af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tree families\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_trees_and_shrubs_by_taxonomic_family'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find all wikiparts\n",
    "wiki_links = soup.find_all('a')\n",
    "# Create links \n",
    "tree_families = [pages.get('href') for pages in wiki_links \n",
    "                       if pages.get('href') != None \n",
    "                       if pages.get('href').startswith('/wiki/')\n",
    "                       if pages.get('href').endswith('eae')]\n",
    "\n",
    "# Drop duplicates\n",
    "tree_families = list(set(tree_families))\n",
    "# Clean list\n",
    "tree_families = [trees.strip('/wiki/') for trees in tree_families]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93579434-72cd-4b04-874c-cd0f9dc0c2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Init links\\nipni_links = []\\n\\n# Init driver\\nbrowser = webdriver.Safari()\\n\\ntime.sleep(0.5)\\n\\nfor tree_familie in tqdm(tree_families):\\n    # Loop over search pages\\n    for i in range(1, 100):\\n        # create URL base\\n        page_url = \\'https://www.ipni.org/?perPage=500&page={0}&q=family%3A{1}%2Cspecies%3A*\\'.format(i, tree_familie)\\n        # open webpage\\n        browser.get(page_url)\\n        \\n        time.sleep(0.5)\\n        \\n        # Extract links on the page\\n        links = [elem.get_attribute(\"href\") for elem in browser.find_elements_by_tag_name(\\'a\\') \\n                    if elem.get_attribute(\"href\") != None \\n                    if elem.get_attribute(\"href\").startswith(\\'https://www.ipni.org/n/\\')]\\n        \\n        if not links:\\n            break\\n        # Append to ipni links\\n        ipni_links += links\\n\\nbrowser.close()\\n\\nwith open(\\'../data/processed/intermediate_ipni_links_trees.pkl\\', \\'wb\\') as f:\\n    pickle.dump(ipni_links, f)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init links\n",
    "ipni_links = []\n",
    "\n",
    "# Init driver\n",
    "browser = webdriver.Safari()\n",
    "\n",
    "time.sleep(0.5)\n",
    "\n",
    "for tree_familie in tqdm(tree_families):\n",
    "    # Loop over search pages\n",
    "    for i in range(1, 100):\n",
    "        # create URL base\n",
    "        page_url = 'https://www.ipni.org/?perPage=500&page={0}&q=family%3A{1}%2Cspecies%3A*'.format(i, tree_familie)\n",
    "        # open webpage\n",
    "        browser.get(page_url)\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Extract links on the page\n",
    "        links = [elem.get_attribute(\"href\") for elem in browser.find_elements_by_tag_name('a') \n",
    "                    if elem.get_attribute(\"href\") != None \n",
    "                    if elem.get_attribute(\"href\").startswith('https://www.ipni.org/n/')]\n",
    "        \n",
    "        if not links:\n",
    "            break\n",
    "        # Append to ipni links\n",
    "        ipni_links += links\n",
    "\n",
    "browser.close()\n",
    "\n",
    "with open('../data/processed/intermediate_ipni_links_trees.pkl', 'wb') as f:\n",
    "    pickle.dump(ipni_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0dd506f-72b3-4a7e-99ec-b80487b4e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████| 100/100 [03:11<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# Init empty dict\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "# Extract the indices\n",
    "ipni_index = [link.strip('https://www.ipni.org/n/') for link in ipni_links]\n",
    "# Create powo links\n",
    "powo_links = ['http://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:' + index for index in ipni_index]\n",
    "\n",
    "# Init driver\n",
    "browser = webdriver.Safari()\n",
    "\n",
    "# Loop over the links\n",
    "for powo_link in tqdm(powo_links[301100:301200]):\n",
    "    # Navigate to page\n",
    "    browser.get(powo_link)\n",
    "    # Get title page\n",
    "    page_title = browser.title\n",
    "    # Create species\n",
    "    species = page_title.split(' ')[0:2]\n",
    "    species = ' '.join(species)\n",
    "    # Get text page and clean it\n",
    "    page_text = browser.find_element_by_xpath(\"/html/body\").text\n",
    "    # Clean\n",
    "    text_list = page_text.split('\\n')\n",
    "    text_list = [text.strip() for text in text_list \n",
    "                         if text.strip() != '' \n",
    "                         if len(text.strip().split(' ')) > 1]\n",
    "    # Get prediction with BERT\n",
    "    predictions = [SpanPredictor(span) for span in text_list]\n",
    "    # Extract data that match description\n",
    "    descriptions = [span for span, pred in zip(text_list, predictions) if pred == 1]\n",
    "    # If data found add to dict\n",
    "    if descriptions:\n",
    "        # Add data\n",
    "        data[species] = descriptions\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    time.sleep(0.1)\n",
    "    \n",
    "browser.close()   \n",
    "        \n",
    "with open('../data/processed/descriptions_plants_powo.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f30d49b5-bbc3-4055-8c5f-a426728a37ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Ceiba pentandra', 'Ceiba rubriflora', 'Ceiba speciosa'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d52b59f1-db72-4937-b921-ec4c1d88255a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tree up to 20\\xa0m tall, foliage caducous when flowered; trunk ventricose, swollen near the base, when young presenting longitudinal green stripes, provided with stout conical woody prickles to 20\\xa0mm long; vegetative branches short, aculeate, with leaves clustered toward the apex; flowering branches short, diverging from larger branches at an angle of c. 90°',\n",
       " 'Leaves (3 –) 5-foliolate; petiole 25 – 75\\xa0mm long, slightly widened at the base, covered by whitish wax at the ends; leaflets sessile, glabrous, narrowly elliptic, oblong-elliptic, obovate-oblong or ovate-lanceolate, 3 – 3.8 × longer than wide, apex acute, c. 12°, provided with c. 4 mm long, caducous aristae, base cuneate, c. 11°, margin entire, midrib prominent beneath, inconspicuous above; two basal leaflets 20 – 35 × 6 – 10\\xa0mm, shorter than the three distal leaflets, these 45 – 85 × 11 – 25\\xa0mm',\n",
       " 'Stipules c. 3 × 1\\xa0mm, triangular, early caducous',\n",
       " 'Flowers 48 – 53\\xa0mm long; calyx 17 – 20 (23) × 11 – 15\\xa0mm, light green, urceolate, 3-lobed, lobes 5 – 7 mm long, outer surface glabrous, inner surface sericeous in the upper half; corolla urceolate, petals 48 – 51 mm long, uniformly deep red, claw 18 – 23 × 3 – 5\\xa0mm, expanded in a obovate limb, 28 – 31 × 11 – 16\\xa0mm, margin undulate, apex unilaterally apiculate, outer surface sericeous toward the base, puberulous distally, inner surface glabrous becoming sparsely puberulous near the apex; stamens 5, filaments red, joined for 2/3 of their length; staminodial appendages absent; staminal column 28 – 30 mm long, glabrous, base widened, provided with a puberulous band near the swollen portion; free filaments c. 10 mm long, resupinate; anthers sinuous, yellow, 7 – 10 mm long; ovary c. 3 mm, subpyriform, 5-furrowed, glabrous; style 45 – 50 mm long, puberulous only in the basal third, exserted from the staminal column by c. 20 mm; stigma clavate, pubescent',\n",
       " 'Capsule c. 100 × 30 mm, oblong; kapok white',\n",
       " 'Racemes 2 – 4.5\\xa0cm long, 2 – 5-flowered; pedicel 10 – 15 × 2 – 5\\xa0mm; bracteoles 3, caducous, 1 × 3\\xa0mm, spirally alternate near the apex of the pedicel',\n",
       " 'Seed c. 5 × 4\\xa0mm, subpyriform, with microtrichomes, black, hilum c. 2\\xa0mm, triangular, salient.',\n",
       " 'Ceiba rubriflora is diagnosed by the relatively small flowers (to 53\\xa0mm long), deep red petals and stamens, staminal column lacking basal staminodial appendages, and relatively narrow leaflets with entire margins. A similar combination of non-appendaged column and entire leaflets is found in C. erianthos, C. schottii Britten & Baker f. and in some forms of C. pubiflora. C. schottii is restricted to dry woodlands in Mexico and Guatemala (Gibbs & Semir 2003) and presents a quite distinctive cylindrical calyx and much larger flowers with white, narrowly oblong (vs elliptical, oblong-elliptical, obovate-oblong) longer petals (170 – 190\\xa0mm long vs 48 – 51\\xa0mm long in C. rubriflora). Ceiba erianthos occurs mostly in Eastern Brazil but is unknown from the western São Francisco river basin (Gibbs & Semir 2003) where C. rubriflora is found. It differs from this new species by white (vs deep red), densely lanate-villous (vs puberulous) and larger petals (65 – 90\\xa0mm long vs 48 – 51\\xa0mm long), and petiolulate (vs sessile) leaflets.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Ceiba rubriflora']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db4120-d3be-4ddb-a882-682ee7bae053",
   "metadata": {},
   "source": [
    "### PDF part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5513e-ab1d-4e81-ae65-bac98657e40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e4794-5542-4037-940b-eb7ff64b245d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a292cd-2f9f-4d5f-9008-60539546d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a PDF file\n",
    "with pdfplumber.open(\"../data/external/Trees of Peru.pdf\") as pdf:\n",
    "    # Get page\n",
    "    page = pdf.pages[29]\n",
    "    \n",
    "    # Clip top and split page\n",
    "    left = page.crop((0, 0.0 * float(page.height), 0.5 * float(page.width), 1.0 * float(page.height)))\n",
    "    right = page.crop((0.5 * float(page.width), 0.0 * float(page.height), page.width, 1.0 * float(page.height)))\n",
    "    \n",
    "    # Extract text\n",
    "    text = left.extract_text()\n",
    "    # Split on \\n\n",
    "    text_list = text.split('\\n')\n",
    "    # Join text\n",
    "    text_page = ''.join(text_list)\n",
    "    # Split on points\n",
    "    span_list = text_page.split('. ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725c51e-2de9-4c9d-8ad8-5dff416aba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c74ccc-cfa9-40e2-802f-91fe6bb7c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c6b82-600d-4d22-81ec-91c0f3b91e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d4aa6-63a6-464f-adad-7f839dfc6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "predictions = [SpanPredictor(span, pred_values=True) for span in span_list]\n",
    "end = time.time()\n",
    "print(\"Time consumed in working: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d96290-8c4c-4460-8434-9b028697190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b515c-376d-445c-b693-63f24810d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [tuple([span, pred]) for span, pred in zip(span_list, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b805b-332f-4d3d-99ec-41e111392e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb572f7e-1bee-4482-bb6b-215d2e4d23fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
