{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d364341-b3ec-41c7-84a1-5c696fde676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import transformers\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from selenium import webdriver\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../src/models/')\n",
    "import predict_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3221abf-8e46-4279-b6ef-37a2d78fb5a4",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32002984-2337-49bf-9de7-03cd408307f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Success\n"
     ]
    }
   ],
   "source": [
    "# Load BERT\n",
    "model = predict_model.loadBERT(\"../models/\", 'model_weights_splitted_reducednegatives.pt')\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96880bb-a2c6-4504-a0f8-b43944aa5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpanPredictor(span, pred_values=False, threshold=False):\n",
    "      \n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean text\n",
    "    TextCleaner = [\n",
    "        '\\(\\d+.+?Close\\n\\t\\n\\)',\n",
    "        '\\[\\d*\\]',\n",
    "        '\\([^)]*\\)',\n",
    "        '<[^<]+>',\n",
    "    ]\n",
    "    \n",
    "    for Cleaner in TextCleaner:\n",
    "        span = re.sub(Cleaner, '', span, flags=re.DOTALL)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=True)\n",
    "        # Predict class\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        # Print the prediction values\n",
    "        if pred_values:\n",
    "            return span_class, exps[0]\n",
    "        else:\n",
    "            return span_class    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6305dd2-6842-414e-a58c-d08f35e068c9",
   "metadata": {},
   "source": [
    "## Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd94be-b54e-41a1-b96a-5b09af1e9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "The European has a black bill and orange breast.\n",
    "\"\"\"\n",
    "\n",
    "SpanPredictor(string, pred_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3b22e-ebc7-4626-a728-95c7a7d6e218",
   "metadata": {},
   "source": [
    "### Web part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bdab5a-b062-4e3b-bad3-1f6fda9cd18b",
   "metadata": {},
   "source": [
    "#### LLIFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e04d92-e44e-4630-a9a4-4d6a32d696a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init index list\n",
    "tree_links_index = []\n",
    "# Extract index pages\n",
    "for i in range(1, 8):\n",
    "    tree_links_index.append('http://www.llifle.com/Encyclopedia/TREES/Species/all/{0}/100/'.format(i))\n",
    "\n",
    "# Init empty list\n",
    "tree_links = []\n",
    "\n",
    "for index_pages in tqdm(tree_links_index):\n",
    "    # Extract XML\n",
    "    URL = index_pages\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Extract links incomplete\n",
    "    tree_links_half = soup.find_all('a')\n",
    "\n",
    "    # Complete the links\n",
    "    tree_links_temp = ['http://www.llifle.com' + pages.get('href') for pages in tree_links_half\n",
    "                           if pages.get('href') != None \n",
    "                           if pages.get('href').startswith('/Encyclopedia/TREES/Family/')]\n",
    "    # Add to all trees\n",
    "    tree_links += tree_links_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e267628-e948-4c63-8b97-453a74509c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty dict\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "# Loop over URLS\n",
    "for tree_link in tqdm(tree_links):\n",
    "    # Get URL\n",
    "    URL = tree_link\n",
    "    # Get Page\n",
    "    page = requests.get(URL)\n",
    "    # Structure page\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Get species name\n",
    "    species = soup.title.text.replace('\\n', '')\n",
    "    # List page \n",
    "    page_list = soup.getText().split('\\n')\n",
    "    # Clean the page\n",
    "    page_list = [spans for spans in page_list if spans != '']\n",
    "    # Get prediction with BERT\n",
    "    predictions = [SpanPredictor(span) for span in page_list]\n",
    "    # Extract data that match description\n",
    "    descriptions = [span for span, pred in zip(page_list, predictions) if pred == 1]\n",
    "    # If data found add to dict\n",
    "    if descriptions:\n",
    "        # Add data\n",
    "        data[species].append(descriptions)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "with open('../data/processed/descriptions_trees_llifle.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76c545-22f2-4104-b3a5-e95748e8deaa",
   "metadata": {},
   "source": [
    "#### POWO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93579434-72cd-4b04-874c-cd0f9dc0c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▋                                 | 122/2572 [1:03:01<21:17:04, 31.28s/it]"
     ]
    }
   ],
   "source": [
    "# Init links\n",
    "powo_links = []\n",
    "\n",
    "# Init driver\n",
    "browser = webdriver.Safari()\n",
    "\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Loop over search pages\n",
    "for i in tqdm(range(1, 2573)):\n",
    "    # create URL base\n",
    "    #page_url = 'https://www.ipni.org/?perPage=500&page={0}&q=family%3A{1}%2Cspecies%3A*'.format(i, tree_familie)\n",
    "    page_url = 'https://www.ipni.org/?perPage=500&page{0}=&q=species%3A*'.format(i)\n",
    "\n",
    "    # open webpage\n",
    "    browser.get(page_url)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # Extract links on the page\n",
    "    links = [elem.get_attribute(\"href\") for elem in browser.find_elements_by_tag_name('a') \n",
    "                if elem.get_attribute(\"href\") != None\n",
    "                if elem.get_attribute(\"href\").startswith('http://powo.science.kew.org/taxon')]\n",
    "\n",
    "    if not links:\n",
    "        continue\n",
    "    # Append to powo links\n",
    "    powo_links += links\n",
    "\n",
    "with open('../data/processed/intermediate_powo_links.pkl', 'wb') as f:\n",
    "    pickle.dump(powo_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd506f-72b3-4a7e-99ec-b80487b4e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty dict\n",
    "data_web = collections.defaultdict(list)\n",
    "\n",
    "# loop over URLs\n",
    "for URL in tqdm(powo_links):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Get title\n",
    "    species = soup.title.text.split(' | ')[0]\n",
    "    # Check spans\n",
    "    for span in soup.find_all('dd'):\n",
    "        span = span.text.strip()\n",
    "        # If true append\n",
    "        if SpanPredictor(span):\n",
    "            data_web[species].append(span)\n",
    "            \n",
    "with open('../data/processed/description_web_powo.pkl', 'wb') as f:\n",
    "    pickle.dump(powo_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c9c7ad-365c-4a6d-9863-7d79fa3454a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Get tree families\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_trees_and_shrubs_by_taxonomic_family'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find all wikiparts\n",
    "wiki_links = soup.find_all('a')\n",
    "# Create links \n",
    "tree_families = [pages.get('href') for pages in wiki_links \n",
    "                       if pages.get('href') != None \n",
    "                       if pages.get('href').startswith('/wiki/')\n",
    "                       if pages.get('href').endswith('eae')]\n",
    "\n",
    "# Drop duplicates\n",
    "tree_families = list(set(tree_families))\n",
    "# Clean list\n",
    "tree_families = [trees.strip('/wiki/') for trees in tree_families]\n",
    "\n",
    "# Init links\n",
    "ipni_links = []\n",
    "\n",
    "# Init driver\n",
    "browser = webdriver.Safari()\n",
    "\n",
    "time.sleep(0.5)\n",
    "\n",
    "for tree_familie in tqdm(tree_families):\n",
    "    # Loop over search pages\n",
    "    for i in range(1, 2):\n",
    "        # create URL base\n",
    "        #page_url = 'https://www.ipni.org/?perPage=500&page={0}&q=family%3A{1}%2Cspecies%3A*'.format(i, tree_familie)\n",
    "        page_url = 'https://www.ipni.org/?perPage=500&page{0}=&q=species%3A*'.format(i)\n",
    "        \n",
    "        # open webpage\n",
    "        browser.get(page_url)\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Extract links on the page\n",
    "        links = [elem.get_attribute(\"href\") for elem in browser.find_elements_by_tag_name('a') \n",
    "                    if elem.get_attribute(\"href\") != None \n",
    "                    if elem.get_attribute(\"href\").startswith('https://www.ipni.org/n/')]\n",
    "        \n",
    "        if not links:\n",
    "            break\n",
    "        # Append to ipni links\n",
    "        ipni_links += links\n",
    "\n",
    "\n",
    "browser.close()\n",
    "\n",
    "with open('../data/processed/intermediate_ipni_links_trees.pkl', 'wb') as f:\n",
    "    pickle.dump(ipni_links, f)\n",
    "    \n",
    "ipni_links = pickle.load(open('../data/processed/intermediate_ipni_links_trees.pkl', 'rb'))\n",
    "\n",
    "# Extract the indices\n",
    "ipni_index = [link.strip('https://www.ipni.org/n/') for link in ipni_links]\n",
    "# Create powo links\n",
    "powo_links = ['http://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:' + index for index in ipni_index]\n",
    "\n",
    "\n",
    "# Init driver\n",
    "browser = webdriver.Safari()\n",
    "\n",
    "# Loop over the links\n",
    "for powo_link in tqdm(powo_links):\n",
    "    # Navigate to page\n",
    "    browser.get(powo_link)\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    # Get title page\n",
    "    page_title = browser.title\n",
    "    # Create species\n",
    "    species = page_title.split(' | ')[0]\n",
    "    #print(species)\n",
    "    # Get text page and clean it\n",
    "    page_text = browser.find_element_by_xpath(\"/html/body\").text\n",
    "    # Clean\n",
    "    text_list = page_text.split('\\n')\n",
    "    text_list = [text.strip() for text in text_list \n",
    "                         if text.strip() != '' \n",
    "                         if len(text.strip().split(' ')) > 1]\n",
    "    # Get prediction with BERT\n",
    "    predictions = [SpanPredictor(span) for span in text_list]\n",
    "    # Extract data that match description\n",
    "    descriptions = [span for span, pred in zip(text_list, predictions) if pred == 1]\n",
    "    # If data found add to dict\n",
    "    if descriptions:\n",
    "        # Add data\n",
    "        data[species].append(descriptions)\n",
    "    else:\n",
    "        continue\n",
    "            \n",
    "browser.close()   \n",
    "with open('../data/processed/descriptions_plants_powo.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
