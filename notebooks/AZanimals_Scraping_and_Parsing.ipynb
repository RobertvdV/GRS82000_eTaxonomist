{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81530565-19c5-46e3-8a35-7a3e1ab842f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd79668-57ce-4b9d-b3b9-38330512659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init dict\n",
    "Data = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3551733-1de6-40f2-98d8-333a2ee6ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://a-z-animals.com/animals/'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Finds all links\n",
    "Animals = soup.find_all('a')\n",
    "\n",
    "# Create a list with links\n",
    "AnimalsA_Zpages = [pages.get('href') for pages in Animals \n",
    "                   if pages.get('href') != None\n",
    "                   if pages.get('href').startswith('https://a-z-animals.com')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2854e86e-5dc7-4b01-a3ac-0cfe8ca7ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes references in text\n",
    "ReferenceRemover = '\\[\\d*\\]'\n",
    "\n",
    "for AnimalPage in AnimalsA_Zpages[:]:\n",
    "    \n",
    "    # Open the page\n",
    "    page = requests.get(AnimalPage, timeout=5)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    for Tags in soup.find_all('h2'):\n",
    "\n",
    "        # Get the chapters\n",
    "        try:\n",
    "            Chapter = Tags['id']\n",
    "        # skip other stuff\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Finds descriptions or similar\n",
    "        if Chapter.endswith('appearance'):\n",
    "            \n",
    "            '''\n",
    "            # Get species name\n",
    "            Species = soup.title\\\n",
    "                            .string\\\n",
    "                            .split(' Animal')[0]\\\n",
    "                            .lower()\\\n",
    "                            .capitalize()\n",
    "            '''\n",
    "            \n",
    "            # Get species name\n",
    "            Species = soup.find('h1').text\\\n",
    "                                        .lower()\\\n",
    "                                        .capitalize()\n",
    "\n",
    "\n",
    "            # Get the next sibling (text)\n",
    "            for Text in Tags.find_next_siblings('p'):\n",
    "\n",
    "                # Add description data to dict\n",
    "                if Chapter in Text.find_previous_siblings('h2')[0]['id']:\n",
    "                    # Remove source\n",
    "                    Paragraph = re.sub(ReferenceRemover, '', Text.text)\n",
    "                    # Add to dict\n",
    "                    Data[Species].append(tuple([1, Paragraph]))\n",
    "                    \n",
    "                    # Split into Sentences\n",
    "                    #SentenceList = Paragraph.split('. ')\n",
    "                    # Add to the dict\n",
    "                    #Data[Species] += [(1, Sentence) for Sentence in SentenceList]\n",
    "          \n",
    "                # Add non description data to dict\n",
    "                elif Chapter not in Text.find_previous_siblings('h2')[0]['id']:\n",
    "                    # Remove source\n",
    "                    Paragraph = re.sub(ReferenceRemover, '', Text.text)\n",
    "                    # Add to dict\n",
    "                    Data[Species].append(tuple([0, Paragraph]))\n",
    "                    \n",
    "                    # Split into Sentences\n",
    "                    #SentenceList = Paragraph.split('. ')\n",
    "                    # Add to the dict\n",
    "                    #Data[Species] += [(0, Sentence) for Sentence in SentenceList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d17d2e-7735-409c-a411-09a03df65045",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/dataAZ_withMeasurements.pkl', 'wb') as f:\n",
    "    pickle.dump(Data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4c4b05-89f1-4269-903d-cfd229743a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3da05e-c171-4c6b-b846-03bcd356e67d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eTaxonomist]",
   "language": "python",
   "name": "conda-env-eTaxonomist-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
