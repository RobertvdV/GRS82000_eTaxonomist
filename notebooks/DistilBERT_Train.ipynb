{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16508ef0-2c9f-48e5-96fb-f1fdb7bc1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import re\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c15c67-e618-4585-96d9-387b8dfc8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify device\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f74e800-fc01-4784-b70b-c67c6cf5a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptionDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Description dataset without species names.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.samples = []\n",
    "        self._init_dataset()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "     \n",
    "    def _init_dataset(self):\n",
    "        \n",
    "        # Load the pickle list\n",
    "        datalist = glob.glob(self.root_dir + 'train*.pkl')\n",
    "        # Init list\n",
    "        data_values = []\n",
    "        # Loop over the pickles\n",
    "        for data in datalist:\n",
    "            # Open the pickles\n",
    "            datadict = pickle.load(open(data, 'rb'))\n",
    "            # Undict and append\n",
    "            data_values += (list(chain.from_iterable(datadict.values())))\n",
    "        \n",
    "        # Drop double values \n",
    "        data_values = list(set(data_values))\n",
    "        \n",
    "        nested_values = [[tuple([1, span]) if text[0] == 1 else tuple([0, span]) \n",
    "                          for span in self.random_text_splitter(text[1])] \n",
    "                         for text in data_values]\n",
    "        \n",
    "        self.samples += list(chain.from_iterable(nested_values))\n",
    "        \n",
    "    def random_text_splitter(self, text):\n",
    "        \n",
    "        # Dont split short sentences\n",
    "        if len(text.split()) <= 10:\n",
    "            return [text]\n",
    "\n",
    "        # Split text\n",
    "        words = text.split()\n",
    "        # Get length\n",
    "        sentlength = len(words)\n",
    "        # Random int\n",
    "        randomint = random.randint(10, sentlength)\n",
    "        # Check sentences from text\n",
    "        parts = sentlength // randomint\n",
    "\n",
    "        # Create sentences\n",
    "        sentences = [' '.join(words[randomint*i:randomint*(i+1)]) for i in range(0, parts)]\n",
    "        last_part = ' '.join(words[randomint*parts:])\n",
    "        if len(last_part.split()) <= 10:\n",
    "            sentences[-1] = sentences[-1] + last_part\n",
    "        else:\n",
    "            sentences += [last_part]\n",
    "\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82615b37-f981-447f-a349-33b8d7d4bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted @Local\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Colab\n",
    "    from google.colab import drive\n",
    "    root = '/content/gdrive/My Drive/'\n",
    "    drive.mount('/content/gdrive')\n",
    "    print('Mounted @Google')\n",
    "except:\n",
    "    # Local\n",
    "    root = \"../data/processed/\"\n",
    "    print('Mounted @Local')\n",
    "\n",
    "# Load data\n",
    "data = DescriptionDataset(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41aff96c-c4b9-4e8b-8f2c-67914df059d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1656393 samples.\n",
      "71004 labels with 1 (true).\n",
      "1585389 labels with 0 (false).\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data\n",
    "ones = Counter(ones[0] for ones in data if ones[0] == 1)\n",
    "zeros = Counter(ones[0] for ones in data if ones[0] == 0)\n",
    "\n",
    "print('{0} samples.'. format(len(data)))\n",
    "print('{0} labels with 1 (true).'.format(ones[1]))\n",
    "print('{0} labels with 0 (false).'.format(zeros[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8b6ff3-09b2-47be-ad1a-4cf28a195217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame using data\n",
    "df = pd.DataFrame(data[0:10000], columns =['label', 'text'])\n",
    "\n",
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2021, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2021, \n",
    "                                                                test_size=0.8, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4e23d8-ce37-4dcf-9f51-a3dbb1ccc40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Bert mode\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d37d8ed-f847-4807-8c50-dc990eb4b736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [01:17<?, ?it/s]\n",
      "  0%|                                                     | 0/2 [00:57<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#start = time.time()\n",
    "# Tokenize train data\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 512,\n",
    "    padding=True,\n",
    "    truncation=True)\n",
    "\n",
    "# Tokenize validation data\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 512,\n",
    "    padding=True,\n",
    "    truncation=True)\n",
    "\n",
    "# tokenize test data\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 512,\n",
    "    padding=True,\n",
    "    truncation=True)\n",
    "#end = time.time()\n",
    "#print(\"Time consumed in working: \",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cced7c-6883-4dc0-960c-8ac2ea4fec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to Tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02debd18-23af-431c-9f90-5c7cf5b28b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch\n",
    "batch_size = 1\n",
    "\n",
    "# Warp tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "# Random sample (skewed set)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# DataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "# Random sample\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "# DataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d381a7ad-31de-4877-b253-9f3a8471d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51506f3-8673-4a16-a5af-e5ffda02c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        # Distil Bert model\n",
    "        self.bert = bert\n",
    "        ## Additional layers\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # Relu \n",
    "        self.relu =  nn.ReLU()\n",
    "        # Linear I \n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        # Linear II (Out)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        # Softmax\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        # Pass data trough bert and extract \n",
    "        cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "        hidden_state = cls_hs[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        \n",
    "        # Dense layer 1        \n",
    "        x = self.fc1(pooler)\n",
    "        # ReLU activation\n",
    "        x = self.relu(x)\n",
    "        # Drop out\n",
    "        x = self.dropout(x)\n",
    "        # Dense layer 2\n",
    "        x = self.fc2(x)\n",
    "        # Activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e579b2d-d817-4ced-989b-095b59530251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire model\n",
    "model = BERT(bert)\n",
    "\n",
    "# Load trained model (colab)\n",
    "try:\n",
    "    try:\n",
    "        model_save_name = 'saved_weights.pt'\n",
    "        path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        print('Google Success')\n",
    "\n",
    "    except:\n",
    "        model_save_name = 'saved_weights.pt'\n",
    "        path = \"../models/\" + model_save_name\n",
    "        model.load_state_dict(torch.load(path, \n",
    "                                         map_location=torch.device('cpu')))\n",
    "        print('Local Success')\n",
    "except:\n",
    "    print('No pretrained model found.')\n",
    "\n",
    "# Push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0a6f1-fcf1-4cb0-80f2-4b2f015b4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing, Deactivate dropout layer\n",
    "model.eval()\n",
    "# Push a dataset trough the mode\n",
    "BatchTest = next(iter(train_dataloader))\n",
    "\n",
    "# Push the batch to gpu\n",
    "batch = [r.to(device) for r in BatchTest]\n",
    "sent_id, mask, labels = batch\n",
    "\n",
    "# Push the data trough the model\n",
    "preds = model(sent_id, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053dbe7-c225-47d3-b244-fee78b90117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction \n",
    "torch.exp(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be957a-467b-407d-acf3-0d703cb35864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimizer (Adam best for bert)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=1e-5)\n",
    "# Define loss function\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af240d66-a4d6-4d91-95dc-ff8b59c8eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "    \"\"\"\n",
    "    Function to train classification Bert model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        # Push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        # Clear gradients \n",
    "        model.zero_grad()        \n",
    "        # Get predictions\n",
    "        preds = model(sent_id, mask)\n",
    "        # Compute loss\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # Update total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # Backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # Clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to test classification Bert model.\n",
    "    \"\"\"\n",
    "  \n",
    "    # Deactivate dropout layers\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate over batches\n",
    "    for batch in tqdm(val_dataloader):   \n",
    "        # Push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # Deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # Model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            # Compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "    # Compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f3a79-c9b0-4d24-8500-b4630fdb2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "epochs = 1\n",
    "\n",
    "# Init loss\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# data lists\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    # Train model\n",
    "    train_loss = train() \n",
    "    # Evaluate model\n",
    "    valid_loss  = evaluate()\n",
    "        \n",
    "    # Append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.6f}')\n",
    "    print(f'Validation Loss: {valid_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576497f5-063b-4b14-9309-53df3311ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Save @Google\n",
    "    model_save_name = 'saved_weights.pt'\n",
    "    path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
    "    torch.save(model.state_dict(), path)\n",
    "    print('Saved @Google Drive')\n",
    "except:\n",
    "    # Save locally\n",
    "    model_save_name = 'saved_weights.pt'\n",
    "    path = '/notebooks/model/\" + model_save_name\n",
    "    torch.save(model.state_dict(), path)\n",
    "    prin('Saved @local drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701e7b5-bf0a-417d-b00d-0d3d31904a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over values\n",
    "for count, test in enumerate(test_seq):\n",
    "    with torch.no_grad():\n",
    "        # Create prediction (CAN BE DONE AT ONCE WITH LARGER GPU)\n",
    "        preds_temp = model(test_seq[count:count+1].to(device), test_mask[count:count+1].to(device))\n",
    "        preds_temp = preds_temp.detach().cpu().numpy()\n",
    "        \n",
    "        # Init predictions\n",
    "        if count == 0:\n",
    "            preds = preds_temp\n",
    "        else:\n",
    "            # Concat arrays\n",
    "            preds = np.concatenate((preds, preds_temp), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af3538-3fc8-453b-918e-d882b964995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e41b9a-fd4d-4091-b47b-a816b3b503d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d892446-53ef-483d-a6e7-c62ce930652a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7dc74-f337-4af9-add2-c4847dd7b20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b2aa5-e73a-44fc-87bf-ce543230c0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f9e5d-ff98-43c9-b555-fc1fdd06c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# number of training epochs\n",
    "epochs = 1\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        \n",
    "        for batch in tepoch:\n",
    "            \n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "         \n",
    "            # Set model to train\n",
    "            model.train()\n",
    "            # empty list to save model predictions\n",
    "            total_preds=[]\n",
    "\n",
    "            # push the batch to gpu\n",
    "            batch = [r.to(device) for r in batch]\n",
    "            sent_id, mask, labels = batch\n",
    "\n",
    "            # clear previously calculated gradients \n",
    "            model.zero_grad()        \n",
    "            # get model predictions for the current batch\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)           \n",
    "            # backward pass to calculate the gradients\n",
    "            loss.backward()\n",
    "            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Set taqaddum params   \n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy=total_accuracy)\n",
    "            \n",
    "            \n",
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 4 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating:\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "    \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 4 == 0 and not step == 0:\n",
    "      \n",
    "            # Calculate elapsed time in minutes.\n",
    "            #elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27cc1c-1c97-4693-a272-278e6485d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sklearn version slightly faster\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Split\n",
    "TotalCount = len(data)\n",
    "TrainCount = int(0.8 * TotalCount)\n",
    "ValidCount = int(0.1 * TotalCount)\n",
    "TestCount = TotalCount - TrainCount - ValidCount\n",
    "TrainDataset, ValidDataset, TestDataset = random_split(data, (TrainCount, ValidCount, TestCount), \n",
    "                                                       generator=torch.Generator().manual_seed(33))\n",
    "train_label = [label[0] for label in TrainDataset]\n",
    "train_text = [text[1] for text in TrainDataset]\n",
    "\n",
    "valid_label = [label[0] for label in ValidDataset]\n",
    "val_text = [text[1] for text in ValidDataset]\n",
    "\n",
    "test_label = [label[0] for label in TestDataset]\n",
    "test_text = [text[1] for text in TestDataset]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7915d9a-2fa9-470d-b2b4-72a70294fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "class DescriptionDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Description dataset without species names.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.samples = []\n",
    "        self._init_dataset()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):       \n",
    "        return self.samples[idx]\n",
    "    \n",
    "    def _init_dataset(self):\n",
    "        \n",
    "        # Load the pickle list\n",
    "        datalist = glob.glob(self.root_dir + 'train*.pkl')\n",
    "        # Loop over the pickles\n",
    "        for data in datalist:\n",
    "            # Open the pickles\n",
    "            datadict = pickle.load(open(data, 'rb'))\n",
    "            # Undict and append\n",
    "            self.samples += (list(chain.from_iterable(datadict.values())))\n",
    "        \n",
    "        # Drop double values \n",
    "        self.samples = list(set(self.samples))\n",
    "def random_text_sampler(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Randomly breaks a piece of text into x pieces.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dont split short sentences\n",
    "    if len(text.split()) <= 10:\n",
    "        return [text]\n",
    "    \n",
    "    # Split text\n",
    "    words = text.split()\n",
    "    # Get length\n",
    "    sentlength = len(words)\n",
    "    # Random int\n",
    "    randomint = random.randint(10, sentlength)\n",
    "    # Check sentences from text\n",
    "    parts = sentlength // randomint\n",
    "    \n",
    "    # Create sentences\n",
    "    sentences = [' '.join(words[randomint*i:randomint*(i+1)]) for i in range(0, parts)]\n",
    "    sentences += [' '.join(words[randomint*parts:])]\n",
    "    \n",
    "    return sentences\n",
    "    \n",
    "def random_text_sampler(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Randomly breaks a piece of text into 1 to 5 pieces.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(text.split()) < 10:\n",
    "        return text\n",
    "    \n",
    "    # Get the length of the sentence\n",
    "    n = int(len(text) / random.randint(1, 5))\n",
    "    # Break up the text into spans\n",
    "    spans = [text[i:i+n] for i in range(0, len(text), n)]\n",
    "    \n",
    "    # Check if end span is long enough\n",
    "    if len(spans[-1].split()) < 2:\n",
    "        spans_new = spans[:-2]\n",
    "        spans_new.append(spans[-2] + spans[-1])\n",
    "        \n",
    "        return spans_new\n",
    "    else:\n",
    "        return spans\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
