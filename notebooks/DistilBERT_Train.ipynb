{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16508ef0-2c9f-48e5-96fb-f1fdb7bc1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c15c67-e618-4585-96d9-387b8dfc8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify device\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d1d6b8-9352-40a9-9434-9c042efdf0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptionDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Description dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.samples = []\n",
    "        self._init_dataset()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):       \n",
    "        return self.samples[idx]\n",
    "    \n",
    "    def _init_dataset(self):\n",
    "        \n",
    "        # Load the pickle list\n",
    "        datalist = glob.glob(self.root_dir + '*.pkl')\n",
    "        # Loop over the pickles\n",
    "        for data in datalist:\n",
    "            # Open the pickles\n",
    "            datadict = pickle.load(open(data, 'rb'))\n",
    "            # Undict and append\n",
    "            self.samples += (list(chain.from_iterable(datadict.values())))\n",
    "        \n",
    "        # Drop double values (from WIKI)\n",
    "        self.samples = list(set(self.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82615b37-f981-447f-a349-33b8d7d4bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Colab\n",
    "    from google.colab import drive\n",
    "    root = '/content/gdrive/My Drive/'\n",
    "    drive.mount('/content/gdrive')\n",
    "except:\n",
    "    # Local\n",
    "    root = \"../data/processed/\"\n",
    "\n",
    "# Load data\n",
    "data = DescriptionDataset(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aff96c-c4b9-4e8b-8f2c-67914df059d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = Counter(ones[0] for ones in data if ones[0] == 1)\n",
    "zeros = Counter(ones[0] for ones in data if ones[0] == 0)\n",
    "\n",
    "print('{0} samples.'. format(len(data)))\n",
    "print('{0} labels with 1 (true).'.format(ones[1]))\n",
    "print('{0} labels with 0 (false).'.format(zeros[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca195c-006d-4dc7-aa39-97123b7794b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sklearn version slightly faster\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Split\n",
    "TotalCount = len(data)\n",
    "TrainCount = int(0.8 * TotalCount)\n",
    "ValidCount = int(0.1 * TotalCount)\n",
    "TestCount = TotalCount - TrainCount - ValidCount\n",
    "TrainDataset, ValidDataset, TestDataset = random_split(data, (TrainCount, ValidCount, TestCount), \n",
    "                                                       generator=torch.Generator().manual_seed(33))\n",
    "train_label = [label[0] for label in TrainDataset]\n",
    "train_text = [text[1] for text in TrainDataset]\n",
    "\n",
    "valid_label = [label[0] for label in ValidDataset]\n",
    "val_text = [text[1] for text in ValidDataset]\n",
    "\n",
    "test_label = [label[0] for label in TestDataset]\n",
    "test_text = [text[1] for text in TestDataset]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b6ff3-09b2-47be-ad1a-4cf28a195217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame using data\n",
    "df = pd.DataFrame(data, columns =['label', 'text'])\n",
    "\n",
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2021, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2021, \n",
    "                                                                test_size=0.8, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4e23d8-ce37-4dcf-9f51-a3dbb1ccc40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Bert mode\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37d8ed-f847-4807-8c50-dc990eb4b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Tokenize \n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 512,\n",
    "    padding=True,\n",
    "    truncation=True)\n",
    "\n",
    "# Tokenize \n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 512,\n",
    "    padding=True,\n",
    "    truncation=True)\n",
    "\n",
    "# tokenize \n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 512,\n",
    "    padding=True,\n",
    "    truncation=True)\n",
    "end = time.time()\n",
    "print(\"Time consumed in working: \",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cced7c-6883-4dc0-960c-8ac2ea4fec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to Tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02debd18-23af-431c-9f90-5c7cf5b28b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch\n",
    "batch_size = 1\n",
    "\n",
    "# Warp tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "# Random sample (skewed set)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# DataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "# Random sample\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "# DataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d381a7ad-31de-4877-b253-9f3a8471d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51506f3-8673-4a16-a5af-e5ffda02c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        # Distil Bert model\n",
    "        self.bert = bert\n",
    "        ## Additional layers\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # Relu \n",
    "        self.relu =  nn.ReLU()\n",
    "        # Linear I \n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        # Linear II (Out)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        # Softmax\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        # Pass data trough bert and extract \n",
    "        cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "        hidden_state = cls_hs[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        \n",
    "        # Dense layer 1        \n",
    "        x = self.fc1(pooler)\n",
    "        # ReLU activation\n",
    "        x = self.relu(x)\n",
    "        # Drop out\n",
    "        x = self.dropout(x)\n",
    "        # Dense layer 2\n",
    "        x = self.fc2(x)\n",
    "        # Activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e579b2d-d817-4ced-989b-095b59530251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Success\n"
     ]
    }
   ],
   "source": [
    "# Load the entire model\n",
    "model = BERT(bert)\n",
    "\n",
    "# Load trained model (colab)\n",
    "try:\n",
    "    try:\n",
    "        model_save_name = 'saved_weights.pt'\n",
    "        path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        print('Google Success')\n",
    "\n",
    "    except:\n",
    "        model_save_name = 'saved_weights.pt'\n",
    "        path = \"../models/\" + model_save_name\n",
    "        model.load_state_dict(torch.load(path, \n",
    "                                         map_location=torch.device('cpu')))\n",
    "        print('Local Success')\n",
    "except:\n",
    "    print('No pretrained model found.')\n",
    "\n",
    "# Push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f704c3cb-e2de-41d3-84e3-12d44f2aef1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/saved_weights.pt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0a6f1-fcf1-4cb0-80f2-4b2f015b4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing, Deactivate dropout layer\n",
    "model.eval()\n",
    "# Push a dataset trough the mode\n",
    "BatchTest = next(iter(train_dataloader))\n",
    "\n",
    "# Push the batch to gpu\n",
    "batch = [r.to(device) for r in BatchTest]\n",
    "sent_id, mask, labels = batch\n",
    "\n",
    "# Push the data trough the model\n",
    "preds = model(sent_id, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053dbe7-c225-47d3-b244-fee78b90117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction \n",
    "torch.exp(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be957a-467b-407d-acf3-0d703cb35864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimizer (Adam best for bert)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=1e-5)\n",
    "# Define loss function\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af240d66-a4d6-4d91-95dc-ff8b59c8eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "    \"\"\"\n",
    "    Function to train classification Bert model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        # Push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        # Clear gradients \n",
    "        model.zero_grad()        \n",
    "        # Get predictions\n",
    "        preds = model(sent_id, mask)\n",
    "        # Compute loss\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # Update total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # Backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # Clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "  \n",
    "    # Deactivate dropout layers\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate over batches\n",
    "    for batch in tqdm(val_dataloader):   \n",
    "        # Push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # Deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # Model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            # Compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "    # Compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f3a79-c9b0-4d24-8500-b4630fdb2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "epochs = 1\n",
    "\n",
    "# Init loss\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# data lists\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    # Train model\n",
    "    train_loss = train() \n",
    "    # Evaluate model\n",
    "    valid_loss  = evaluate()\n",
    "        \n",
    "    # Append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.6f}')\n",
    "    print(f'Validation Loss: {valid_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576497f5-063b-4b14-9309-53df3311ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = 'saved_weights.pt'\n",
    "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ace1f-1118-4edd-aa2e-4f1517adb993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the files for re-use\n",
    "\n",
    "output_model_file = '../models/pytorch_distilbert.bin'\n",
    "output_vocab_file = '../models/vocab_distilbert.bin'\n",
    "\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80302e-f66a-4191-a98b-79e1b2bdca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq[200:400].to(device), test_mask[200:400].to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af3538-3fc8-453b-918e-d882b964995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y[200:400], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e41b9a-fd4d-4091-b47b-a816b3b503d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d892446-53ef-483d-a6e7-c62ce930652a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7dc74-f337-4af9-add2-c4847dd7b20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b2aa5-e73a-44fc-87bf-ce543230c0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f9e5d-ff98-43c9-b555-fc1fdd06c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# number of training epochs\n",
    "epochs = 1\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        \n",
    "        for batch in tepoch:\n",
    "            \n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "         \n",
    "            # Set model to train\n",
    "            model.train()\n",
    "            # empty list to save model predictions\n",
    "            total_preds=[]\n",
    "\n",
    "            # push the batch to gpu\n",
    "            batch = [r.to(device) for r in batch]\n",
    "            sent_id, mask, labels = batch\n",
    "\n",
    "            # clear previously calculated gradients \n",
    "            model.zero_grad()        \n",
    "            # get model predictions for the current batch\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)           \n",
    "            # backward pass to calculate the gradients\n",
    "            loss.backward()\n",
    "            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Set taqaddum params   \n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy=total_accuracy)\n",
    "            \n",
    "            \n",
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 4 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating:\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "    \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 4 == 0 and not step == 0:\n",
    "      \n",
    "            # Calculate elapsed time in minutes.\n",
    "            #elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
