{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f21df-02e6-44a8-bb27-a5fbeace22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422cc81a-3376-4c69-b226-9dde93d0fc8e",
   "metadata": {},
   "source": [
    "## Scrape Species List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a5324-02d3-471a-bc28-906cd9848245",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://birdsoftheworld.org/bow/specieslist'\n",
    "#URL = 'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/specieslist'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "BOWlist = soup.find_all('a')\n",
    "\n",
    "URL = 'https://birdsoftheworld.org/bow/specieslist'\n",
    "#URL = 'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/specieslist'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "'''\n",
    "# Create a list with links\n",
    "BOWpages = ['https://birdsoftheworld.org' + pages.get('href').replace('introduction', '') for pages in BOWlist \n",
    "                   if pages.get('href') != None\n",
    "                   if pages.get('href').startswith('/bow/species')]\n",
    "\n",
    "'''\n",
    "# Create a list with links\n",
    "BOWpages = ['https://birdsoftheworld-org.ezproxy.library.wur.nl' + pages.get('href').replace('introduction', '') for pages in BOWlist \n",
    "                   if pages.get('href') != None\n",
    "                   if pages.get('href').startswith('/bow/species')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dddf42e-3bd6-435d-b0c8-7b14e6aebecc",
   "metadata": {},
   "source": [
    "## Open Driver (Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d31f4-c372-4c3f-a073-c1aa8f8d6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init driver\n",
    "browser = webdriver.Safari()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Login URL\n",
    "browser.get(\"https://edepot.wur.nl/541124\")\n",
    "\n",
    "time.sleep(8)\n",
    "\n",
    "# Current page\n",
    "main_page = browser.current_window_handle\n",
    "\n",
    "# Hit Okay\n",
    "Ok = browser.find_element_by_id(\"loginlinkBtn\")\n",
    "Ok.send_keys(u'\\ue007')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# changing the handles to access login page\n",
    "for handle in browser.window_handles:\n",
    "    if handle != main_page:\n",
    "        login_page = handle\n",
    "          \n",
    "# change the control to signin page        \n",
    "browser.switch_to.window(login_page)\n",
    "\n",
    "# Username\n",
    "username = browser.find_element_by_id(\"userNameInput\")\n",
    "username.clear()\n",
    "username.send_keys(\"robert.vandevlasakker@wur.nl\")\n",
    "\n",
    "# Password\n",
    "password = browser.find_element_by_name(\"Password\")\n",
    "password.clear()\n",
    "password.send_keys(\"mantle4_tad0_9Fatty\")\n",
    "\n",
    "# Hit Okay\n",
    "Ok = browser.find_element_by_id(\"submitButton\")\n",
    "Ok.send_keys(u'\\ue007')\n",
    "\n",
    "time.sleep(8)\n",
    "\n",
    "'''\n",
    "# changing the handles to access login page\n",
    "for handle in browser.window_handles:\n",
    "    if handle != main_page or handle!= login_page:\n",
    "        BOW = handle\n",
    "'''\n",
    "\n",
    "# change the control to signin page        \n",
    "browser.switch_to.window(main_page)\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadc540-6dd4-4252-9973-de5a6f715a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible pages\n",
    "Pages = ['Introduction', 'Appearance', 'Systematics', 'Distribution',\n",
    "         'Habitat', 'Movement', 'Foodhabits', 'Sounds',\n",
    "         'Behavior', 'Breeding', 'Demography', 'Conservation',\n",
    "         'Other', 'Priorities']\n",
    "\n",
    "for BirdWebPage in BOWpages[8000:]:\n",
    "    \n",
    "    # Init page\n",
    "    BirdURL = BirdWebPage\n",
    "\n",
    "    # Get the bird page\n",
    "    browser.get(BirdURL)\n",
    "\n",
    "    # Set a sleep timer\n",
    "    SleepTime = random.randint(3, 4)\n",
    "\n",
    "    # Read the XML file\n",
    "    HTML = browser.page_source\n",
    "\n",
    "    # Species name\n",
    "    soup = BeautifulSoup(HTML, 'html.parser')\n",
    "    SpeciesName = soup.title.text.split(' - ')[0].lstrip()\n",
    "\n",
    "    # Create folder\n",
    "    os.mkdir('../data/raw/' + SpeciesName)\n",
    "\n",
    "    # Dump the HTML file\n",
    "    with open('../data/raw/' + SpeciesName + '/' + Pages[0] + '.html', \"w\") as f:\n",
    "          f.write(HTML)\n",
    "\n",
    "    # Check URL for shorter account\n",
    "    URL_intro = browser.current_url\n",
    "\n",
    "    # Sleep Just in case\n",
    "    time.sleep(SleepTime)\n",
    "\n",
    "    # Loop over possible pages\n",
    "    for Index, Page in enumerate(Pages[1:]):\n",
    "\n",
    "        # Create URL\n",
    "        URL = BirdURL + Page.lower()\n",
    "        # Go to URL\n",
    "        browser.get(URL)\n",
    "\n",
    "        # Get current URL\n",
    "        URL_check = browser.current_url\n",
    "\n",
    "        # Check if short account\n",
    "        if URL_check == URL_intro and Index < 2:\n",
    "            time.sleep(4)\n",
    "            break\n",
    "        # Check if links are missing    \n",
    "        elif URL_check == URL_intro and Index > 2:\n",
    "            time.sleep(4)\n",
    "            continue            \n",
    "\n",
    "        # Get the HTML info \n",
    "        HTML = browser.page_source\n",
    "\n",
    "        # Dump the HTML file\n",
    "        with open('../data/raw/' + SpeciesName + '/' + Page + '.html', \"w\") as f:\n",
    "              f.write(HTML)\n",
    "\n",
    "        # Sleep \n",
    "        time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba0dc5-6cf7-4d58-b912-267397b669c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible pages\n",
    "Pages = ['Introduction', 'Appearance', 'Systematics', 'Distribution',\n",
    "         'Habitat', 'Movement', 'Foodhabits', 'Sounds',\n",
    "         'Behavior', 'Breeding', 'Demography', 'Conservation',\n",
    "         'Other', 'Priorities']\n",
    "\n",
    "Testpages = ['https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/bertin1/cur/',\n",
    "             'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/norcar/cur/',\n",
    "             'https://birdsoftheworld-org.ezproxy.library.wur.nl/bow/species/ostric2/cur/']\n",
    "\n",
    "for BirdWebPage in Testpages:\n",
    "    \n",
    "    # Init page\n",
    "    BirdURL = BirdWebPage\n",
    "\n",
    "    # Get the bird page\n",
    "    browser.get(BirdURL)\n",
    "\n",
    "    # Set a sleep timer\n",
    "    SleepTime = random.randint(3, 4)\n",
    "\n",
    "    # Read the XML file\n",
    "    HTML = browser.page_source\n",
    "\n",
    "    # Species name\n",
    "    soup = BeautifulSoup(HTML, 'html.parser')\n",
    "    SpeciesName = soup.title.text.split(' - ')[0].lstrip()\n",
    "\n",
    "    # Create folder\n",
    "    os.mkdir('../data/raw/' + SpeciesName)\n",
    "\n",
    "    # Dump the HTML file\n",
    "    with open('../data/raw/' + SpeciesName + '/' + Pages[0] + '.html', \"w\") as f:\n",
    "          f.write(HTML)\n",
    "\n",
    "    # Check URL for shorter account\n",
    "    URL_intro = browser.current_url\n",
    "    print(URL_intro)\n",
    "\n",
    "    # Sleep Just in case\n",
    "    time.sleep(SleepTime)\n",
    "\n",
    "    # Loop over possible pages\n",
    "    for Index, Page in enumerate(Pages[1:]):\n",
    "\n",
    "        # Create URL\n",
    "        URL = BirdURL + Page.lower()\n",
    "        # Go to URL\n",
    "        browser.get(URL)\n",
    "\n",
    "        # Get current URL\n",
    "        URL_check = browser.current_url\n",
    "        print(URL_check)\n",
    "\n",
    "        # Check if short account\n",
    "        if URL_check == URL_intro and Index < 2:\n",
    "            time.sleep(4)\n",
    "            break\n",
    "        # Check if links are missing    \n",
    "        elif URL_check == URL_intro and Index > 2:\n",
    "            time.sleep(4)\n",
    "            continue            \n",
    "\n",
    "        # Get the HTML info \n",
    "        HTML = browser.page_source\n",
    "\n",
    "        # Dump the HTML file\n",
    "        with open('../data/raw/' + SpeciesName + '/' + Page + '.html', \"w\") as f:\n",
    "              f.write(HTML)\n",
    "\n",
    "        # Sleep \n",
    "        time.sleep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b1ea9-fbda-4080-bbbd-13590b5adb70",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dd1e9-12bb-41b2-91cf-2530a403978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SpeciesList = glob.glob('../data/raw/*')\n",
    "SingleList = [Species + '/Introduction.html' for Species in SpeciesList if len(glob.glob(Species + '/*')) == 1]\n",
    "\n",
    "DataBOW = collections.defaultdict(list)\n",
    "\n",
    "SourceRemover = [\n",
    "    ' \\(\\d+.+?Close\\n\\t\\n\\)'\n",
    "]\n",
    "\n",
    "for BirdXML in SingleList[0:]:\n",
    "    with open(BirdXML) as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "    \n",
    "    # Birdname\n",
    "    BirdName = soup.title.text.split(' - ')[0].lstrip()\n",
    "    \n",
    "    # Chapters\n",
    "    Paragraphs = soup.find_all('p')\n",
    "    \n",
    "    for Para in Paragraphs:\n",
    "        try:\n",
    "            Chapter = Para.find_previous_sibling().find('h2').text\n",
    "            \n",
    "            # Clean text                  \n",
    "            for Remover in SourceRemover:\n",
    "                TextCleaned = re.sub(Remover, '', Para.text, flags=re.DOTALL)\n",
    "            TextCleaned = TextCleaned.replace('\\n', '') \n",
    "            \n",
    "            if Chapter == 'Identification':\n",
    "                DataBOW[BirdName].append(tuple([1, TextCleaned]))\n",
    "            else:\n",
    "                DataBOW[BirdName].append(tuple([0, TextCleaned]))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "with open('../data/processed/dataBOW.pkl', 'wb') as f:\n",
    "    pickle.dump(DataBOW, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78efd03-19c3-445f-86e6-cddda144df9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eTaxonomist]",
   "language": "python",
   "name": "conda-env-eTaxonomist-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
