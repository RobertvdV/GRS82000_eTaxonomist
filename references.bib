
@article{wu_visual_2020,
	title = {Visual Transformers: Token-based Image Representation and Processing for Computer Vision},
	url = {http://arxiv.org/abs/2006.03677},
	shorttitle = {Visual Transformers},
	abstract = {Computer vision has achieved remarkable success by (a) representing images as uniformly-arranged pixel arrays and (b) convolving highly-localized features. However, convolutions treat all image pixels equally regardless of importance; explicitly model all concepts across all images, regardless of content; and struggle to relate spatially-distant concepts. In this work, we challenge this paradigm by (a) representing images as semantic visual tokens and (b) running transformers to densely model token relationships. Critically, our Visual Transformer operates in a semantic token space, judiciously attending to different image parts based on context. This is in sharp contrast to pixel-space transformers that require orders-of-magnitude more compute. Using an advanced training recipe, our {VTs} significantly outperform their convolutional counterparts, raising {ResNet} accuracy on {ImageNet} top-1 by 4.6 to 7 points while using fewer {FLOPs} and parameters. For semantic segmentation on {LIP} and {COCO}-stuff, {VT}-based feature pyramid networks ({FPN}) achieve 0.35 points higher {mIoU} while reducing the {FPN} module's {FLOPs} by 6.5x.},
	journaltitle = {{arXiv}:2006.03677 [cs, eess]},
	author = {Wu, Bichen and Xu, Chenfeng and Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Yan, Zhicheng and Tomizuka, Masayoshi and Gonzalez, Joseph and Keutzer, Kurt and Vajda, Peter},
	urldate = {2021-05-12},
	date = {2020-11-19},
	eprinttype = {arxiv},
	eprint = {2006.03677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{huang_interpretable_2020,
	title = {Interpretable and Accurate Fine-grained Recognition via Region Grouping},
	url = {http://arxiv.org/abs/2005.10411},
	abstract = {We present an interpretable deep model for fine-grained visual recognition. At the core of our method lies the integration of region-based part discovery and attribution within a deep neural network. Our model is trained using image-level object labels, and provides an interpretation of its results via the segmentation of object parts and the identification of their contributions towards classification. To facilitate the learning of object parts without direct supervision, we explore a simple prior of the occurrence of object parts. We demonstrate that this prior, when combined with our region-based part discovery and attribution, leads to an interpretable model that remains highly accurate. Our model is evaluated on major fine-grained recognition datasets, including {CUB}-200, {CelebA} and {iNaturalist}. Our results compare favorably to state-of-the-art methods on classification tasks, and our method outperforms previous approaches on the localization of object parts.},
	journaltitle = {{arXiv}:2005.10411 [cs]},
	author = {Huang, Zixuan and Li, Yin},
	urldate = {2021-05-12},
	date = {2020-05-20},
	eprinttype = {arxiv},
	eprint = {2005.10411},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{radford_learning_2021,
	title = {Learning Transferable Visual Models From Natural Language Supervision},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn {SOTA} image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as {OCR}, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original {ResNet}-50 on {ImageNet} zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/{OpenAI}/{CLIP}.},
	journaltitle = {{arXiv}:2103.00020 [cs]},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	urldate = {2021-05-12},
	date = {2021-02-26},
	eprinttype = {arxiv},
	eprint = {2103.00020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@incollection{ishikawa_contextual_2021,
	location = {Cham},
	title = {Contextual Semantic Interpretability},
	volume = {12625},
	isbn = {978-3-030-69537-8 978-3-030-69538-5},
	url = {http://link.springer.com/10.1007/978-3-030-69538-5_22},
	abstract = {Convolutional neural networks ({CNN}) are known to learn an image representation that captures concepts relevant to the task, but do so in an implicit way that hampers model interpretability. However, one could argue that such a representation is hidden in the neurons and can be made explicit by teaching the model to recognize semantically interpretable attributes that are present in the scene. We call such an intermediate layer a semantic bottleneck. Once the attributes are learned, they can be re-combined to reach the ﬁnal decision and provide both an accurate prediction and an explicit reasoning behind the {CNN} decision. In this paper, we look into semantic bottlenecks that capture context: we want attributes to be in groups of a few meaningful elements and participate jointly to the ﬁnal decision. We use a two-layer semantic bottleneck that gathers attributes into interpretable, sparse groups, allowing them contribute diﬀerently to the ﬁnal output depending on the context. We test our contextual semantic interpretable bottleneck ({CSIB}) on the task of landscape scenicness estimation and train the semantic interpretable bottleneck using an auxiliary database ({SUN} Attributes). Our model yields in predictions as accurate as a non-interpretable baseline when applied to a real-world test set of Flickr images, all while providing clear and interpretable explanations for each prediction.},
	pages = {351--368},
	booktitle = {Computer Vision – {ACCV} 2020},
	publisher = {Springer International Publishing},
	author = {Marcos, Diego and Fong, Ruth and Lobry, Sylvain and Flamary, Rémi and Courty, Nicolas and Tuia, Devis},
	editor = {Ishikawa, Hiroshi and Liu, Cheng-Lin and Pajdla, Tomas and Shi, Jianbo},
	urldate = {2021-05-12},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-69538-5_22},
	note = {Series Title: Lecture Notes in Computer Science},
}