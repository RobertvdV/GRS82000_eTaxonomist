
@article{han_survey_2021,
	title = {A Survey of Label-noise Representation Learning: Past, Present and Future},
	url = {http://arxiv.org/abs/2011.04406},
	shorttitle = {A Survey of Label-noise Representation Learning},
	abstract = {Classical machine learning implicitly assumes that labels of the training data are sampled from a clean distribution, which can be too restrictive for real-world scenarios. However, statistical-learning-based methods may not train deep learning models robustly with these noisy labels. Therefore, it is urgent to design Label-Noise Representation Learning ({LNRL}) methods for robustly training deep models with noisy labels. To fully understand {LNRL}, we conduct a survey study. We ﬁrst clarify a formal deﬁnition for {LNRL} from the perspective of machine learning. Then, via the lens of learning theory and empirical study, we ﬁgure out why noisy labels affect deep models’ performance. Based on the theoretical guidance, we categorize different {LNRL} methods into three directions. Under this uniﬁed taxonomy, we provide a thorough discussion of the pros and cons of different categories. More importantly, we summarize the essential components of robust {LNRL}, which can spark new directions. Lastly, we propose possible research directions within {LNRL}, such as new datasets, instance-dependent {LNRL}, and adversarial {LNRL}. We also envision potential directions beyond {LNRL}, such as learning with feature-noise, preference-noise, domain-noise, similarity-noise, graph-noise and demonstration-noise.},
	journaltitle = {{arXiv}:2011.04406 [cs]},
	author = {Han, Bo and Yao, Quanming and Liu, Tongliang and Niu, Gang and Tsang, Ivor W. and Kwok, James T. and Sugiyama, Masashi},
	urldate = {2021-12-16},
	date = {2021-02-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2011.04406},
	keywords = {Computer Science - Machine Learning},
}

@article{kumar_text_2020,
	title = {Text classification algorithms for mining unstructured data: a {SWOT} analysis},
	volume = {12},
	issn = {2511-2112},
	url = {https://doi.org/10.1007/s41870-017-0072-1},
	doi = {10.1007/s41870-017-0072-1},
	shorttitle = {Text classification algorithms for mining unstructured data},
	abstract = {It has become increasingly crucial and imperative to facilitate knowledge extraction for decision support and deliver targeted information to analysts that span wide application domains. Interestingly, the buzzing term “big data” which is estimated to be 90\% unstructured further makes it difficult to tap and analyze information with traditional tools. Text mining entails defining a process which transforms and substitutes this unstructured data into a structured one to discover knowledge. Use of classification algorithms to intelligently mine text has been studied extensively across literature. This study predominantly surveys the text classification algorithms employed in the process of mining unstructured data to report a conclusive analysis on the trend of their use in terms of their respective strengths, weaknesses, opportunities and threats ({SWOT}). The scope of these algorithms is then explored apropos the application area of sentiment analysis, a typical text classification task. A mapping which determines the unexplored social media technologies and the extent of use of these algorithms within respective social media is proffered to give an insight to the amount of work that has been done in the domain of machine learning based sentiment analysis on social media.},
	pages = {1159--1169},
	number = {4},
	journaltitle = {International Journal of Information Technology},
	shortjournal = {Int. j. inf. tecnol.},
	author = {Kumar, Akshi and Dabas, Vikrant and Hooda, Parul},
	urldate = {2021-12-16},
	date = {2020-12-01},
	langid = {english},
}

@online{billerman_birds_2020,
	title = {Birds of the World - Comprehensive life histories for all bird species and families},
	url = {https://birdsoftheworld.org/bow/home},
	abstract = {Species accounts for all the birds of the world.},
	titleaddon = {Birds of the World},
	type = {Database},
	author = {Billerman, S.M. and Keeney, B.K. and Rodewald, P.G. and Schulenberg, T.S.},
	urldate = {2021-07-13},
	date = {2020},
	langid = {english},
}

@online{facilitated_by_the_royal_botanic_gardens_plants_2019,
	title = {Plants of the World Online},
	url = {https://powo.science.kew.org/},
	abstract = {Launched in March 2017 by the Royal Botanic Gardens, Kew, with an initial focus on tropical African Floras was made possible through the generous support of our benefactors, Michel and Hélène David-Weill. {POWO}’s aim is to empower and inform citizens, policy makers, conservationists, horticulturalists, farmers, gardeners and plant enthusiasts globally. The codebase is open source and Kew supports existing partner networks to set up their own portals, creating a distributed network of botanical data hubs.},
	titleaddon = {{POWO}},
	type = {Database},
	author = {Facilitated by the Royal Botanic Gardens},
	urldate = {2021-07-13},
	date = {2019},
	langid = {english},
}

@article{saito_precision-recall_2015,
	title = {The Precision-Recall Plot Is More Informative than the {ROC} Plot When Evaluating Binary Classifiers on Imbalanced Datasets},
	volume = {10},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/},
	doi = {10.1371/journal.pone.0118432},
	abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics ({ROC}) plots. Alternative measures such as positive predictive value ({PPV}) and the associated Precision/Recall ({PRC}) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While {ROC} plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether {ROC} plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of {ROC} plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. {PRC} plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use {ROC} plots on imbalanced datasets.},
	pages = {e0118432},
	number = {3},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} One},
	author = {Saito, Takaya and Rehmsmeier, Marc},
	urldate = {2021-12-10},
	date = {2015-03-04},
	pmid = {25738806},
	pmcid = {PMC4349800},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	pages = {2825--2830},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	date = {2011},
}

@article{pham_learning_2021,
	title = {Learning to Predict Visual Attributes in the Wild},
	url = {http://arxiv.org/abs/2106.09707},
	abstract = {Visual attributes constitute a large portion of information contained in a scene. Objects can be described using a wide variety of attributes which portray their visual appearance (color, texture), geometry (shape, size, posture), and other intrinsic properties (state, action). Existing work is mostly limited to study of attribute prediction in specific domains. In this paper, we introduce a large-scale in-the-wild visual attribute prediction dataset consisting of over 927K attribute annotations for over 260K object instances. Formally, object attribute prediction is a multi-label classification problem where all attributes that apply to an object must be predicted. Our dataset poses significant challenges to existing methods due to large number of attributes, label sparsity, data imbalance, and object occlusion. To this end, we propose several techniques that systematically tackle these challenges, including a base model that utilizes both low- and high-level {CNN} features with multi-hop attention, reweighting and resampling techniques, a novel negative label expansion scheme, and a novel supervised attribute-aware contrastive learning algorithm. Using these techniques, we achieve near 3.7 {mAP} and 5.7 overall F1 points improvement over the current state of the art. Further details about the {VAW} dataset can be found at http://vawdataset.com/.},
	journaltitle = {{arXiv}:2106.09707 [cs]},
	author = {Pham, Khoi and Kafle, Kushal and Lin, Zhe and Ding, Zhihong and Cohen, Scott and Tran, Quan and Shrivastava, Abhinav},
	urldate = {2021-12-07},
	date = {2021-06-17},
	eprinttype = {arxiv},
	eprint = {2106.09707},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{choudhury_curious_2021,
	title = {The Curious Layperson: Fine-Grained Image Recognition without Expert Labels},
	url = {http://arxiv.org/abs/2111.03651},
	shorttitle = {The Curious Layperson},
	abstract = {Most of us are not experts in speciﬁc ﬁelds, such as ornithology. Nonetheless, we do have general image and language understanding capabilities that we use to match what we see to expert resources. This allows us to expand our knowledge and perform novel tasks without ad-hoc external supervision. On the contrary, machines have a much harder time consulting expert-curated knowledge bases unless trained speciﬁcally with that knowledge in mind. Thus, in this paper we consider a new problem: ﬁne-grained image recognition without expert annotations, which we address by leveraging the vast knowledge available in web encyclopedias. First, we learn a model to describe the visual appearance of objects using non-expert image descriptions. We then train a ﬁnegrained textual similarity model that matches image descriptions with documents on a sentence-level basis. We evaluate the method on two datasets and compare with several strong baselines and the state of the art in cross-modal retrieval. Code is available at: https://github.com/subhc/clever.},
	journaltitle = {{arXiv}:2111.03651 [cs]},
	author = {Choudhury, Subhabrata and Laina, Iro and Rupprecht, Christian and Vedaldi, Andrea},
	urldate = {2021-12-06},
	date = {2021-11-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2111.03651},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@article{liu_learning_2021,
	title = {Learning to Compose Visual Relations},
	url = {http://arxiv.org/abs/2111.09297},
	abstract = {The visual world around us can be described as a structured set of objects and their associated relations. An image of a room may be conjured given only the description of the underlying objects and their associated relations. While there has been significant work on designing deep neural networks which may compose individual objects together, less work has been done on composing the individual relations between objects. A principal difficulty is that while the placement of objects is mutually independent, their relations are entangled and dependent on each other. To circumvent this issue, existing works primarily compose relations by utilizing a holistic encoder, in the form of text or graphs. In this work, we instead propose to represent each relation as an unnormalized density (an energy-based model), enabling us to compose separate relations in a factorized manner. We show that such a factorized decomposition allows the model to both generate and edit scenes that have multiple sets of relations more faithfully. We further show that decomposition enables our model to effectively understand the underlying relational scene structure. Project page at: https://composevisualrelations.github.io/.},
	journaltitle = {{arXiv}:2111.09297 [cs, stat]},
	author = {Liu, Nan and Li, Shuang and Du, Yilun and Tenenbaum, Joshua B. and Torralba, Antonio},
	urldate = {2021-12-06},
	date = {2021-11-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2111.09297},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{kokhlikyan_captum_2020,
	title = {Captum: A unified and generic model interpretability library for {PyTorch}},
	author = {Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and Reblitz-Richardson, Orion},
	date = {2020},
	note = {\_eprint: 2009.07896},
}

@article{strumbelj_efficient_2010,
	title = {An Efficient Explanation of Individual Classifications using Game Theory},
	doi = {10.1145/1756006.1756007},
	abstract = {The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values and overcome the method's initial exponential time complexity with a sampling-based approximation. We present a general method for explaining individual predictions of classification models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method's initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efficient and that the explanations are intuitive and useful.},
	journaltitle = {J. Mach. Learn. Res.},
	author = {Štrumbelj, E. and Kononenko, I.},
	date = {2010},
}

@article{castro_polynomial_2009,
	title = {Polynomial calculation of the Shapley value based on sampling},
	volume = {36},
	issn = {03050548},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054808000804},
	doi = {10.1016/j.cor.2008.04.004},
	abstract = {In this paper we develop a polynomial method based on sampling theory that can be used to estimate the Shapley value (or any semivalue) for cooperative games. Besides analyzing the complexity problem, we examine some desirable statistical properties of the proposed approach and provide some computational results.},
	pages = {1726--1730},
	number = {5},
	journaltitle = {Computers \& Operations Research},
	shortjournal = {Computers \& Operations Research},
	author = {Castro, Javier and Gómez, Daniel and Tejada, Juan},
	urldate = {2021-11-19},
	date = {2009-05},
	langid = {english},
}

@article{shrikumar_computationally_2018,
	title = {Computationally Efficient Measures of Internal Neuron Importance},
	url = {http://arxiv.org/abs/1807.09946},
	abstract = {The challenge of assigning importance to individual neurons in a network is of interest when interpreting deep learning models. In recent work, Dhamdhere et al. proposed Total Conductance, a "natural reﬁnement of Integrated Gradients" for attributing importance to internal neurons. Unfortunately, the authors found that calculating conductance in tensorﬂow required the addition of several custom gradient operators and did not scale well. In this work, we show that the formula for Total Conductance is mathematically equivalent to Path Integrated Gradients computed on a hidden layer in the network. We provide a scalable implementation of Total Conductance using standard tensorﬂow gradient operators that we call Neuron Integrated Gradients. We compare Neuron Integrated Gradients to {DeepLIFT}, a pre-existing computationally efﬁcient approach that is applicable to calculating internal neuron importance. We ﬁnd that {DeepLIFT} produces strong empirical results and is faster to compute, but because it lacks the theoretical properties of Neuron Integrated Gradients, it may not always be preferred in practice.},
	journaltitle = {{arXiv}:1807.09946 [cs, stat]},
	author = {Shrikumar, Avanti and Su, Jocelin and Kundaje, Anshul},
	urldate = {2021-11-19},
	date = {2018-07-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.09946},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{gheisari_survey_2017,
	location = {Guangzhou, China},
	title = {A Survey on Deep Learning in Big Data},
	isbn = {978-1-5386-3220-8 978-1-5386-3221-5},
	url = {http://ieeexplore.ieee.org/document/8005992/},
	doi = {10.1109/CSE-EUC.2017.215},
	abstract = {Big Data means extremely huge large data sets that can be analyzed to ﬁnd patterns, trends. One technique that can be used for data analysis so that able to help us ﬁnd abstract patterns in Big Data is Deep Learning. If we apply Deep Learning to Big Data, we can ﬁnd unknown and useful patterns that were impossible so far. With the help of Deep Learning, {AI} is getting smart. There is a hypothesis in this regard, the more data, the more abstract knowledge. So a handy survey of Big Data, Deep Learning and its application in Big Data is necessary. In this paper, we provide a comprehensive survey on what is Big Data, comparing methods, its research problems, and trends. Then a survey of Deep Learning, its methods, comparison of frameworks, and algorithms is presented. And at last, application of Deep Learning in Big Data, its challenges, open research problems and future trends are presented.},
	eventtitle = {2017 {IEEE} International Conference on Computational Science and Engineering ({CSE}) and {IEEE} International Conference on Embedded and Ubiquitous Computing ({EUC})},
	pages = {173--180},
	booktitle = {22017 {IEEE} International Conference on Computational Science and Engineering ({CSE}) and {IEEE} International Conference on Embedded and Ubiquitous Computing ({EUC})},
	publisher = {{IEEE}},
	author = {Gheisari, Mehdi and Wang, Guojun and Bhuiyan, Md Zakirul Alam},
	urldate = {2021-11-19},
	date = {2017-07},
	langid = {english},
}

@article{xue-wen_chen_big_2014,
	title = {Big Data Deep Learning: Challenges and Perspectives},
	volume = {2},
	issn = {2169-3536},
	url = {http://ieeexplore.ieee.org/document/6817512/},
	doi = {10.1109/ACCESS.2014.2325029},
	shorttitle = {Big Data Deep Learning},
	abstract = {Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.},
	pages = {514--525},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {{Xue-Wen Chen} and {Xiaotong Lin}},
	urldate = {2021-11-19},
	date = {2014},
	langid = {english},
}

@article{tishby_deep_2015,
	title = {Deep Learning and the Information Bottleneck Principle},
	url = {http://arxiv.org/abs/1503.02406},
	abstract = {Deep Neural Networks ({DNNs}) are analyzed via the theoretical framework of the information bottleneck ({IB}) principle. We ﬁrst show that any {DNN} can be quantiﬁed by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the {DNN} and obtain ﬁnite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantiﬁable both by the generalization bound and by the network’s simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	journaltitle = {{arXiv}:1503.02406 [cs]},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	urldate = {2021-11-17},
	date = {2015-03-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1503.02406},
	keywords = {Computer Science - Machine Learning},
}

@article{yosinski_understanding_2015,
	title = {Understanding Neural Networks Through Deep Visualization},
	url = {http://arxiv.org/abs/1506.06579},
	abstract = {Recent years have produced great advances in training large, deep neural networks ({DNNs}), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the ﬁeld will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The ﬁrst is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a {DNN} via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pretrained convnet with minimal setup.},
	journaltitle = {{arXiv}:1506.06579 [cs]},
	author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
	urldate = {2021-11-16},
	date = {2015-06-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.06579},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{rudin_stop_2019,
	title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
	url = {http://arxiv.org/abs/1811.10154},
	abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to explain black box models, rather than creating models that are interpretable in the ﬁrst place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward – it is to design models that are inherently interpretable. This manuscript clariﬁes the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identiﬁes challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
	journaltitle = {{arXiv}:1811.10154 [cs, stat]},
	author = {Rudin, Cynthia},
	urldate = {2021-11-11},
	date = {2019-09-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1811.10154},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@report{welinder_caltech-ucsd_2010,
	title = {Caltech-{UCSD} Birds 200},
	number = {{CNS}-{TR}-2010-001},
	institution = {California Institute of Technology},
	author = {Welinder, P. and Branson, S. and Mita, T. and Wah, C. and Schroff, F. and Belongie, S. and Perona, P.},
	date = {2010},
}

@online{petkova_crafting_2020,
	title = {Crafting a Knowledge Graph: The Semantic Data Modeling Way},
	url = {https://www.ontotext.com/blog/knowledge-graph-with-semantic-data-modeling/},
	shorttitle = {Crafting a Knowledge Graph},
	abstract = {Building a knowledge graph the semantic data modeling way in 10 steps with the help of Ontotext's knowledge graph technology experts.},
	titleaddon = {Ontotext},
	author = {Petkova, Teodora},
	urldate = {2021-11-01},
	date = {2020-02-20},
	langid = {american},
}

@incollection{hutchison_knowledge_2013,
	location = {Berlin, Heidelberg},
	title = {Knowledge Graph Identification},
	volume = {7908},
	isbn = {978-3-642-38708-1 978-3-642-38709-8},
	url = {http://link.springer.com/10.1007/978-3-642-41335-3_34},
	abstract = {Large-scale information processing systems are able to extract massive collections of interrelated facts, but unfortunately transforming these candidate facts into useful knowledge is a formidable challenge. In this paper, we show how uncertain extractions about entities and their relations can be transformed into a knowledge graph. The extractions form an extraction graph and we refer to the task of removing noise, inferring missing information, and determining which candidate facts should be included into a knowledge graph as knowledge graph identiﬁcation. In order to perform this task, we must reason jointly about candidate facts and their associated extraction conﬁdences, identify coreferent entities, and incorporate ontological constraints. Our proposed approach uses probabilistic soft logic ({PSL}), a recently introduced probabilistic modeling framework which easily scales to millions of facts. We demonstrate the power of our method on a synthetic Linked Data corpus derived from the {MusicBrainz} music community and a real-world set of extractions from the {NELL} project containing over 1M extractions and 70K ontological relations. We show that compared to existing methods, our approach is able to achieve improved {AUC} and F1 with signiﬁcantly lower running time.},
	pages = {542--557},
	booktitle = {Advanced Information Systems Engineering},
	publisher = {Springer Berlin Heidelberg},
	author = {Pujara, Jay and Miao, Hui and Getoor, Lise and Cohen, William},
	editor = {Salinesi, Camille and Norrie, Moira C. and Pastor, Óscar},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2021-11-01},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-3-642-41335-3_34},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{gardner_allennlp_2017,
	title = {{AllenNLP}: A Deep Semantic Natural Language Processing Platform},
	author = {Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson F. and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke S.},
	date = {2017},
	note = {\_eprint: {arXiv}:1803.07640},
}

@article{you_large_2020,
	title = {Large Batch Optimization for Deep Learning: Training {BERT} in 76 Minutes},
	abstract = {Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is {LARS}, which by employing layerwise adaptive learning rates trains {RESNET} on {ImageNet} in a few minutes. However, {LARS} performs poorly for attention models like {BERT}, indicating that its performance gains are not consistent across tasks. In this paper, we ﬁrst study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called {LAMB}; we then provide convergence analysis of {LAMB} as well as {LARS}, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of {LAMB} across various tasks such as {BERT} and {RESNET}-50 training with very little hyperparameter tuning. In particular, for {BERT} training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a {TPUv}3 Pod, {BERT} training time can be reduced from 3 days to just 76 minutes (Table 1). The {LAMB} implementation is available online1.},
	pages = {38},
	author = {You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
	date = {2020},
	langid = {english},
}

@article{nobre_land-use_2016,
	title = {Land-use and climate change risks in the Amazon and the need of a novel sustainable development paradigm},
	volume = {113},
	rights = {©  . Freely available online through the {PNAS} open access option.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/113/39/10759},
	doi = {10.1073/pnas.1605516113},
	abstract = {For half a century, the process of economic integration of the Amazon has been based on intensive use of renewable and nonrenewable natural resources, which has brought significant basin-wide environmental alterations. The rural development in the Amazonia pushed the agricultural frontier swiftly, resulting in widespread land-cover change, but agriculture in the Amazon has been of low productivity and unsustainable. The loss of biodiversity and continued deforestation will lead to high risks of irreversible change of its tropical forests. It has been established by modeling studies that the Amazon may have two “tipping points,” namely, temperature increase of 4 °C or deforestation exceeding 40\% of the forest area. If transgressed, large-scale “savannization” of mostly southern and eastern Amazon may take place. The region has warmed about 1 °C over the last 60 y, and total deforestation is reaching 20\% of the forested area. The recent significant reductions in deforestation—80\% reduction in the Brazilian Amazon in the last decade—opens up opportunities for a novel sustainable development paradigm for the future of the Amazon. We argue for a new development paradigm—away from only attempting to reconcile maximizing conservation versus intensification of traditional agriculture and expansion of hydropower capacity—in which we research, develop, and scale a high-tech innovation approach that sees the Amazon as a global public good of biological assets that can enable the creation of innovative high-value products, services, and platforms through combining advanced digital, biological, and material technologies of the Fourth Industrial Revolution in progress.},
	pages = {10759--10768},
	number = {39},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Nobre, Carlos A. and Sampaio, Gilvan and Borma, Laura S. and Castilla-Rubio, Juan Carlos and Silva, José S. and Cardoso, Manoel},
	urldate = {2021-10-12},
	date = {2016-09-27},
	langid = {english},
	pmid = {27638214},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {Amazon land use, Amazon savannization, Amazon sustainability, Amazon tropical forests, climate change impacts},
}

@article{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2021-10-11},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
}

@article{joppa_biodiversity_2011,
	title = {Biodiversity hotspots house most undiscovered plant species},
	volume = {108},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1109389108},
	doi = {10.1073/pnas.1109389108},
	pages = {13171--13176},
	number = {32},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc Natl Acad Sci {USA}},
	author = {Joppa, Lucas N. and Roberts, David L. and Myers, Norman and Pimm, Stuart L.},
	urldate = {2021-10-09},
	date = {2011-08-09},
	langid = {english},
}

@article{pimm_how_2018,
	title = {How to protect half of Earth to ensure it protects sufficient biodiversity},
	volume = {4},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.aat2616},
	doi = {10.1126/sciadv.aat2616},
	pages = {eaat2616},
	number = {8},
	journaltitle = {Science Advances},
	shortjournal = {Sci. Adv.},
	author = {Pimm, Stuart L. and Jenkins, Clinton N. and Li, Binbin V.},
	urldate = {2021-10-09},
	date = {2018-08-15},
	langid = {english},
}

@article{pimentel_economic_1997,
	title = {Economic and Environmental Benefits of Biodiversity},
	volume = {47},
	issn = {00063568, 15253244},
	url = {https://academic.oup.com/bioscience/article-lookup/doi/10.2307/1313097},
	doi = {10.2307/1313097},
	pages = {747--757},
	number = {11},
	journaltitle = {{BioScience}},
	shortjournal = {{BioScience}},
	author = {Pimentel, David and Wilson, Christa and {McCullum}, Christine and Huang, Rachel and Dwen, Paulette and Flack, Jessica and Tran, Quynh and Saltman, Tamara and Cliff, Barbara},
	urldate = {2021-10-08},
	date = {1997-12},
	langid = {english},
}

@article{gowdy_value_1997,
	title = {The Value of Biodiversity: Markets, Society, and Ecosystems},
	volume = {73},
	issn = {00237639},
	url = {http://www.jstor.org/stable/3147075?origin=crossref},
	doi = {10.2307/3147075},
	shorttitle = {The Value of Biodiversity},
	pages = {25},
	number = {1},
	journaltitle = {Land Economics},
	shortjournal = {Land Economics},
	author = {Gowdy, John M.},
	urldate = {2021-10-08},
	date = {1997-02},
	langid = {english},
}

@incollection{raffaelli_links_2010,
	location = {Cambridge},
	title = {The links between biodiversity, ecosystem services and human well-being},
	isbn = {978-0-511-75045-8},
	url = {https://www.cambridge.org/core/product/identifier/CBO9780511750458A013/type/book_part},
	pages = {110--139},
	booktitle = {Ecosystem Ecology},
	publisher = {Cambridge University Press},
	author = {Haines-Young, Roy and Potschin, Marion},
	editor = {Raffaelli, David G. and Frid, Christopher L. J.},
	urldate = {2021-10-08},
	date = {2010},
	langid = {english},
	doi = {10.1017/CBO9780511750458.007},
}

@article{chandler_contribution_2017,
	title = {Contribution of citizen science towards international biodiversity monitoring},
	volume = {213},
	issn = {0006-3207},
	url = {https://www.sciencedirect.com/science/article/pii/S0006320716303639},
	doi = {10.1016/j.biocon.2016.09.004},
	series = {{SI}:Measures of biodiversity},
	abstract = {To meet collective obligations towards biodiversity conservation and monitoring, it is essential that the world's governments and non-governmental organisations as well as the research community tap all possible sources of data and information, including new, fast-growing sources such as citizen science ({CS}), in which volunteers participate in some or all aspects of environmental assessments. Through compilation of a database on {CS} and community-based monitoring ({CBM}, a subset of {CS}) programs, we assess where contributions from {CS} and {CBM} are significant and where opportunities for growth exist. We use the Essential Biodiversity Variable framework to describe the range of biodiversity data needed to track progress towards global biodiversity targets, and we assess strengths and gaps in geographical and taxonomic coverage. Our results show that existing {CS} and {CBM} data particularly provide large-scale data on species distribution and population abundance, species traits such as phenology, and ecosystem function variables such as primary and secondary productivity. Only birds, Lepidoptera and plants are monitored at scale. Most {CS} schemes are found in Europe, North America, South Africa, India, and Australia. We then explore what can be learned from successful {CS}/{CBM} programs that would facilitate the scaling up of current efforts, how existing strengths in data coverage can be better exploited, and the strategies that could maximise the synergies between {CS}/{CBM} and other approaches for monitoring biodiversity, in particular from remote sensing. More and better targeted funding will be needed, if {CS}/{CBM} programs are to contribute further to international biodiversity monitoring.},
	pages = {280--294},
	journaltitle = {Biological Conservation},
	shortjournal = {Biological Conservation},
	author = {Chandler, Mark and See, Linda and Copas, Kyle and Bonde, Astrid M. Z. and López, Bernat Claramunt and Danielsen, Finn and Legind, Jan Kristoffer and Masinde, Siro and Miller-Rushing, Abraham J. and Newman, Greg and Rosemartin, Alyssa and Turak, Eren},
	urldate = {2021-10-08},
	date = {2017-09-01},
	langid = {english},
	keywords = {Citizen science, Community-based monitoring, Databases, Essential biodiversity variables ({EBV}), Global Biodiversity Information Facility ({GBIF}), Group on Earth Observations Biodiversity Observation Network ({GEO} {BON})},
}

@inproceedings{sui_knowledge_2021,
	location = {Online},
	title = {Knowledge Guided Metric Learning for Few-Shot Text Classification},
	url = {https://www.aclweb.org/anthology/2021.naacl-main.261},
	doi = {10.18653/v1/2021.naacl-main.261},
	eventtitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	pages = {3266--3271},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Sui, Dianbo and Chen, Yubo and Mao, Binjie and Qiu, Delai and Liu, Kang and Zhao, Jun},
	urldate = {2021-10-07},
	date = {2021},
	langid = {english},
}

@article{forbes_neural_2019,
	title = {Neural Naturalist: Generating Fine-Grained Image Comparisons},
	url = {http://arxiv.org/abs/1909.04101},
	shorttitle = {Neural Naturalist},
	abstract = {We introduce the new Birds-to-Words dataset of 41k sentences describing ﬁne-grained differences between photographs of birds. The language collected is highly detailed, while remaining understandable to the everyday observer (e.g., “heart-shaped face,” “squat body”). Paragraph-length descriptions naturally adapt to varying levels of taxonomic and visual distance—drawn from a novel stratiﬁed sampling approach—with the appropriate level of detail. We propose a new model called Neural Naturalist that uses a joint image encoding and comparative module to generate comparative language, and evaluate the results with humans who must use the descriptions to distinguish real images.},
	journaltitle = {{arXiv}:1909.04101 [cs]},
	author = {Forbes, Maxwell and Kaeser-Chen, Christine and Sharma, Piyush and Belongie, Serge},
	urldate = {2021-10-06},
	date = {2019-11-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.04101},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@article{de_lutio_digital_2021,
	title = {Digital Taxonomist: Identifying Plant Species in Citizen Scientists' Photographs},
	url = {http://arxiv.org/abs/2106.03774},
	shorttitle = {Digital Taxonomist},
	abstract = {Automatic identiﬁcation of plant specimens from amateur photographs could improve species range maps, thus supporting ecosystems research as well as conservation eﬀorts. However, classifying plant specimens based on image data alone is challenging: some species exhibit large variations in visual appearance, while at the same time diﬀerent species are often visually similar; additionally, species observations follow a highly imbalanced, long-tailed distribution due to diﬀerences in abundance as well as observer biases. On the other hand, most species observations are accompanied by side information about the spatial, temporal and ecological context. Moreover, biological species are not an unordered list of classes but embedded in a hierarchical taxonomic structure. We propose a machine learning model that takes into account these additional cues in a uniﬁed framework. Our Digital Taxonomist is able to identify plant species in photographs more correctly.},
	journaltitle = {{arXiv}:2106.03774 [cs]},
	author = {de Lutio, Riccardo and She, Yihang and D'Aronco, Stefano and Russo, Stefania and Brun, Philipp and Wegner, Jan D. and Schindler, Konrad},
	urldate = {2021-10-05},
	date = {2021-06-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2106.03774},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{van_horn_devil_2017,
	title = {The Devil is in the Tails: Fine-grained Classification in the Wild},
	url = {http://arxiv.org/abs/1709.01450},
	shorttitle = {The Devil is in the Tails},
	abstract = {The world is long-tailed. What does this mean for computer vision and visual recognition? The main two implications are (1) the number of categories we need to consider in applications can be very large, and (2) the number of training examples for most categories can be very small. Current visual recognition algorithms have achieved excellent classification accuracy. However, they require many training examples to reach peak performance, which suggests that long-tailed distributions will not be dealt with well. We analyze this question in the context of {eBird}, a large fine-grained classification dataset, and a state-of-the-art deep network classification algorithm. We find that (a) peak classification performance on well-represented categories is excellent, (b) given enough data, classification performance suffers only minimally from an increase in the number of classes, (c) classification performance decays precipitously as the number of training examples decreases, (d) surprisingly, transfer learning is virtually absent in current methods. Our findings suggest that our community should come to grips with the question of long tails.},
	journaltitle = {{arXiv}:1709.01450 [cs]},
	author = {Van Horn, Grant and Perona, Pietro},
	urldate = {2021-10-03},
	date = {2017-09-05},
	eprinttype = {arxiv},
	eprint = {1709.01450},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wolf_huggingfaces_2020,
	title = {{HuggingFace}'s Transformers: State-of-the-art Natural Language Processing},
	url = {http://arxiv.org/abs/1910.03771},
	shorttitle = {{HuggingFace}'s Transformers},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. {\textbackslash}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified {API}. Backing this library is a curated collection of pretrained models made by and available for the community. {\textbackslash}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at {\textbackslash}url\{https://github.com/huggingface/transformers\}.},
	journaltitle = {{arXiv}:1910.03771 [cs]},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	urldate = {2021-10-02},
	date = {2020-07-13},
	eprinttype = {arxiv},
	eprint = {1910.03771},
	keywords = {Computer Science - Computation and Language},
}

@article{loshchilov_decoupled_2019,
	title = {Decoupled Weight Decay Regularization},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard {SGD} and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with {SGD} with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in {TensorFlow} and {PyTorch}; the complete source code for our experiments is available at https://github.com/loshchil/{AdamW}-and-{SGDW}},
	journaltitle = {{arXiv}:1711.05101 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2021-10-01},
	date = {2019-01-04},
	eprinttype = {arxiv},
	eprint = {1711.05101},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@article{musgrave_pytorch_2020,
	title = {{PyTorch} Metric Learning},
	url = {http://arxiv.org/abs/2008.09164},
	abstract = {Deep metric learning algorithms have a wide variety of applications, but implementing these algorithms can be tedious and time consuming. {PyTorch} Metric Learning is an open source library that aims to remove this barrier for both researchers and practitioners. The modular and flexible design allows users to easily try out different combinations of algorithms in their existing code. It also comes with complete train/test workflows, for users who want results fast. Code and documentation is available at https://www.github.com/{KevinMusgrave}/pytorch-metric-learning.},
	journaltitle = {{arXiv}:2008.09164 [cs]},
	author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
	urldate = {2021-09-29},
	date = {2020-08-20},
	eprinttype = {arxiv},
	eprint = {2008.09164},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: Sentence Embeddings using Siamese {BERT}-Networks},
	url = {http://arxiv.org/abs/1908.10084},
	shorttitle = {Sentence-{BERT}},
	abstract = {{BERT} (Devlin et al., 2018) and {RoBERTa} (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity ({STS}). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with {BERT}. The construction of {BERT} makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-{BERT} ({SBERT}), a modification of the pretrained {BERT} network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with {BERT} / {RoBERTa} to about 5 seconds with {SBERT}, while maintaining the accuracy from {BERT}. We evaluate {SBERT} and {SRoBERTa} on common {STS} tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	journaltitle = {{arXiv}:1908.10084 [cs]},
	author = {Reimers, Nils and Gurevych, Iryna},
	urldate = {2021-09-28},
	date = {2019-08-27},
	eprinttype = {arxiv},
	eprint = {1908.10084},
	keywords = {Computer Science - Computation and Language},
}

@article{schmidhuber_deep_2015,
	title = {Deep Learning in Neural Networks: An Overview},
	volume = {61},
	issn = {08936080},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	shorttitle = {Deep Learning in Neural Networks},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	pages = {85--117},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Schmidhuber, Juergen},
	urldate = {2021-09-27},
	date = {2015-01},
	eprinttype = {arxiv},
	eprint = {1404.7828},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{marcos_semantically_2019,
	title = {Semantically Interpretable Activation Maps: what-where-how explanations within {CNNs}},
	url = {http://arxiv.org/abs/1909.08442},
	shorttitle = {Semantically Interpretable Activation Maps},
	abstract = {A main issue preventing the use of Convolutional Neural Networks ({CNN}) in end user applications is the low level of transparency in the decision process. Previous work on {CNN} interpretability has mostly focused either on localizing the regions of the image that contribute to the result or on building an external model that generates plausible explanations. However, the former does not provide any semantic information and the latter does not guarantee the faithfulness of the explanation. We propose an intermediate representation composed of multiple Semantically Interpretable Activation Maps ({SIAM}) indicating the presence of predefined attributes at different locations of the image. These attribute maps are then linearly combined to produce the final output. This gives the user insight into what the model has seen, where, and a final output directly linked to this information in a comprehensive and interpretable way. We test the method on the task of landscape scenicness (aesthetic value) estimation, using an intermediate representation of 33 attributes from the {SUN} Attributes database. The results confirm that {SIAM} makes it possible to understand what attributes in the image are contributing to the final score and where they are located. Since it is based on learning from multiple tasks and datasets, {SIAM} improve the explanability of the prediction without additional annotation efforts or computational overhead at inference time, while keeping good performances on both the final and intermediate tasks.},
	journaltitle = {{arXiv}:1909.08442 [cs]},
	author = {Marcos, Diego and Lobry, Sylvain and Tuia, Devis},
	urldate = {2021-09-27},
	date = {2019-09-18},
	eprinttype = {arxiv},
	eprint = {1909.08442},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{hoffer_deep_2015,
	location = {Cham},
	title = {Deep Metric Learning Using Triplet Network},
	isbn = {978-3-319-24261-3},
	doi = {10.1007/978-3-319-24261-3_7},
	series = {Lecture Notes in Computer Science},
	abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
	pages = {84--92},
	booktitle = {Similarity-Based Pattern Recognition},
	publisher = {Springer International Publishing},
	author = {Hoffer, Elad and Ailon, Nir},
	editor = {Feragen, Aasa and Pelillo, Marcello and Loog, Marco},
	date = {2015},
	langid = {english},
	keywords = {Deep learning, Metric learning, Representation learning},
}

@software{honnibal_spacy_2020,
	title = {{spaCy}: Industrial-strength Natural Language Processing in Python},
	url = {https://doi.org/10.5281/zenodo.1212303},
	publisher = {Zenodo},
	author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
	date = {2020},
	doi = {10.5281/zenodo.1212303},
}

@inproceedings{van_horn_inaturalist_2018,
	location = {Salt Lake City, {UT}},
	title = {The {iNaturalist} Species Classification and Detection Dataset},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8579012/},
	doi = {10.1109/CVPR.2018.00914},
	abstract = {Existing image classiﬁcation datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the {iNaturalist} species classiﬁcation and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been veriﬁed by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classiﬁcation and detection models. Results show that current nonensemble based methods achieve only 67\% top one classiﬁcation accuracy, illustrating the difﬁculty of the dataset. Speciﬁcally, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {8769--8778},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
	urldate = {2021-09-24},
	date = {2018-06},
	langid = {english},
}

@inproceedings{bucher_semantic_2019,
	location = {Cham},
	title = {Semantic Bottleneck for Computer Vision Tasks},
	isbn = {978-3-030-20890-5},
	doi = {10.1007/978-3-030-20890-5_44},
	series = {Lecture Notes in Computer Science},
	abstract = {This paper introduces a novel method for the representation of images that is semantic by nature, addressing the question of computation intelligibility in computer vision tasks. More specifically, our proposition is to introduce what we call a semantic bottleneck in the processing pipeline, which is a crossing point in which the representation of the image is entirely expressed with natural language, while retaining the efficiency of numerical representations. We show that our approach is able to generate semantic representations that give state-of-the-art results on semantic content-based image retrieval and also perform very well on image classification tasks. Intelligibility is evaluated through user centered experiments for failure detection.},
	pages = {695--712},
	booktitle = {Computer Vision – {ACCV} 2018},
	publisher = {Springer International Publishing},
	author = {Bucher, Maxime and Herbin, Stéphane and Jurie, Frédéric},
	editor = {Jawahar, C. V. and Li, Hongdong and Mori, Greg and Schindler, Konrad},
	date = {2019},
	langid = {english},
}

@article{selvaraju_grad-cam_2017,
	title = {Grad-{CAM}: Visual Explanations From Deep Networks via Gradient-Based Localization},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network ({CNN})-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-{CAM}), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), ﬂowing into the ﬁnal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, {GradCAM} is applicable to a wide variety of {CNN} model-families: (1) {CNNs} with fully-connected layers (e.g. {VGG}), (2) {CNNs} used for structured outputs (e.g. captioning), (3) {CNNs} used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-{CAM} with existing ﬁne-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-{CAM}, and apply it to image classiﬁcation, image captioning, and visual question answering ({VQA}) models, including {ResNet}-based architectures. In the context of image classiﬁcation models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the {ILSVRC}-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and {VQA}, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-{CAM} explanations help users establish appropriate trust in predictions from deep networks and show that Grad-{CAM} helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on {CloudCV} [2]1 and video at youtu.be/{COjUB}9Izk6E.},
	pages = {9},
	author = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	date = {2017},
	langid = {english},
}

@incollection{fleet_visualizing_2014,
	location = {Cham},
	title = {Visualizing and Understanding Convolutional Networks},
	volume = {8689},
	isbn = {978-3-319-10589-5 978-3-319-10590-1},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1_53},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the {ImageNet} benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the {ImageNet} classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our {ImageNet} model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
	pages = {818--833},
	booktitle = {Computer Vision – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	urldate = {2021-09-23},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-10590-1_53},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{zintgraf_visualizing_2017,
	title = {Visualizing Deep Neural Network Decisions: Prediction Difference Analysis},
	url = {http://arxiv.org/abs/1702.04595},
	shorttitle = {Visualizing Deep Neural Network Decisions},
	abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a speciﬁc input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classiﬁers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classiﬁers in application areas such as medicine. We illustrate the method in experiments on natural images ({ImageNet} data), as well as medical images ({MRI} brain scans).},
	journaltitle = {{arXiv}:1702.04595 [cs]},
	author = {Zintgraf, Luisa M. and Cohen, Taco S. and Adel, Tameem and Welling, Max},
	urldate = {2021-09-23},
	date = {2017-02-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1702.04595},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{smilkov_smoothgrad_2017,
	title = {{SmoothGrad}: removing noise by adding noise},
	url = {http://arxiv.org/abs/1706.03825},
	shorttitle = {{SmoothGrad}},
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classiﬁer, one type of explanation is to identify pixels that strongly inﬂuence the ﬁnal decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces {SMOOTHGRAD}, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	journaltitle = {{arXiv}:1706.03825 [cs, stat]},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	urldate = {2021-09-22},
	date = {2017-06-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.03825},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{fleet_visualizing_2014-1,
	location = {Cham},
	title = {Visualizing and Understanding Convolutional Networks},
	volume = {8689},
	isbn = {978-3-319-10589-5 978-3-319-10590-1},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1_53},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the {ImageNet} benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the {ImageNet} classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our {ImageNet} model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
	pages = {818--833},
	booktitle = {Computer Vision – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	urldate = {2021-09-22},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-10590-1_53},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{vig_multiscale_2019,
	location = {Florence, Italy},
	title = {A Multiscale Visualization of Attention in the Transformer Model},
	url = {https://www.aclweb.org/anthology/P19-3007},
	doi = {10.18653/v1/P19-3007},
	abstract = {The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difﬁcult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on {BERT} and {OpenAI} {GPT}-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.},
	eventtitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
	pages = {37--42},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
	publisher = {Association for Computational Linguistics},
	author = {Vig, Jesse},
	urldate = {2021-09-22},
	date = {2019},
	langid = {english},
}

@article{sundararajan_axiomatic_2017,
	title = {Axiomatic Attribution for Deep Networks},
	url = {http://arxiv.org/abs/1703.01365},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisﬁed by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modiﬁcation to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	journaltitle = {{arXiv}:1703.01365 [cs]},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	urldate = {2021-09-22},
	date = {2017-06-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.01365},
	keywords = {Computer Science - Machine Learning},
}

@article{lees_species_2015,
	title = {Species, extinct before we know them?},
	volume = {25},
	issn = {0960-9822},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982214016182},
	doi = {10.1016/j.cub.2014.12.017},
	abstract = {Species are going extinct rapidly, while taxonomic catalogues are still incomplete for even the best-known taxa. Intensive fieldwork is finding species so rare and threatened that some become extinct within years of discovery. Recent bird extinctions in Brazil’s coastal forests suggest that some species may have gone extinct before we knew of their existence.},
	pages = {R177--R180},
	number = {5},
	journaltitle = {Current Biology},
	shortjournal = {Current Biology},
	author = {Lees, Alexander C. and Pimm, Stuart L.},
	urldate = {2021-09-22},
	date = {2015-03-02},
	langid = {english},
}

@article{musgrave_pytorch_2020-1,
	title = {{PyTorch} Metric Learning},
	url = {http://arxiv.org/abs/2008.09164},
	abstract = {Deep metric learning algorithms have a wide variety of applications, but implementing these algorithms can be tedious and time consuming. {PyTorch} Metric Learning is an open source library that aims to remove this barrier for both researchers and practitioners. The modular and flexible design allows users to easily try out different combinations of algorithms in their existing code. It also comes with complete train/test workflows, for users who want results fast. Code and documentation is available at https://www.github.com/{KevinMusgrave}/pytorch-metric-learning.},
	journaltitle = {{arXiv}:2008.09164 [cs]},
	author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
	urldate = {2021-09-15},
	date = {2020-08-20},
	eprinttype = {arxiv},
	eprint = {2008.09164},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{al-halah_automatic_nodate,
	title = {Automatic Discovery, Association Estimation and Learning of Semantic Attributes for a Thousand Categories},
	abstract = {Attribute-based recognition models, due to their impressive performance and their ability to generalize well on novel categories, have been widely adopted for many computer vision applications. However, usually both the attribute vocabulary and the class-attribute associations have to be provided manually by domain experts or large number of annotators. This is very costly and not necessarily optimal regarding recognition performance, and most importantly, it limits the applicability of attribute-based models to large scale data sets. To tackle this problem, we propose an endto-end unsupervised attribute learning approach. We utilize online text corpora to automatically discover a salient and discriminative vocabulary that correlates well with the human concept of semantic attributes. Moreover, we propose a deep convolutional model to optimize class-attribute associations with a linguistic prior that accounts for noise and missing data in text. In a thorough evaluation on {ImageNet}, we demonstrate that our model is able to efﬁciently discover and learn semantic attributes at a large scale. Furthermore, we demonstrate that our model outperforms the state-ofthe-art in zero-shot learning on three data sets: {ImageNet}, Animals with Attributes and {aPascal}/{aYahoo}. Finally, we enable attribute-based learning on {ImageNet} and will share the attributes and associations for future research.},
	pages = {10},
	author = {Al-Halah, Ziad and Stiefelhagen, Rainer},
	langid = {english},
}

@article{schroff_facenet_2015,
	title = {{FaceNet}: A Unified Embedding for Face Recognition and Clustering},
	url = {http://arxiv.org/abs/1503.03832},
	doi = {10.1109/CVPR.2015.7298682},
	shorttitle = {{FaceNet}},
	abstract = {Despite signiﬁcant recent advances in the ﬁeld of face recognition [10, 14, 15, 17], implementing face veriﬁcation and recognition efﬁciently at scale presents serious challenges to current approaches. In this paper we present a system, called {FaceNet}, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, veriﬁcation and clustering can be easily implemented using standard techniques with {FaceNet} embeddings as feature vectors.},
	pages = {815--823},
	journaltitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	urldate = {2021-09-13},
	date = {2015-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1503.03832},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ghojogh_fisher_2020,
	title = {Fisher Discriminant Triplet and Contrastive Losses for Training Siamese Networks},
	url = {http://arxiv.org/abs/2004.04674},
	doi = {10.1109/IJCNN48605.2020.9206833},
	abstract = {Siamese neural network is a very powerful architecture for both feature extraction and metric learning. It usually consists of several networks that share weights. The Siamese concept is topology-agnostic and can use any neural network as its backbone. The two most popular loss functions for training these networks are the triplet and contrastive loss functions. In this paper, we propose two novel loss functions, named Fisher Discriminant Triplet ({FDT}) and Fisher Discriminant Contrastive ({FDC}). The former uses anchor-neighbor-distant triplets while the latter utilizes pairs of anchor-neighbor and anchor-distant samples. The {FDT} and {FDC} loss functions are designed based on the statistical formulation of the Fisher Discriminant Analysis ({FDA}), which is a linear subspace learning method. Our experiments on the {MNIST} and two challenging and publicly available histopathology datasets show the effectiveness of the proposed loss functions.},
	pages = {1--7},
	journaltitle = {2020 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Ghojogh, Benyamin and Sikaroudi, Milad and Shafiei, Sobhan and Tizhoosh, H. R. and Karray, Fakhri and Crowley, Mark},
	urldate = {2021-09-13},
	date = {2020-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.04674},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kaya_deep_2019,
	title = {Deep Metric Learning: A Survey},
	volume = {11},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/11/9/1066},
	doi = {10.3390/sym11091066},
	shorttitle = {Deep Metric Learning},
	abstract = {Metric learning aims to measure the similarity among samples while using an optimal distance metric for learning tasks. Metric learning methods, which generally use a linear projection, are limited in solving real-world problems demonstrating non-linear characteristics. Kernel approaches are utilized in metric learning to address this problem. In recent years, deep metric learning, which provides a better solution for nonlinear data through activation functions, has attracted researchers’ attention in many diﬀerent areas. This article aims to reveal the importance of deep metric learning and the problems dealt with in this ﬁeld in the light of recent studies. As far as the research conducted in this ﬁeld are concerned, most existing studies that are inspired by Siamese and Triplet networks are commonly used to correlate among samples while using shared weights in deep metric learning. The success of these networks is based on their capacity to understand the similarity relationship among samples. Moreover, sampling strategy, appropriate distance metric, and the structure of the network are the challenging factors for researchers to improve the performance of the network model. This article is considered to be important, as it is the ﬁrst comprehensive study in which these factors are systematically analyzed and evaluated as a whole and supported by comparing the quantitative results of the methods.},
	pages = {1066},
	number = {9},
	journaltitle = {Symmetry},
	shortjournal = {Symmetry},
	author = {{Kaya} and {Bilge}},
	urldate = {2021-09-13},
	date = {2019-08-21},
	langid = {english},
}

@article{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	url = {http://arxiv.org/abs/1910.01108},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing ({NLP}), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called {DistilBERT}, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a {BERT} model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	journaltitle = {{arXiv}:1910.01108 [cs]},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	urldate = {2021-09-09},
	date = {2020-02-29},
	eprinttype = {arxiv},
	eprint = {1910.01108},
	keywords = {Computer Science - Computation and Language},
}

@online{ontotext_what_2021,
	title = {What is a Knowledge Graph?},
	url = {https://www.ontotext.com/knowledgehub/fundamentals/what-is-a-knowledge-graph/},
	abstract = {Knowledge graphs are a collection of interlinked descriptions of entities that put data into context and enable data integration, analytics \& sharing.},
	titleaddon = {Ontotext},
	author = {Ontotext},
	urldate = {2021-09-09},
	date = {2021},
	langid = {american},
}

@article{van_lent_explainable_2004,
	title = {An Explainable Artificial Intelligence System for Small-unit Tactical Behavior},
	pages = {8},
	author = {van Lent, Michael and Fisher, William and Mancuso, Michael},
	date = {2004},
	langid = {english},
}

@article{doshi-velez_towards_2017,
	title = {Towards A Rigorous Science of Interpretable Machine Learning},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	journaltitle = {{arXiv}:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	urldate = {2021-08-31},
	date = {2017-03-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1702.08608},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	pages = {8024--8035},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and {DeVito}, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	date = {2019},
}

@inproceedings{munappy_data_2019,
	location = {Kallithea-Chalkidiki, Greece},
	title = {Data Management Challenges for Deep Learning},
	isbn = {978-1-72813-421-5},
	url = {https://ieeexplore.ieee.org/document/8906736/},
	doi = {10.1109/SEAA.2019.00030},
	abstract = {Deep learning is one of the most exciting and fastgrowing techniques in Artiﬁcial Intelligence. The unique capacity of deep learning models to automatically learn patterns from the data differentiates it from other machine learning techniques. Deep learning is responsible for a signiﬁcant number of recent breakthroughs in {AI}. However, deep learning models are highly dependent on the underlying data. So, consistency, accuracy, and completeness of data is essential for a deep learning model. Thus, data management principles and practices need to be adopted throughout the development process of deep learning models. The objective of this study is to identify and categorise data management challenges faced by practitioners in different stages of end-to-end development. In this paper, a case study approach is employed to explore the data management issues faced by practitioners across various domains when they use real-world data for training and deploying deep learning models. Our case study is intended to provide valuable insights to the deep learning community as well as for data scientists to guide discussion and future research in applied deep learning with real-world data.},
	eventtitle = {2019 45th Euromicro Conference on Software Engineering and Advanced Applications ({SEAA})},
	pages = {140--147},
	booktitle = {2019 45th Euromicro Conference on Software Engineering and Advanced Applications ({SEAA})},
	publisher = {{IEEE}},
	author = {Munappy, Aiswarya and Bosch, Jan and Olsson, Helena Holmstrom and Arpteg, Anders and Brinne, Bjorn},
	urldate = {2021-08-29},
	date = {2019-08},
	langid = {english},
}

@article{reed_training_2015,
	title = {Training Deep Neural Networks on Noisy Labels with Bootstrapping},
	url = {http://arxiv.org/abs/1412.6596},
	abstract = {Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overﬁtting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On {MNIST} handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-theart results, and can also beneﬁt from unlabeled face images with no modiﬁcation to our method. On the {ILSVRC}2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.},
	journaltitle = {{arXiv}:1412.6596 [cs]},
	author = {Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	urldate = {2021-08-23},
	date = {2015-04-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.6596},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{samek_explainable_2017,
	title = {Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models},
	url = {http://arxiv.org/abs/1708.08296},
	shorttitle = {Explainable Artificial Intelligence},
	abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of {AI} systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
	journaltitle = {{arXiv}:1708.08296 [cs, stat]},
	author = {Samek, Wojciech and Wiegand, Thomas and Müller, Klaus-Robert},
	urldate = {2021-08-03},
	date = {2017-08-28},
	eprinttype = {arxiv},
	eprint = {1708.08296},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{lei_opening_2018,
	title = {Opening the black box of deep learning},
	url = {http://arxiv.org/abs/1805.08355},
	abstract = {The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various ﬁelds, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physical system, examines deep learning from three different perspectives: microscopic, macroscopic, and physical world views, answers multiple theoretical puzzles in deep learning by using physics principles. For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etc., and proposes the theoretical direction and basis for the further development of deep learning now and in the future. The brilliance of physics ﬂashes in deep learning, we try to establish the deep learning technology based on the scientiﬁc theory of physics.},
	journaltitle = {{arXiv}:1805.08355 [cs, stat]},
	author = {Lei, Dian and Chen, Xiaoxiao and Zhao, Jianfei},
	urldate = {2021-08-03},
	date = {2018-05-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.08355},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{fu_speech--knowledge-graph_2020,
	location = {Yokohama, Japan},
	title = {A Speech-to-Knowledge-Graph Construction System},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/777},
	doi = {10.24963/ijcai.2020/777},
	abstract = {This paper presents a {HAO}-Graph system that generates and visualizes knowledge graphs from a speech in real-time. When a user speaks to the system, {HAO}-Graph transforms the voice into knowledge graphs with key phrases from the original speech as nodes and edges. Different from language-to-language systems, such as Chinese-{toEnglish} and English-to-English, {HAO}-Graph converts a speech into graphs, and is the ﬁrst of its kind. The effectiveness of our {HAO}-Graph system is veriﬁed by a two-hour chairman’s talk in front of two thousand participants at an annual meeting in the form of a satisfaction survey.},
	eventtitle = {Twenty-Ninth International Joint Conference on Artificial Intelligence and Seventeenth Pacific Rim International Conference on Artificial Intelligence \{{IJCAI}-{PRICAI}-20\}},
	pages = {5303--5305},
	booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Fu, Xiaoyi and Zhang, Jie and Yu, Hao and Li, Jiachen and Chen, Dong and Yuan, Jie and Wu, Xindong},
	urldate = {2021-08-02},
	date = {2020-07},
	langid = {english},
}

@article{lipton_mythos_2017,
	title = {The Mythos of Model Interpretability},
	url = {http://arxiv.org/abs/1606.03490},
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	journaltitle = {{arXiv}:1606.03490 [cs, stat]},
	author = {Lipton, Zachary C.},
	urldate = {2021-07-30},
	date = {2017-03-06},
	eprinttype = {arxiv},
	eprint = {1606.03490},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{he_delving_2015,
	location = {Santiago, Chile},
	title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on {ImageNet} Classification},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410480/},
	doi = {10.1109/ICCV.2015.123},
	shorttitle = {Delving Deep into Rectifiers},
	abstract = {Rectiﬁed activation units (rectiﬁers) are essential for state-of-the-art neural networks. In this work, we study rectiﬁer neural networks for image classiﬁcation from two aspects. First, we propose a Parametric Rectiﬁed Linear Unit ({PReLU}) that generalizes the traditional rectiﬁed unit. {PReLU} improves model ﬁtting with nearly zero extra computational cost and little overﬁtting risk. Second, we derive a robust initialization method that particularly considers the rectiﬁer nonlinearities. This method enables us to train extremely deep rectiﬁed models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the {ImageNet} 2012 classiﬁcation dataset. This is a 26\% relative improvement over the {ILSVRC} 2014 winner ({GoogLeNet}, 6.66\% [33]). To our knowledge, our result is the ﬁrst1 to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
	eventtitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {1026--1034},
	booktitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2021-07-30},
	date = {2015-12},
	langid = {english},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2021-07-30},
	date = {2015-05-28},
	langid = {english},
}

@inproceedings{chakraborty_interpretability_2017,
	location = {San Francisco, {CA}},
	title = {Interpretability of deep learning models: A survey of results},
	isbn = {978-1-5386-0435-9},
	url = {https://ieeexplore.ieee.org/document/8397411/},
	doi = {10.1109/UIC-ATC.2017.8397411},
	shorttitle = {Interpretability of deep learning models},
	abstract = {Deep neural networks have achieved near-human accuracy levels in various types of classiﬁcation and prediction tasks including images, text, speech, and video data. However, the networks continue to be treated mostly as black-box function approximators, mapping a given input to a classiﬁcation output. The next step in this human-machine evolutionary process –incorporating these networks into mission critical processes such as medical diagnosis, planning and control – requires a level of trust association with the machine output.},
	eventtitle = {2017 {IEEE} {SmartWorld}, Ubiquitous Intelligence \& Computing, Advanced \& Trusted Computed, Scalable Computing \& Communications, Cloud \& Big Data Computing, Internet of People and Smart City Innovation ({SmartWorld}/{SCALCOM}/{UIC}/{ATC}/{CBDCom}/{IOP}/{SCI})},
	pages = {1--6},
	booktitle = {2017 {IEEE} {SmartWorld}, Ubiquitous Intelligence \& Computing, Advanced \& Trusted Computed, Scalable Computing \& Communications, Cloud \& Big Data Computing, Internet of People and Smart City Innovation ({SmartWorld}/{SCALCOM}/{UIC}/{ATC}/{CBDCom}/{IOP}/{SCI})},
	publisher = {{IEEE}},
	author = {Chakraborty, Supriyo and Tomsett, Richard and Raghavendra, Ramya and Harborne, Daniel and Alzantot, Moustafa and Cerutti, Federico and Srivastava, Mani and Preece, Alun and Julier, Simon and Rao, Raghuveer M. and Kelley, Troy D. and Braines, Dave and Sensoy, Murat and Willis, Christopher J. and Gurram, Prudhvi},
	urldate = {2021-07-30},
	date = {2017-08},
	langid = {english},
}

@article{yang_denert-kg_2020,
	title = {{DeNERT}-{KG}: Named Entity and Relation Extraction Model Using {DQN}, Knowledge Graph, and {BERT}},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2076-3417/10/18/6429},
	doi = {10.3390/app10186429},
	shorttitle = {{DeNERT}-{KG}},
	abstract = {Along with studies on artificial intelligence technology, research is also being carried out actively in the field of natural language processing to understand and process people\&rsquo;s language, in other words, natural language. For computers to learn on their own, the skill of understanding natural language is very important. There are a wide variety of tasks involved in the field of natural language processing, but we would like to focus on the named entity registration and relation extraction task, which is considered to be the most important in understanding sentences. We propose {DeNERT}-{KG}, a model that can extract subject, object, and relationships, to grasp the meaning inherent in a sentence. Based on the {BERT} language model and Deep Q-Network, the named entity recognition ({NER}) model for extracting subject and object is established, and a knowledge graph is applied for relation extraction. Using the {DeNERT}-{KG} model, it is possible to extract the subject, type of subject, object, type of object, and relationship from a sentence, and verify this model through experiments.},
	pages = {6429},
	number = {18},
	journaltitle = {Applied Sciences},
	author = {Yang, {SungMin} and Yoo, {SoYeop} and Jeong, {OkRan}},
	urldate = {2021-07-28},
	date = {2020-01},
	langid = {english},
	note = {Number: 18
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {{BERT}, {DQN}, knowledge graph, named entity recognition, relation extraction},
}

@article{wu_scalable_2020,
	title = {Scalable Zero-shot Entity Linking with Dense Entity Retrieval},
	url = {http://arxiv.org/abs/1911.03814},
	abstract = {This paper introduces a conceptually simple, scalable, and highly effective {BERT}-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is deﬁned only by a short textual description. The ﬁrst stage does retrieval in a dense space deﬁned by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a crossencoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zeroshot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. {TACKBP}-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive crossencoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github. com/facebookresearch/{BLINK}.},
	journaltitle = {{arXiv}:1911.03814 [cs]},
	author = {Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke},
	urldate = {2021-07-27},
	date = {2020-09-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1911.03814},
	keywords = {Computer Science - Computation and Language},
}

@article{stewart_icdm_2019,
	title = {{ICDM} 2019 Knowledge Graph Contest: Team {UWA}},
	url = {http://arxiv.org/abs/1909.01807},
	shorttitle = {{ICDM} 2019 Knowledge Graph Contest},
	abstract = {We present an overview of our triple extraction system for the {ICDM} 2019 Knowledge Graph Contest. Our system uses a pipeline-based approach to extract a set of triples from a given document. It offers a simple and effective solution to the challenge of knowledge graph construction from domain-specific text. It also provides the facility to visualise useful information about each triple such as the degree, betweenness, structured relation type(s), and named entity types.},
	journaltitle = {{arXiv}:1909.01807 [cs]},
	author = {Stewart, Michael and Enkhsaikhan, Majigsuren and Liu, Wei},
	urldate = {2021-07-27},
	date = {2019-09-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.01807},
	keywords = {Computer Science - Computation and Language},
}

@article{sergeev_application_2020,
	title = {An Application of Semantic Relation Extraction Models},
	abstract = {Natural language processing ({NLP}) is growing to be one of the largest subfields of computer science, with application to many other fields such as linguistics or information extraction. This large growth has resulted in higher availability and a variety of resources that contributors can work with. Semantic relation datasets as a result have seen an increase in number and many today, including high profile companies at the head of {NLP}, are creating stateof-the-art machine learning models. These models are capable of identifying these relations from sentences by training on this selection of datasets. The number of models applied outside of their datasets, however, is still relatively low. With such a high amount of data being produced daily in our technological world, semantic relation models could potentially be used to help increase the speed at which specific information can be identified and extracted from their collections of data. This project will look at the accuracy of trained semantic models outside of their training datasets to assess the applicability of them as a tool for a general information search.},
	pages = {5},
	author = {Sergeev, Aleksandr},
	date = {2020},
	langid = {english},
}

@article{kim_greg_2020,
	title = {{GREG}: A Global Level Relation Extraction with Knowledge Graph Embedding},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/3/1181},
	doi = {10.3390/app10031181},
	shorttitle = {{GREG}},
	abstract = {In an age overﬂowing with information, the task of converting unstructured data into structured data are a vital task of great need. Currently, most relation extraction modules are more focused on the extraction of local mention-level relations—usually from short volumes of text. However, in most cases, the most vital and important relations are those that are described in length and detail. In this research, we propose {GREG}: A Global level Relation Extractor model using knowledge graph embeddings for document-level inputs. The model uses vector representations of mention-level ‘local’ relation’s to construct knowledge graphs that can represent the input document. The knowledge graph is then used to predict global level relations from documents or large bodies of text. The proposed model is largely divided into two modules which are synchronized during their training. Thus, each of the model’s modules is designed to deal with local relations and global relations separately. This allows the model to avoid the problem of struggling against loss of information due to too much information crunched into smaller sized representations when attempting global level relation extraction. Through evaluation, we have shown that the proposed model yields high performances in both predicting global level relations and local level relations consistently.},
	pages = {1181},
	number = {3},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Kim, Kuekyeng and Hur, Yuna and Kim, Gyeongmin and Lim, Heuiseok},
	urldate = {2021-07-20},
	date = {2020-02-10},
	langid = {english},
}

@article{martinez-rodriguez_openie-based_2018,
	title = {{OpenIE}-based approach for Knowledge Graph construction from text},
	volume = {113},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417418304329},
	doi = {10.1016/j.eswa.2018.07.017},
	pages = {339--355},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Martinez-Rodriguez, Jose L. and Lopez-Arevalo, Ivan and Rios-Alvarado, Ana B.},
	urldate = {2021-07-20},
	date = {2018-12},
	langid = {english},
}

@online{noauthor_openie-based_nodate,
	title = {{OpenIE}-based approach for Knowledge Graph construction from text {\textbar} Elsevier Enhanced Reader},
	url = {https://reader.elsevier.com/reader/sd/pii/S0957417418304329?token=2EEAB1A078C9DD135C1E6513ED6CAEA37E228CEB29BAD3B64F5EC0DB23AC90577D2252D7EC5F6BAFA5CB0A5403498D81&originRegion=eu-west-1&originCreation=20210720114836},
	urldate = {2021-07-20},
	langid = {english},
	doi = {10.1016/j.eswa.2018.07.017},
}

@article{martinez-rodriguez_openie-based_2018-1,
	title = {{OpenIE}-based approach for Knowledge Graph construction from text},
	volume = {113},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418304329},
	doi = {10.1016/j.eswa.2018.07.017},
	abstract = {Transforming unstructured text into a formal representation is an important goal of the Semantic Web in order to facilitate the integration and retrieval of information. The construction of Knowledge Graphs ({KGs}) pursues such an idea, where named entities (real world things) and their relations are extracted from text. In recent years, many approaches for the construction of {KGs} have been proposed by exploiting Discourse Analysis, Semantic Frames, or Machine Learning algorithms with existing Semantic Web data. Although such approaches are useful for processing taxonomies and connecting beliefs, they provide several linguistic descriptions, which lead to semantic data heterogeneity and thus, complicating data consumption. Moreover, Open Information Extraction ({OpenIE}) approaches have been slightly explored for the construction of {KGs}, which provide binary relations representing atomic units of information that could simplify the querying and representation of data. In this paper, we propose an approach to generate {KGs} using binary relations produced by an {OpenIE} approach. For such purpose, we present strategies for favoring the extraction and linking of named entities with {KG} individuals, and additionally, their association with grammatical units that lead to producing more coherent facts. We also provide decisions for selecting the extracted information elements for creating potentially useful {RDF} triples for the {KG}. Our results demonstrate that the integration of information extraction units with grammatical structures provides a better understanding of proposition-based representations provided by {OpenIE} for supporting the construction of {KGs}.},
	pages = {339--355},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Martinez-Rodriguez, Jose L. and Lopez-Arevalo, Ivan and Rios-Alvarado, Ana B.},
	urldate = {2021-07-20},
	date = {2018-12-15},
	langid = {english},
	keywords = {Fact extraction, Knowledge Graph, {RDF} events, Relation Extraction, Semantic Web representation},
}

@article{li_understanding_2017,
	title = {Understanding Neural Networks through Representation Erasure},
	url = {http://arxiv.org/abs/1612.08220},
	abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple {NLP} tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
	journaltitle = {{arXiv}:1612.08220 [cs]},
	author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
	urldate = {2021-07-19},
	date = {2017-01-09},
	eprinttype = {arxiv},
	eprint = {1612.08220},
	keywords = {Computer Science - Computation and Language},
}

@article{vaswani_attention_nodate,
	title = {Attention is All you Need},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.0 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature.},
	pages = {11},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	langid = {english},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the {ImageNet} {LSVRC}-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient {GPU} implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the {ILSVRC}-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	pages = {84--90},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	urldate = {2021-07-13},
	date = {2017-05-24},
	langid = {english},
}

@article{mikolov_distributed_2013,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	url = {http://arxiv.org/abs/1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	journaltitle = {{arXiv}:1310.4546 [cs, stat]},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2021-07-13},
	date = {2013-10-16},
	eprinttype = {arxiv},
	eprint = {1310.4546},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sun_how_2020,
	title = {How to Fine-Tune {BERT} for Text Classification?},
	url = {http://arxiv.org/abs/1905.05583},
	abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, {BERT} (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of {BERT} on text classification task and provide a general solution for {BERT} fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
	journaltitle = {{arXiv}:1905.05583 [cs]},
	author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
	urldate = {2021-07-11},
	date = {2020-02-05},
	eprinttype = {arxiv},
	eprint = {1905.05583},
	keywords = {Computer Science - Computation and Language},
}

@article{minaee_deep_2021,
	title = {Deep Learning Based Text Classification: A Comprehensive Review},
	url = {http://arxiv.org/abs/2004.03705},
	shorttitle = {Deep Learning Based Text Classification},
	abstract = {Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.},
	journaltitle = {{arXiv}:2004.03705 [cs, stat]},
	author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
	urldate = {2021-07-11},
	date = {2021-01-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.03705},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
	journaltitle = {{arXiv}:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2021-07-11},
	date = {2020-07-22},
	eprinttype = {arxiv},
	eprint = {2005.14165},
	keywords = {Computer Science - Computation and Language},
}

@incollection{amershi_modeltracker_2015,
	location = {New York, {NY}, {USA}},
	title = {{ModelTracker}: Redesigning Performance Analysis Tools for Machine Learning},
	isbn = {978-1-4503-3145-6},
	url = {https://doi.org/10.1145/2702123.2702509},
	shorttitle = {{ModelTracker}},
	abstract = {Model building in machine learning is an iterative process. The performance analysis and debugging step typically involves a disruptive cognitive switch from model building to error analysis, discouraging an informed approach to model building. We present {ModelTracker}, an interactive visualization that subsumes information contained in numerous traditional summary statistics and graphs while displaying example-level performance and enabling direct error examination and debugging. Usage analysis from machine learning practitioners building real models with {ModelTracker} over six months shows {ModelTracker} is used often and throughout model building. A controlled experiment focusing on {ModelTracker}'s debugging capabilities shows participants prefer {ModelTracker} over traditional tools without a loss in model performance.},
	pages = {337--346},
	booktitle = {Proceedings of the 33rd Annual {ACM} Conference on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Amershi, Saleema and Chickering, Max and Drucker, Steven M. and Lee, Bongshin and Simard, Patrice and Suh, Jina},
	urldate = {2021-07-11},
	date = {2015-04-18},
	keywords = {debugging, interactive visualization, machine learning, performance analysis},
}

@article{bai_empirical_2018,
	title = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as {LSTMs} across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/{TCN} .},
	journaltitle = {{arXiv}:1803.01271 [cs]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	urldate = {2021-07-11},
	date = {2018-04-19},
	eprinttype = {arxiv},
	eprint = {1803.01271},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{koh_understanding_2020,
	title = {Understanding Black-box Predictions via Influence Functions},
	url = {http://arxiv.org/abs/1703.04730},
	abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
	journaltitle = {{arXiv}:1703.04730 [cs, stat]},
	author = {Koh, Pang Wei and Liang, Percy},
	urldate = {2021-07-09},
	date = {2020-12-29},
	eprinttype = {arxiv},
	eprint = {1703.04730},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ribeiro_why_2016,
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	url = {http://arxiv.org/abs/1602.04938},
	shorttitle = {"Why Should I Trust You?},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose {LIME}, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	journaltitle = {{arXiv}:1602.04938 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2021-07-09},
	date = {2016-08-09},
	eprinttype = {arxiv},
	eprint = {1602.04938},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{carvalho_machine_2019,
	title = {Machine Learning Interpretability: A Survey on Methods and Metrics},
	volume = {8},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2079-9292/8/8/832},
	doi = {10.3390/electronics8080832},
	shorttitle = {Machine Learning Interpretability},
	abstract = {Machine learning systems are becoming increasingly ubiquitous. These systems\&rsquo;s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
	pages = {832},
	number = {8},
	journaltitle = {Electronics},
	author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
	urldate = {2021-07-09},
	date = {2019-08},
	langid = {english},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {{XAI}, explainability, interpretability, machine learning},
}

@article{li_interpretable_2021,
	title = {Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond},
	url = {http://arxiv.org/abs/2103.10689},
	shorttitle = {Interpretable Deep Learning},
	abstract = {Deep neural networks have been well-known for their superb performance in handling various machine learning and artiﬁcial intelligence tasks. However, due to their over-parameterized black-box nature, it is often diﬃcult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal the ways that deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Speciﬁcally, we introduce and clarify two basic concepts—interpretations and interpretability—that people usually get confused. First of all, to address the research eﬀorts in interpretations, we elaborate the design of several recent interpretation algorithms, from diﬀerent perspectives, through proposing a new taxonomy. Then, to understand the results of interpretation, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the existing work in evaluating models’ interpretability using “trustworthy” interpretation algorithms. Finally, we review and discuss the connections between deep models’ interpretations and other factors, such as adversarial robustness and data augmentations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.},
	journaltitle = {{arXiv}:2103.10689 [cs]},
	author = {Li, Xuhong and Xiong, Haoyi and Li, Xingjian and Wu, Xuanyu and Zhang, Xiao and Liu, Ji and Bian, Jiang and Dou, Dejing},
	urldate = {2021-07-09},
	date = {2021-05-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2103.10689},
	keywords = {Computer Science - Machine Learning},
}

@article{zhai_scaling_2021,
	title = {Scaling Vision Transformers},
	url = {http://arxiv.org/abs/2106.04560},
	abstract = {Attention-based neural networks such as the Vision Transformer ({ViT}) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model’s scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale {ViT} models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we reﬁne the architecture and training of {ViT}, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a {ViT} model with two billion parameters, which attains a new state-of-the-art on {ImageNet} of 90.45\% top-1 accuracy. The model also performs well on few-shot learning, for example, reaching 84.86\% top-1 accuracy on {ImageNet} with only 10 examples per class.},
	journaltitle = {{arXiv}:2106.04560 [cs]},
	author = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
	urldate = {2021-07-09},
	date = {2021-06-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2106.04560},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	journaltitle = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2021-07-04},
	date = {2019-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@article{losch_interpretability_2019,
	title = {Interpretability Beyond Classification Output: Semantic Bottleneck Networks},
	url = {http://arxiv.org/abs/1907.10882},
	shorttitle = {Interpretability Beyond Classification Output},
	abstract = {Today’s deep learning systems deliver high performance based on end-to-end training. While they deliver strong performance, these systems are hard to interpret. To address this issue, we propose Semantic Bottleneck Networks ({SBN}): deep networks with semantically interpretable intermediate layers that all downstream results are based on. As a consequence, the analysis on what the ﬁnal prediction is based on is transparent to the engineer and failure cases and modes can be analyzed and avoided by high-level reasoning. We present a case study on street scene segmentation to demonstrate the feasibility and power of {SBN}. In particular, we start from a well performing classic deep network which we adapt to house a {SB}-Layer containing task related semantic concepts (such as object-parts and materials). Importantly, we can recover state of the art performance despite a drastic dimensionality reduction from 1000s (non-semantic feature) to 10s (semantic concept) channels. Additionally we show how the activations of the {SB}-Layer can be used for both the interpretation of failure cases of the network as well as for conﬁdence prediction of the resulting output. For the ﬁrst time, e.g., we show interpretable segmentation results for most predictions at over 99\% accuracy.},
	journaltitle = {{arXiv}:1907.10882 [cs]},
	author = {Losch, Max and Fritz, Mario and Schiele, Bernt},
	urldate = {2021-07-04},
	date = {2019-07-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.10882},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wu_visual_2020,
	title = {Visual Transformers: Token-based Image Representation and Processing for Computer Vision},
	url = {http://arxiv.org/abs/2006.03677},
	shorttitle = {Visual Transformers},
	abstract = {Computer vision has achieved remarkable success by (a) representing images as uniformly-arranged pixel arrays and (b) convolving highly-localized features. However, convolutions treat all image pixels equally regardless of importance; explicitly model all concepts across all images, regardless of content; and struggle to relate spatially-distant concepts. In this work, we challenge this paradigm by (a) representing images as semantic visual tokens and (b) running transformers to densely model token relationships. Critically, our Visual Transformer operates in a semantic token space, judiciously attending to different image parts based on context. This is in sharp contrast to pixel-space transformers that require orders-of-magnitude more compute. Using an advanced training recipe, our {VTs} significantly outperform their convolutional counterparts, raising {ResNet} accuracy on {ImageNet} top-1 by 4.6 to 7 points while using fewer {FLOPs} and parameters. For semantic segmentation on {LIP} and {COCO}-stuff, {VT}-based feature pyramid networks ({FPN}) achieve 0.35 points higher {mIoU} while reducing the {FPN} module's {FLOPs} by 6.5x.},
	journaltitle = {{arXiv}:2006.03677 [cs, eess]},
	author = {Wu, Bichen and Xu, Chenfeng and Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Yan, Zhicheng and Tomizuka, Masayoshi and Gonzalez, Joseph and Keutzer, Kurt and Vajda, Peter},
	urldate = {2021-05-12},
	date = {2020-11-19},
	eprinttype = {arxiv},
	eprint = {2006.03677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{huang_interpretable_2020,
	title = {Interpretable and Accurate Fine-grained Recognition via Region Grouping},
	url = {http://arxiv.org/abs/2005.10411},
	abstract = {We present an interpretable deep model for fine-grained visual recognition. At the core of our method lies the integration of region-based part discovery and attribution within a deep neural network. Our model is trained using image-level object labels, and provides an interpretation of its results via the segmentation of object parts and the identification of their contributions towards classification. To facilitate the learning of object parts without direct supervision, we explore a simple prior of the occurrence of object parts. We demonstrate that this prior, when combined with our region-based part discovery and attribution, leads to an interpretable model that remains highly accurate. Our model is evaluated on major fine-grained recognition datasets, including {CUB}-200, {CelebA} and {iNaturalist}. Our results compare favorably to state-of-the-art methods on classification tasks, and our method outperforms previous approaches on the localization of object parts.},
	journaltitle = {{arXiv}:2005.10411 [cs]},
	author = {Huang, Zixuan and Li, Yin},
	urldate = {2021-05-12},
	date = {2020-05-20},
	eprinttype = {arxiv},
	eprint = {2005.10411},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{radford_learning_2021,
	title = {Learning Transferable Visual Models From Natural Language Supervision},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn {SOTA} image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as {OCR}, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original {ResNet}-50 on {ImageNet} zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/{OpenAI}/{CLIP}.},
	journaltitle = {{arXiv}:2103.00020 [cs]},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	urldate = {2021-05-12},
	date = {2021-02-26},
	eprinttype = {arxiv},
	eprint = {2103.00020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@incollection{ishikawa_contextual_2021,
	location = {Cham},
	title = {Contextual Semantic Interpretability},
	volume = {12625},
	isbn = {978-3-030-69537-8 978-3-030-69538-5},
	url = {http://link.springer.com/10.1007/978-3-030-69538-5_22},
	abstract = {Convolutional neural networks ({CNN}) are known to learn an image representation that captures concepts relevant to the task, but do so in an implicit way that hampers model interpretability. However, one could argue that such a representation is hidden in the neurons and can be made explicit by teaching the model to recognize semantically interpretable attributes that are present in the scene. We call such an intermediate layer a semantic bottleneck. Once the attributes are learned, they can be re-combined to reach the ﬁnal decision and provide both an accurate prediction and an explicit reasoning behind the {CNN} decision. In this paper, we look into semantic bottlenecks that capture context: we want attributes to be in groups of a few meaningful elements and participate jointly to the ﬁnal decision. We use a two-layer semantic bottleneck that gathers attributes into interpretable, sparse groups, allowing them contribute diﬀerently to the ﬁnal output depending on the context. We test our contextual semantic interpretable bottleneck ({CSIB}) on the task of landscape scenicness estimation and train the semantic interpretable bottleneck using an auxiliary database ({SUN} Attributes). Our model yields in predictions as accurate as a non-interpretable baseline when applied to a real-world test set of Flickr images, all while providing clear and interpretable explanations for each prediction.},
	pages = {351--368},
	booktitle = {Computer Vision – {ACCV} 2020},
	publisher = {Springer International Publishing},
	author = {Marcos, Diego and Fong, Ruth and Lobry, Sylvain and Flamary, Rémi and Courty, Nicolas and Tuia, Devis},
	editor = {Ishikawa, Hiroshi and Liu, Cheng-Lin and Pajdla, Tomas and Shi, Jianbo},
	urldate = {2021-05-12},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-69538-5_22},
	note = {Series Title: Lecture Notes in Computer Science},
}