\documentclass{article}
\usepackage{inputenc}
\emergencystretch=2em
\usepackage[breaklinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=magenta,
    citecolor=black}
    
\usepackage[english]{babel}
\usepackage{biblatex}
\usepackage{svg}
\usepackage{graphicx}
% For line numbers
\usepackage[left]{lineno}


%%%% For the scheme
%\usepackage{tikz}
%\usetikzlibrary{matrix,shapes}
%\setlength{\parindent}{0pt}
%\usepackage{pgfgantt}
%\usepackage{lscape}

%%% For the thesis rings %%%
\renewcommand{\baselinestretch}{2.0}
\linenumbers


\addbibresource{references.bib}
\title{A Step Towards Explainable AI: Infer Species Based on Descriptions in Natural Language} 

\author{Robert Vlasakker, van de}
\date{August 2021}

\begin{document}
\graphicspath{ {./figures/} }

\maketitle
{\Large \textbf{FOR THE READER}\par}
\noindent
\textbf{Proposal, preliminary.}
Hi, please read the introduction and if you up to it the object/research questions.
Please focus on the flow of the text and also if the objectives and questions are clear for you. Do the questions make sense to you when reading the introduction?
\noindent
You can check the rest of course, but the proposal is still a work in progress
\noindent
Thanks!

\newpage

\section{Introduction}
Deep learning allows for incredible applications from the automatic classification of text and images, natural language processing to reinforcement learning.
In many cases, especially in computer vision, deep learning models have already surpassed human experts \cite{he_delving_2015}.
In natural language processing (NLP), auto-regressive models like GPT-3 (Generative Pre-trained Transformer 3) can produce text that barely can be distinguished from a text produced by real humans \cite{brown_language_2020}.
Because of their success, deep learning models are already found quite often in consumer-grade electronics, like smartphones and used in a lot of different applications like the \href{https://www.inaturalist.org/}{iNaturalist} application.
%With the \href{https://www.inaturalist.org/}{iNaturalist} model, species can automatically be recognized just by taking a picture of species \cite{radford_learning_2021}.
But deep learning models also make their way into business services and solutions.
Microsoft acquired an exclusive licensing on GPT-3 on 22 September 2020 and will integrate the model into their products and services.
While these results and their applications of the deep learning models are incredible, the reasoning behind the results remains a mystery \cite{li_interpretable_2021, losch_interpretability_2019}.

Unlike classic machine learning models, deep learning models can automatically extract features needed for detection or classification.
Domain knowledge, in combination with careful engineering to extract the necessary features for the detection and/or classification, is no longer needed \cite{lecun_deep_2015}.
While this automatic feature extraction is very convenient, it reduces the interpretability of the models.
To extract the features of the input data, deep learning models use multiple simple neurons that take the input, process it to a slightly more abstract representation and pass it through the next neuron.
Provided enough neurons are 'stacked' upon each other, very complex features can be extracted and correctly detected and/or classified by such a network.
At each passing, the data becomes a bit more abstract and less interpretable.
Stacking multiple neurons on top of each often results in millions of parameters in most recent deep learning models (or even billions in the case of GPT-3).
All of these neurons use non-linear activation functions that increase the overall complexity of the network.
The parameters can be seen as the coefficients of the model, and are initially chosen by the designer or randomly initialized by the model and get updated by the model as it 'learns'\footnote{The learning method varies across different deep learning models, but is based on derivatives.} from the data.
After training, the parameters of the model can no longer be  easily interpreted.
This behaviour is often referred to as a 'back box', which means that the reasoning behind the result is tough to understand or is lacking at all.
With sensitive fields, like health care, financial classification or autonomous driving, this black box behaviour could raise issues as it hampers the trustworthiness of the models \cite{carvalho_machine_2019}.
The increasing complexity already resulted in several regulations, where the European General Data Protection Regulation (GDPR) is probably the most famous one.
It is essential to keep the models comprehensible for humans as their decision will impact human lives.
Also, when the reasoning behind deep learning behaviour is better understood, the insights could improve these models \cite{amershi_modeltracker_2015} and deep learning models can be expanded to more fields \cite{lei_opening_2018}.
Different algorithms and techniques have been proposed to increase the interpretation of the models, like feature reduction algorithms \cite{ribeiro_why_2016}, inference of training sample contribution \cite{koh_understanding_2020}, by changing jittering test samples and see how the prediction changes \cite{li_understanding_2017} and decomposition and partial derivatives techniques \cite{samek_explainable_2017}.
%One of the most popular techniques for making deep learning models easier to interpret are decomposition and partial derivatives techniques \cite{samek_explainable_2017}.
%Both techniques use heat maps.
%The heat map of a decomposed model will show how much each pixel contributes to the model's prediction.
%The heat map of a model with partial derivatives will show how much the changes in each pixel affect the model's prediction.
Van Lent et al. already coined the term explainable artificial intelligence in 2004 to explain the behaviour of AI in game applications \cite{van_lent_explainable_2004}.
While some advances have been made in model understanding, large leaps forward in the field of explainable AI remain limited \cite{lipton_mythos_2017, li_interpretable_2021}.

In the taxonomy, new species are now described by experts in the field.
Estimated is that 50\% of the species are yet to be discovered, and many species will go extinct before every described.
Scalable technologies that can help monitor diversity and help discover new species are more needed than ever.
Deep learning models can help discover new species, automate and speed up this process.
It is, however, important to better understand the reasoning of a deep learning model in such a sensitive field \cite{carvalho_machine_2019}.
Why are certain species labelled as new and others are not?
Explainable AI is vital in the field of taxonomy; it is very important to 'teach' deep learning models how taxonomy experts describe new species and, in the meantime, keep the model output interpretable for humans.
This way reasoning of the model can be tracked, evaluated and improved.
A regular classification model will predict a species based on what it sees on an image and then match this prediction against a known database.
While the recent deep learning model gives amazing results with images with a known label, it is vital to see the model's reasoning when classifying a new species.
Based on what parts of the image (e.g. colour, pattern, parts) does the model classify species on an image as undiscovered?
The interpretability problem might be solved by splitting a basic vision model into a visual-language hybrid model and an NLP model and creating a more explainable AI.
The first model will take an image as input, and instead of directly predicting a species, its output will be data in natural language about what it 'sees' using the language of taxonomists (e.g., "3-petalled with a pink colour.", "The bill is black with blue spots.").
The second model will be a pure NLP model.
This model will take the output of the first model without any information about the species and tells to which species it thinks the predictions belong.
The intermediate results between the two models will natural language.
This will keep the intermediate results easily interpretable for humans.
For both models to work, a large database with species and their descriptions is needed. 
Unfortunately such a database does not exists yet and needs to be created for this research to happen.
This dataset needs as many unique descriptions as possible for each species, so for a taxonomist and an NLP model it would be possible to infer the species name based on provided descriptions.
The next step is to build, train and validate the NLP model.
This pure NLP model will be trained independently from visual-language hybrid model. 
The NLP model needs to make predictions, but in the meantime it should be clear which parts of the description data is used to make to prediction.
The visual-language hybrid model, and the combination between the two models to infer not yet discovered species will be part of another research.



\section{Objective}
The first objective of this research is to create a large database with species names and their corresponding descriptions. 
If the description is unavailable on species level, the descriptions will be stored per genus or even per family. 
The descriptions are as much as possible stored per attribute (e.g. 'Purple plumage.'), instead of complete text spans.
This could either be done by storing the descriptions as much as possible per sentence or as a semantic triple (object, predicate, object).
\noindent 
\begin{itemize}
    \item \emph{How can a high quality database be created that contains species names and contains a combination of semantically unique descriptions per species?}
\end{itemize}
The second objective will be creating a NLP model that can infer species names based on description data. 
The data that will be fed to the model will consist of description data from the database created in the first objective. 
Based on the first results this will either be descriptions sentences or semantic triples.
\noindent 
\begin{itemize}
    \item \emph{How should a deep learning model be built, trained and evaluated to predict existing species from the used database?}
\end{itemize}
\noindent  
It should be tractable which description data points are important for inferring a certain species (e.g. 'black bill', 'fur') by the pure NLP model.

\noindent
\begin{itemize}
    \item \emph{How can the natural language model be interpreted to make it clear which focus points are used for the prediction?}
\end{itemize}


\section{Approach}
\subsection{Creation of the Dataset}
The World Wide Web has potentially an endless amount of species descriptions available.
However, this data is not structured and is certainly not in one place on the internet.
A web crawler that can automatically query species description pages from search engines and then search those queried pages for description sentences can automatically fill a large database with species names and descriptions. 
Description sentences can be theoretically limitless, e.g. for a Brown Bear description sentences could be: 'The fur is brown.', 'The brown bear has brown fur.'.
Sentences can also have a more difficult semantic, e.g. 'The fur of the Brown bear is similar to that of the Grizzly bear'.
It will not be feasible to use a classic machine learning approach that requires a rule-based match system for the sentence classification. 
A deep learning model that can automatically extract the features and classify the text is needed.
However, to properly train a deep learning model that can classify text data, a large, accurate and consistent labeled dataset is needed \cite{munappy_data_2019}.
For the creation of this dataset, several structured web sources like \href{http://www.Wikipedia.com}{Wikipedia}, \href{https://birdsoftheworld.org}{Birds of the World} and \href{http://powo.science.kew.org/}{Plants of the World Online} can be used.
These websites do not actually contain labels, but the paragraph do contain paragraph titles like 'Habitat', 'Characteristics'. 
These titles can be used as labels to label the scraped text (1/True in case of 'Characteristics' and 0/False in case of 'Habitat').
To compensate for text parts that are not really description, but a are labelled as such, a soft loss of \cite{reed_training_2015} will be implemented: \( Softloss(q, t) =  \sum_{L}^{k=1} [\beta t _k + (1- \beta )q _k]log(q _k) \).
In the case of the Wikipedia, additional random page could also be used to increase the number of negatives for the database.
This would also to help the model recognizing non-description text better.
If enough data is gathered from structured sources a deep learning model for text classification can be trained.
This model will aim to assign labels to sentences, paragraphs or even complete documents. 
Using a pretrained model with word embeddings can help models achieve much better results, than models trained from scratch \cite{mikolov_distributed_2013}.
Using a pretrained model is called transfer learning and could speed up the process of training and increase the accuracy of the deep learning model.
BERT (Bidirectional Encoder Representations from Transformers) \cite{devlin_bert_2019} is such a pretrained language model and by using transfer learning can be used for this task; BERT is already trained on a large corpus of English words.
As text classification with two different outputs is a relatively simple task, a smaller and faster version of BERT, called distillBERT will be used \cite{sanh_distilbert_2020}.
\cite{sun_how_2020} already investigated what the best way is to fine tune BERT for text classification. 
Their results will be used to build a model in PyTorch \cite{paszke_pytorch_2019}.
In their best runs they used one dropout layer (0.1) and two linear layers. 
Their dropout layer and first linear layer (768\footnote{The output from the last hidden layer of BERT is a tensor of 768.}, 512) will be kept the same.
The last linear layer will have its output changed to two as there are two different classes.
The final activation function will a log softmax function\footnote{In certain situation the log softmax is proven to be numerically more stable than softmax, by taken the exponent of the value it can be converted to normal prediction values.}
If the results from the BERT descriptions classification model proven to be sufficient it can be used in the web crawler.
Websites can be queried using different search engines like \href{www.google.com}{Google} and \href{www.bing.com}{Microsoft Bing}.
Queries could be constructed by using the species name plus e.g. 'description' or 'diagnosis'. 
It has yet to be seen which species/query is the best combination and yield the most relevant results.
if the a right species/query combination has been found, it can be used to iterate over the species names and return relevant web pages from the search engines.
The text from the returned pages can be used to see if sentences are descriptions data and if yes, it would stored under the queried species as a description part of that species.
Before using the trained model, the retrieved text from the web page needs to be cleaned.
A web page very rarely contains only description information and when retrieving a web page a lot of metadata is also returned like headers, content page, etc.
This irrelevant text data needs to be filtered out. 
It is also highly unlikely that a prediction paragraph of a web page contains pure description data.
Therefore the web pages will be broken down into single sentences.
Per sentences the model will predict if it will be classified as a description or something else.
Some websites will use text information from other website and maybe alter the text slightly. 
In this case both text spans will be detected by the model. 
By using the last hidden state of the model and creating a cosine matrix similarity for all the data per species, the double sentences can be dropped, even if there are some different token/punctuation.
The remaining sentences will be stored in the database.

%\begin{figure} [t]
%    \centering
%    \vspace{-2.0cm}
%    \makebox[\textwidth][c]{\includesvg[inkscapelatex=false, width=1.6\textwidth]{PoS_example.svg}}
%    \caption{An example of part of speech and dependency parsing for the sentence: "The Brown Bear has large claws, and its fur is brown.". The arrows contain the dependency tags and the words contain the part of speech tags.}
%    \label{fig:PoS_example}
%\end{figure}

%As the labels are based on the paragraphs titles, all the text inside a paragraph would get the same label. 
\subsection{Infer Species on Partial Descriptions}
The easiest approach is to use the BERT model that is used for description classification.
The last linear layer should be changed to one with an output of the species used for training (e.g. 20,000).
However, with so much classes the softmax activation function in combination with a cross entropy loss function might not be suitable.
The model will be very expensive to train which such a large amount of output classes; It start with random guesses out of the 20,000 labels and needs to 'learn' the correct classes based on the loss that is returned.
Another downside of the softmax activation is that is does not try to keep different labels as far apart as possible.
With a fixed amount of classes this is not a problem.
However the final goal is to predict existing and new species.
As the softmax layer does not keep track of distances, every new species will be grouped together.
A better approach for a large number of output classes might deep metric learning.
The goal of deep metric learning to minimize the distance if it is looking at the same label and maximize the distance if it is looking at two different labels.
As deep metric learning, like ordinary deep learning models, uses activation the non-linearity can be captured.
Nowadays a popular deep learning model for metric learning is a so called 'Siamese network' \cite{kaya_deep_2019}, and this model will also be used in this research.
It involves two identical networks that are combined into one single loss output.
The output will produce a single distance cost function between two input labels.
For the actual loss function there are two popular function that could be used, the triplet loss or the contrastive loss.
In this research this triplet loss \cite{schroff_facenet_2015} will be used.
Contrastive loss will only update the parameter weights to minimize the distance in case of a similar label or maximize in case of a dissimilar label.
Triplet loss uses an extra anchor label, and tries to push a similar label towards the anchor, but also pushed a dissimilar label away from the item.
This will result in a large distance for different labels.
% Check this with Diego!
% Loss below zero?
For the deep learning network bases, the same basis BERT network as for the web crawler will be used.
As the last hidden state of BERT already contains a matrix, this last hidden state could be used as features for the loss function to compute the distance between two matrices.
It has to be seen which model will perform better (accuracy/timewise), the metric model or the model that uses a soft max activation output layer.

%\newpage
%\begin{landscape}
\section{Feasibility}
In Figure \ref{fig:time_schedule} the schedule can be found. 
As the workload of this thesis is relatively heavy, the start is at 15 July 2021.
Another research is also dependent on some data that will be created in this research.
By bringing the starting date forward, the other research can use the data earlier.
\subsection{Web Crawler}
The first step is to create a web crawler pipeline. 
For this a lot of data needs to be scraped from structured sources.
This is not a computationally expensive task and can be done a personal computer running Python with some basic packages like BeautifulSoup and Selenium.
As the WUR server is not available for student at the time of writing (10 september 2020), the deep learning model for description recognition will be trained on \href{https://colab.research.google.com/}{Google Colab Pro}. 
By storing and reloading the model at each instance, the model can be trained for multiple epochs.
As the dataset will be initialized at every instance the same seed will be set to make sure the training, validating and testing date stays the same.
If the model is trained enough and the results are sufficient, the model will be deployed on a personal laptop to crawl trough the web pages and store sentences that contain description data.
A possible risk is that trained model, will not generalize well and will fail to recognize description sentences.
In this case new data needs to be scraped manually to make sure the model will generalize better.
\subsection{Natural Language Processing Model}
The database part needs to be finished before the NLP model can be trained.
However, as also indicated, pipeline that process and load the data can already be built with some preliminary results from the web crawler.
Both models (metric and normal softmax) will be trained on the WUR server is this is possible. 
Otherwise both models will be trained on Google Colab Pro.


\begin{figure} [t]
    \centering
    \vspace{-2.5cm}
    \hspace{-1.3cm}
    \makebox[\textwidth][c]{\includesvg[inkscapelatex=false, width=1.6\textwidth]{schedule.svg}}
    \caption{Time Schedule}
    \label{fig:time_schedule}
\end{figure}
%\end{landscape}
%\newpage

\newpage
\printbibliography
\end{document}


bockk