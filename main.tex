%\documentclass{article}
\documentclass[a4paper, 12pt, oneside]{book} % Uses the book template for quick start, but scrbook may be better because it is more flexible
\usepackage{inputenc}
\emergencystretch=2em
\usepackage[breaklinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=magenta,
    citecolor=blue!40!black}
\usepackage[english]{babel}
%\usepackage{biblatex}
\usepackage[backend=biber, style=apa]{biblatex}
\setlength\bibitemsep{\baselineskip}
\usepackage{svg}
\usepackage{graphicx}
\usepackage{graphbox}   % allows to add keys to \includegraphics
\usepackage[labelfont=bf]{caption}
\usepackage{booktabs}
%\usepackage[table,xcdraw]{xcolor}
% TOC DEPTH
\setcounter{tocdepth}{3}
% PARAGRAPH DEPTH
\setcounter{secnumdepth}{4}
% Fat caption numbers
\usepackage{caption}
% Footnote indentation
\usepackage[hang,flushmargin]{footmisc} 
\usepackage{subcaption}
\usepackage[left]{lineno} % For line numbers
%\renewcommand{\baselinestretch}{2.0}
%\linenumbers
\usepackage[T1]{fontenc} % Include italic fonts
\usepackage{geometry} % Page margins
\usepackage{titling} % Title page container
\usepackage{wrapfig} % Picture container
\usepackage{titlesec} % Remove chapter header
\renewcommand{\familydefault}{\sfdefault} % Set default f
\usepackage{comment}

% COLORS
\usepackage{xcolor} 
\usepackage{soul}
\usepackage{adjustbox}
\definecolor{color1}{HTML}{4fb688}
\definecolor{color2}{HTML}{005622}
\definecolor{color3}{HTML}{f4fbfc}
\definecolor{color4}{HTML}{70c6ac}
\definecolor{color5}{HTML}{f7fcfd}
\definecolor{color6}{HTML}{2b9452}
\definecolor{color7}{HTML}{61bf9e}
\definecolor{color8}{HTML}{e3f4f7}
\definecolor{color9}{HTML}{005020}
\definecolor{color10}{HTML}{00441b}



\addbibresource{references.bib}
% Note: Fill these in with correct data, they are used throughout the document
\title{A Step Towards Explainable AI: Infer Species Names Based on Partial Descriptions in Natural Language}
\author{Robert van de Vlasakker}
% This can be left as-is to automatically update
\date{\today}
\geometry{top=3cm, 
            bottom=3cm,
            inner=3.5cm,
            outer=3.5cm,
            foot=0cm,
            includeheadfoot} % First page margins
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Large}
  
\pagenumbering{roman}
\begin{document}
 \begin{titlingpage}
\newgeometry{top=1.25cm,bottom=0.96cm,inner=1.91cm,outer=1.91cm,foot=0cm}
{\Large Geo-information Science and Remote Sensing}\vspace{0.9cm}
  
  {\Large Thesis Report GIRS-2020-xx}\vspace{0.9cm}
  
  \hrule\vspace{1.1cm}
  
  {\bfseries \centering \Large \thetitle}
  
  {\bfseries \itshape (Subtitle)}\vspace{2.0cm}
  
  % Note: Remove the ``draft'' from \includegraphics to actually include an image, and adjust the vspace (padding) if you want to use a differently sized image
  %\begin{wrapfigure}{r}{0.55\textwidth}
  %  \vspace{1cm}
    %\includegraphics[height=9.5cm]{figures/picture.png}
    %\includesvg[inkscapelatex=false, height=9.5cm]{figures/wordcloud.svg}
  %\end{wrapfigure}
  
  \includegraphics[%
    width=0.83\textwidth,
    %height=0.83\textwidth,
    align=t,
    smash=br,
    vshift=2.5cm,     % adjust the vertical position
    hshift=2.5cm     % adjust the horizontal position
    ]{figures/frontcover.pdf}%
  
  {\Large \theauthor}\vspace{6.0cm}
  
  \rotatebox{90}{\Large \thedate}\vspace{1.5cm}
  
  % This is the vector version of the new WUR logo; due to layout changes it visually looks larger than the old one
  \sloppypar{\hspace{-2cm}\includegraphics[width=13cm]{figures/WUR_RGB_standard.eps}}
  
  % Note: this image is not stretched out like it is in the MS Word template
  \sloppypar{\noindent\makebox[\textwidth]{\hspace*{\dimexpr\evensidemargin-\oddsidemargin}\includegraphics[width=\paperwidth]{figures/image2.jpeg}}}
  
  %%%%%% PAGE II 
  \restoregeometry
  %\newgeometry{top=1.25cm,bottom=1.25cm,inner=0.66cm,outer=0.53cm,foot=1.19cm,includeheadfoot} % Subsequent page margins
  % Note: this uses the MS Word template margins, you might want to increase them a bit for printing (e.g. inner=1.91cm,outer=1.91cm)
  \thispagestyle{empty}
  
  \begin{center}
  {\bfseries \Large \thetitle}
  \newline
  \newline
  \newline
  \newline
  %{\bfseries \itshape Subtitle}\vspace{2.7cm}
  
  {\Large \theauthor}\vspace{0.25cm}
  
  {Registration number 920523897020}\vspace{2.5cm}
  
  {\large \underline{Supervisors}:}\vspace{.25cm}
  
  {Diego Marcos}
  
  {Ioannis Athanasiadis}\vspace{3.0cm}
  
  %{A thesis submitted in partial fulfilment of the degree of Master of Science}
  
  %{at Wageningen University and Research Centre,}
  
  %{The Netherlands.}\vspace{2.7cm}
  \end{center}
  
  \begin{center}
    {\thedate}
  
    {Wageningen, The Netherlands}
  \end{center}\vspace{5cm}

    Thesis code number: GRS-80400
  
    Thesis Report: Proposal
  
    {Wageningen University and Research Centre}
  
    {Laboratory of Geo-Information Science and Remote Sensing}
 \end{titlingpage}
\graphicspath{ {./figures/} }

\newpage
\thispagestyle{empty}
\textbf{Foreword}
\newpage
\thispagestyle{empty}
\textbf{Summary}
\newpage

\thispagestyle{empty}
\tableofcontents
\thispagestyle{empty}
\listoffigures
\thispagestyle{empty}
\listoftables
\thispagestyle{empty}
\newpage



\renewcommand{\thesection}{\arabic{section}}
\pagenumbering{arabic}
\section{Introduction}
\markboth{Introduction}{Introduction}

Estimated is that 50\% of the species are yet to be discovered, and many species will go extinct before ever being described \autocite{lees_species_2015}.
Scalable technologies that can help monitor diversity and help discover new species are more needed than ever.
Deep neural networks (DNNs) can help discover new species, automate and speed up this process \autocite{van_horn_inaturalist_2018}.
However, DNNs are quite rigid, and their black-box behaviour could raise issues as it hampers the trustworthiness of the models \autocite{carvalho_machine_2019}.
It is essential to get more insights into the reasoning of a deep learning model in sensitive fields like taxonomy.
This way, we can learn a DNN to reason like an experienced taxonomist when describing existing and new species.

Many studies have already stipulated the importance of biodiversity for human life \autocite{pimentel_economic_1997, gowdy_value_1997, raffaelli_links_2010, joppa_biodiversity_2011, pimm_how_2018}.
With the current extinction rate, over 50\% of the species will be gone before ever discovered \autocite{lees_species_2015}.
Protecting biodiversity is now more needed than ever.
However, we cannot conserve undiscovered species; we first need to describe them to protect them \autocite{joppa_biodiversity_2011}.
This is where DNNs come in place.
DNNs can help discover new species and speed up the identification process \autocite{van_horn_inaturalist_2018}.
However, many species look alike.
They are difficult to distinguish and are easily miss-classified, especially with less abundant species.
When a model now missclassifies a species it is now difficult to check and correct the reasoning of that model.
We need a better understanding of how a DNN identifies and classifies species. 
This way, a less rigid DNN can be created, results of the DNN can be interpreted more easily, and the results can be improved.

Deep neural networks (DNNs) allow for remarkable performance in applications: from the automatic classification of text and images, natural language processing (NLP), to reinforcement learning.
DNNs outperform most classic machine learning approaches \autocite{he_delving_2015, brown_language_2020}.
The key to their success is end-to-end training.
Unlike classic machine learning models, deep learning models can automatically extract features needed for detection or classification.
Domain knowledge, in combination with careful engineering to extract the necessary features for the detection or classification, is no longer needed \autocite{lecun_deep_2015}.
However, end-to-end training also results in DNNs that are rigid, difficult to interpret and explain.
To extract the features from the input data, deep learning models use multiple neurons that take the input, process it to a slightly more abstract representation and pass it through the next layer of neurons \autocite{schmidhuber_deep_2015}.
Provided enough layers are stacked upon each other, very complex features can be extracted and correctly detected or classified by such a network.

Because the network's parameters are updated based on its input data, the reasoning of DNNs remains challenging to understand \autocite{li_interpretable_2021, losch_interpretability_2019}, and they do not perform well on long-tailed datasets\footnote{Long-tailed datasets are skewed datasets. A lot of samples are available for a few species and most species only are represented in the dataset only by a few species.}, like most real-world datasets \autocite{van_horn_inaturalist_2018}.
Stacking multiple layers of neurons on top of each other often results in millions of parameters, and all of these neurons use non-linear activation functions that decrease the interpretability of the network.
In long-tailed datasets, the parameters are not well optimised for less represented classes as the neurons cannot extract the necessary features.
While this automatic feature extraction is very convenient, it will become difficult to track models' reasons, and it can hamper performance.

Different algorithms and techniques have been proposed to increase the interpretability of the models.
Common approaches are feature reduction algorithms \autocite{ribeiro_why_2016}, inference of training sample contribution \autocite{koh_understanding_2020}, adding jittering to test samples and see how the prediction changes \autocite{li_understanding_2017} and decomposition and partial derivatives techniques \autocite{samek_explainable_2017}.
These algorithms and techniques all rely on posthoc explanations; they try to interpret an already trained DNN and explain its decisions a posteriori.
They try to identify important features via attributions \autocite{zintgraf_visualizing_2017, selvaraju_grad-cam_2017} or assign meaning to features \autocite{fleet_visualizing_2014}.
While some advances have been made in model understanding, giant leaps forward in the field of explainable AI remain limited \autocite{lipton_mythos_2017, li_interpretable_2021}.
These approaches might explain some of the inner workings of a DNN, but they do not help reason a DNN like a taxonomist.
The models remain rigid, the intermediate results still remain challenging to interpret.
Designing models that are inherently interpretable by design are better solution as these model do not give a false sense of trustworthiness \autocite{rudin_stop_2019}.

An a priori approach entails designing an architecture network with a semantic bottleneck layer that is interpretable for humans \autocite{bucher_semantic_2019}. 
The bottleneck layer is usually an intermediate layer that is fitted between a regular deep learning architecture \autocite{bucher_semantic_2019}.
The DNN is still trained end-to-end, but humans can inspect this semantic bottleneck layer to check the intermediate results.
In the case of a visual semantic bottleneck layer things like colors, shape and texture are usually chosen as these can be semantically expressed and they are overall important for classification or detection \autocite{yosinski_understanding_2015, fleet_visualizing_2014}.
Although all downstream results are based on this layer (as all data has to flow trough this layer) it does not hamper the results \autocite{bucher_semantic_2019}.
The semantic bottleneck layer allows deep learning developers to adjust the architecture more easily in case the model does not perform as expected.

\begin{figure} [tbp]
    \centering
    \vspace{0cm}
    \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{architecture.pdf}}
    \caption[Proposed architecture]{The proposed architecture by \textcite{ishikawa_contextual_2021} for species classification. The model input is an image. The first model will describe the attributes present in the image in natural language. The second model will take the output of the first model and will make a prediction. The intermediate results remain interpretable by using natural language.}
    \label{fig:intro}
\end{figure}

\textcite{ishikawa_contextual_2021} extended this bottleneck layer architecture by training a model that first learns the intermediate results.
The intermediate results are in this case similar to the semantic bottleneck.
The model is first trained to predict results in a for humans interpretable semantic way.
A second model will take these intermediate results to make a final prediction.
A more explainable, less rigid, model for species classification might be created by extending the concept of the semantic bottleneck layer from \textcite{ishikawa_contextual_2021} and splitting a regular convolution neural network (CNN) for image classification into two separate models that communicate using natural language.
The first model will describe species features and traits present in the image of a species, and the second model will take these descriptions and tries to infer the species (see Figure \ref{fig:intro}).
This way, DNNs reasoning for species classification can be tracked, mistakes can be spotted, and the models' performance can be improved.

The first model will be a visual-language model that extracts the species attributes from an image and describes its results in natural language. 
This vision hybrid model will be based on the researches of \textcite{radford_learning_2021} and \textcite{huang_interpretable_2020}.
Their findings allow the first model to learn to extract information from images by looking at raw text data that comes with image captions and describe objects present in those images.
This visual-language hybrid model will describe learned species attributes and use zero-shot learning to describe unseen (new) distinctive species traits.
By first describing traits, less abundant species can benefit from more abundant species if they share common traits.
This allows to model to perform better on long-tailed species datasets \autocite{van_horn_inaturalist_2018}.
Each species present in the dataset will be described equally well, because the DNN can learn traits from more abundant species.
The second model is a pure natural language processing (NLP) model that takes the partial descriptions and infer the species.
This way, a models reasoning for species predictions might be tracked by investigating the intermediate results \autocite{ishikawa_contextual_2021} and the final model results will be less rigid.

By first training the NLP model, we can create a model that can infer species based on partial textual descriptions of the species. 
As DNNs need we large datasets to properly learn the necessary features from the data \autocite{xue-wen_chen_big_2014, gheisari_survey_2017}, we first need to create this dataset.
This dataset needs as many unique descriptions as possible for each species, so for both a taxonomist and the NLP model, it would be possible to infer the species name based on provided descriptions.
We expect that a DNN model will learn the most important traits of a species if we can feed it enough unique descriptions.
For example, if the models sees multiple unique sentences about a European robin (also see \ref{fig:workflow}, the model will learn that an important trait for an European robin it its orange breast and plumage.
By applying posthoc explanatory techniques we can extract the most disciminative features from the text that are used for making a prediction.

This brings us to the first objective of this research: (1) creating an high-quality database with species and their descriptions.
We will train a model that can recognize descriptions sentences and deploy this model in a web crawler that can automatically query search engines.
The text from the returned URLs are broken down into single sentences and the text is classified by the model.
We hypothesise that this will result in a large database with clean descriptions about species, that an NLP model can learn distinctive features per species.

Once enough data is gathered, we train a NLP model to predict species based on the predictions.
The NLP model needs to make predictions on partial descriptions, but in the meantime, it should be clear which parts of the description data is used to make to prediction so the reasoning can be evaluated. 
This brings us to the second and third objective of this research: (2) creating an NLP model that can infer species names based on description data, (3) while in the meantime keeping traceability throughout the model.
This way we can validate if the model is focusing on useful traits of species.
We expect that if train an NLP model on text descriptions and extract the most important text for making a predictions, we can extract the most relevant and distinctive features of each species.

We first will describe our approach in the Methodology.
In Section \ref{par:dataset} we will describe how we gather enough description data per species.
In Sections \ref{par:Architecture} we define an NLP architecture and in Section \ref{par:keywords} we describe a method for comparing different posthoc explanatory techniques to extract the most distinctive traits of each species.
In Section \ref{par:results} we present the results and in Section \ref{par:discussion} we discuss our results.

\newpage
\section{Methodology} \label{par:methodoly}
\markboth{Approach}{Approach}


\begin{figure} [t]
    \centering
    %\vspace{-2cm}
    \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{figures/workflow_v3.pdf}}
    \caption[Workflow]{A simplified workflow. The web crawler searches for species descriptions on the word wide web and stores it in a database. The data is used to train a NLP model the infer species bases on the textual data.}
    \label{fig:workflow}
\end{figure}

The core of our idea is gathering enough text data per species, so an NLP model can learn key words describing each species.
Using natural natural language for this idea has several potential benefits.
Humans communicate using natural language, resulting in an theoretically endless amount of data that can be used and by using natural language, the results remain interpretable for humans.
We first start by creating a DNN that can classify species descriptions and deploy this DNN in a web crawler.
This web crawler can automatically query species and store descriptive sentences.
We use the harvested data to train the NLP model.
Finally we assess the NLP model.
In Figure \ref{fig:workflow} a simplified version of the workflow can be found.
This chapter will give a detailed description of the steps.

\subsection{Creating an Extensive Dataset} \label{par:dataset}
The World Wide Web has potentially an endless amount of species descriptions available.
This makes the internet suitable for harvesting training data.
However, description sentences can be theoretically limitless, e.g. for a Brown bear (Ursus arctos) the description text could be: "The fur is brown", "The brown bear has brown fur".
These are relatively simple sentences.
Sentences can also have a more difficult semantic, e.g. "The fur of the Brown bear is similar to that of the Grizzly bear".
A classic machine learning approach that requires a rule-based match system for sentence classification is not feasible to harvest description data from the internet.
Instead of a classic rule-based approach we train a DNN that can classify text into description or non-description.
To properly train a deep learning model that can classify text data into description and non-description text, a large, accurate and consistent labelled dataset is needed \autocite{munappy_data_2019}.

\subsubsection{Training a Model for Text Classification} \label{par:reedloss}
%The internet has a vast amount of free data available that can be easily harvested.
We create this large dataset by scraping text from structured web pages.
We select three websites that have large databases, are structured enough and have a rich scholarly content about species:
\begin{itemize}
    \item \href{www.wikipedia.com}{Wikipedia}. Wikipedia is a fee online encyclopedia that is maintained by volunteers. Everyone is allowed to edit pages. Quality is maintained by moderators and other volunteers.
    \item \href{https://birdsoftheworld.org/bow/home}{Birds of the World (BoW)}. BOW is constructed and maintained by ornitnologists from all over the word \autocite{billerman_birds_2020}
    \item \href{https://powo.science.kew.org/}{Plant of the World Online (PoWO)}. PoWO is international collaborative database of the world's flora. Its data is based on scientific papers. It is maintained by the Royal Botanic Gardens, Kew  \autocite{facilitated_by_the_royal_botanic_gardens_plants_2019}
\end{itemize}
All three websites divide their information into paragraphs such as "Introduction", "Appearance", "Characteristics", "Habit".
These headers give us the opportunity to automatically label large amount of data with a few lines of code.
In our case we stored text from paragraphs with titles like:  "Introduction" and "Habit" as negative labels and "Introduction" and "Appearance" as positive values, making this a two class classification problem.
The text chunks inside the paragraphs are used as features.
Random pages from Wikipedia that are not about species are also used to gather additional negative values.
We hypothesise that the model will be better in classifying text not related to species descriptions.
%BERT models use two specials tokens.
%This first token is always \verb|[CLS]|, which stands for "Classification".
%Using \verb|[CLS]|, the model knows the sequence is used for classification.
%The second token is \verb|[SEP]|, which stand for "Separation". 

However, automatically label the data will not result in a consistent dataset.
There is a chance that we miss certain headers and miss label them during the preprocessing.
Even if we correctly label all features not all data will be labelled correctly as text is generally unstructured, even in structured databases \autocite{kumar_text_2020}.
E.g. some species description might be located in the paragraph "Introduction" and some general information might be in the paragraph "Characteristics".
Consider the text "The European robin is red-breasted with distinctive eyes.".
This text is likely to be found in a paragraph about descriptions.
Besides, the text can also occur in the introduction paragraph about an European robin or it could be part of an image caption of an entirely different paragraph. 
In second cases the text will be miss classified with a binary cross entropy loss the model parameters will not be updated correctly. 
By implementing the semi-supervised loss function from \textcite{reed_training_2015}, this will be prevented to a certain extent:
\begin{equation} \label{eq:softloss}
 SoftLoss(q, t) = \sum_{k=1}^{L}[\beta t _k + (1- \beta )q _k]log(q _k)
\end{equation}
~$q$ is directly used to calculate regression targets for each batch,~$t$ are the target values and \(\beta\) is used to balance the prediction.
Using natural language in combination with the soft loss function from \textcite{reed_training_2015}, allows the model for semi-supervised learning.
This combination has the advantage that it is now very easy to scale natural language training as we do not need hand annotate the datasets.
With Equation \ref{eq:softloss}, the model is now 'allowed' to disagree with the training label to a certain extend when the prediction certainty of the model is high enough.
%When the prediction values reach a set threshold (\(\beta\)), the prediction is treated as correct and the loss is calculated accordingly.
%If the model prediction reaches the set threshold, the prediction is seen as a the correct label and the label is changed during the training process, but if the model prediction is below the threshold, the loss will be calculated accordingly.
We assume most labels are correct, allowing to the model the model to learn enough from these samples.
therefore we use a \(\beta\) of 0.80\footnote{Best practice would be find the correct \(\beta\) through cross validation \autocite{reed_training_2015, han_survey_2021}. However, as we train the model on Google Colab instances this would not be feasible.}.
If we start with a low \(\beta\) (e.g. 0.50), there is a chance the model does not learn correctly as it can bootstrap too many samples.
If we use a high \(\beta\) (e.g. 1,00), the model will get penalised very hard for these miss labeled samples as it cannot bootstrap them.

\subsubsection{Model Architecture} \label{par:Architecture}
As a base for the text classification model, we used a distilled version (distillBERT) of the Bidirectional Encoder Representations from Transformers (BERT) \autocite{devlin_bert_2019}, created by \textcite{sanh_distilbert_2020}. 
This version of BERT is 60\% faster, has 40\% less parameters and still reaches 97\% on general language understanding \autocite{sanh_distilbert_2020}.
Using a pre-trained model can provide significantly improvements over models trained from scratch \autocite{mikolov_distributed_2013}.
The use of a pre-trained model is called transfer learning and could speed up the training process and increase the accuracy of the deep learning model.
Both distillBERT and BERT are already trained on a large corpus of English words and can be used freely.
\textcite{sun_how_2020} already investigated how to fine-tune BERT for text classification.
We use their findings on the full BERT model, to fine-tune the distilBERT model for description text classification.

Like \textcite{sun_how_2020} we first fit a dropout layer (0.1) on top of the distillBERT architecture to enable some regularisation.
This dropout layers is followed by a Rectified Linear Unit (ReLU) activation function:
\begin{equation} \label{ReLU}
    f(x) = \max(0, x)
\end{equation}
The ReLU activation function is followed by two fully connected layers.
The first linear layer has an input size of 786 (BERT and distillBERT both output a vector of 768 for each token) and an output size of 512.
The second linear layer has an input size of 512 (output of the first linear) and an output of 2, as this is a binary classification problem.
The final activation function is a log-likelihood (log-softmax) function:
\begin{equation} \label{eq:logsoftmax}
    f(x) = \log_{}(\frac{e^{x_i}}{\sum_{j=1}^K e^{x_j}})
\end{equation}
Where~$x$ is the~$log$ probability.
Using a log-softmax will slightly increase the error factor of the model over a normal softmax function, punishing mistakes a bit higher.
By taking the exponent, the standard likelihood (softmax) probabilities can be computed:
\begin{equation}
    p = e^{x} 
\end{equation}
Where~$p$ is the probability value between 0 and 1 and~$X$ is the output of the log-softmax layer.

The basis distillBERT architecture can be fine-tuned further by updating the inner parameter of the transformer layers during training. 
This can be useful if BERT is utilised in the non-general domain \autocite{devlin_bert_2019, sun_how_2020, sanh_distilbert_2020}, which is the case with description data.
However, we decided not to fine-tune the inner workings of BERT for two reasons: 
(1) We used additional Wikipedia to increase the text data with negatives values.
These random Wikipedia pages are highly likely to contain information from the general
domain, resulting in a  data distribution close(r) to the general domain.
(2) A binary classification problem is a relatively simple task; updating the inner parameters during training cost additional time and computing power, while it only might result in a small accuracy increase.

To prepare the text spans, we use the tokeniser of \textcite{wolf_huggingfaces_2020}.
The used BERT architecture can only process up to 512 token at once \autocite{sanh_distilbert_2020, devlin_bert_2019}.
Most tokenisers will truncate text spans larger then 512 tokens as most text information is in the beginning (head-encoding), end (tail-encoding) or beginning and end (head+tail-encoding).
We decide not truncated the text.
Instead we randomly split the text into text spans with a minimum of 10 words and a maximum of 512 words.
We hypothesised that the model might be better in recognising shorter description spans by splitting the raw spans.  
This way we enlarge the dataset and prevent loosing information with truncating spans larger then 512 tokens. 
We use a minimum of 10 words to prevent the model fixating on specific words.

%The tokeniser of \textcite{wolf_huggingfaces_2020} returns a Python dictionary with the input ID's and the attention mask of the text.
%E.g. by tokenizing the text "This is a test.", the tokeniser returns the following:
%~$'input\_ids': [101, 2023, 2003, 1037, 3231, 1012, 102]$, 
%~$'attention\_mask': [1, 1, 1, 1, 1, 1, 1]$.
%~$[101]$ indicates the start of a sequence and ~$[102]$ indicates the end of a sequence.
%During the training the text is also padded to the maximum of 512 tokens and the attention masks are randomised between 0 and 1.

\subsubsection{Training \& Evaluation}
%We train the model on a Google Colab instance.
We use the Adam optimiser \autocite{kingma_adam_2017} during the training process.
The Adam optimiser has shown good performance when fine-tuning BERT for text classification \autocite{you_large_2020}.
The hyperparameters were set based on the paper of \textcite{sun_how_2020}.
The learning rate was set to 3e\textsuperscript{-5} and the batch size was set to 32 as this is generally found the most effective batch size for BERT \autocite{devlin_bert_2019, sanh_distilbert_2020, sun_how_2020, you_large_2020}.
During the training the gradients are clipped to prevent the gradients from exploding.
we train the model for 35 epochs.

As there is a high probability that the dataset is will be unbalanced, it will be evaluated with a precision-recall summary and a precision-recall curve.
The precision-recall summary and curve give more reliable results with unbalanced datasets \autocite{saito_precision-recall_2015}.
When the model reaches an f1-score of at least 0.90 on the test set, the model is tested on two additional left-out databases, the \href{http://www.llifle.com/}{LLifle} dataset and the \href{https://www.worldagroforestry.org/}{AgroForestry} dataset.
When the model reaches a f1-score on 0.80 on these external datasets, the model will be deployed in a web crawler.
The f1-score is calculated with the following formula:
\begin{equation}
    F_1 = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}} 
\end{equation}
For calculating precision-recall metrics we use the Python package from \textcite{pedregosa_scikit-learn_2011}.

Before the text of the external dataset is tokenised, it is first cleaned of references and footnotes.
We no longer split the into random chunks, but we use a sentecizer of \textcite{honnibal_spacy_2020} to split the text into sentences.
This is also how text will be treated that is retrieved by the web crawler.
By splitting text into sentences instead of random text chunks we prevent information loss.

Like the training data, the left-out datasets will also have miss labelled sentences.
We use the following equation\footnote{This actually looks like the \texttt{hardloss} (Equation 7) from \textcite{reed_training_2015}. In this case the equation is changed from a loss function to an If-Then statement.} to correct these labels to a certain extent:
\begin{equation} \label{eq:softloss_ifthen}
(\hat{\gamma}^{(i)} >= \beta \rightarrow \hat{\gamma}^{(i)} = \gamma^{(i)}) \wedge ( \leftharpoondown \hat{\gamma}^{(i)} = \hat{\gamma}^{(i)})
\end{equation}
As the loss is no longer calculated, we simple calculate the prediction certainty of the model.
If the model reaches a probability of 0.80 or higher (\(\beta\)) the model is allowed to chance the label, in all other cases the predicted label is matched against the actual (paragraph) label.


\subsubsection{Gathering \& Storing Description Data}
We deploy the classification model in a web crawler.
This web crawler can automatically query search engines with a predefined list of birds and plants.
The list for plants is based on the \href{https://www.ipni.org/}{International Plant Name Index} (IPNI) and the list for birds is based on the BoW database %of \href{https://birdsoftheworld.org/bow/home}{Birds of the World} (BoW).
The IPNI database contains over 1,3 million plant species and the BoW database contains over ten thousand birds.

The IPNI database is large, containing all 1,3+ million known plant species.
There is a high chance that not all species in this database can be queried within the time-frame of this research.
Therefore we sort the plant species based on their number of description in the PoWO dataset that is used to train the classifier.
Plant species with more descriptions also have a higher chance of being described somewhere else on the web.
This way we make sure the plants that will be queried have a high chance to contain a description somewhere on the web.
We do the same for the bird species; bird species are sorted based on their number of descriptions in the BoW dataset.

%The web crawler queries the search engines. 
%This results in a list URLs with possible species descriptions.
\begin{figure} [t]
    \centering
    %\textbf{The Web Crawler}
    %\vspace{-2cm}
    \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{figures/webcrawler_v2.pdf}}
    \caption[Web Crawler]{The process of the web crawler. The search engines are queries with five different queries per species. The search engines return a list of URLs per species. BeautifulSoup break extract the HTML of each URL and retrieves the text. SpaCy break the text down into single sentences. The sentences are presented to the classifier.}
    \label{fig:webcrawler}
\end{figure}

In Figure \ref{fig:webcrawler} the process overview of the web crawler can be found.
We use five different ways of constructing a query: the name of the species without any additional search term and the name of the species plus description, diagnosis, attributes or captions.
We wrap quotations around the species to force the search engine to only return the exact species name. 
This way search engines cannot return any results that it deems similar.
Each query will return a list of candidate URLs and are appended to a list for a single species.
%The URL list is checked for duplicates as this is computationally less expense than calculating the cosine similarity for each sentence to drop duplicates.
The URLs are visited and all the text if retrieved.
If a URL points towards a PDF file er text file, it is skipped.
PDF files and text files are read line by line and not as a single text span and it is difficult to break these down into single sentences. 
Finally the header of the URL is checked against the queried species to make sure that the page really contains information about the queried species.
%Not every query will return correct websites, i.e. websites about the queried species.
%This way we prevent accidentally storing description data for the wrong species.

\begin{figure}[bh!]
    \centering
    
    \begin{subfigure}[b]{1.00\textwidth}
        \centering
          \framebox{\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule}{%
          "Plumage, legs and beak orange. The bill and the legs are both black. The house is large with enormous windows. This is something random, but the sexes are similar. Nuclear power might solve the world's energy crisis. The tree has a brown bark and the leaves are pointed. The trunk bears conical spines to deter animal attacks. By growing in shaded places, the plant reduces evaporation. Seeds are 3 cm. Branches usually in whorls of 3."}}
        \caption{A random piece of text.}
        \label{fig:webcrawler_sents_nopred}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{1.00\textwidth}
        \centering
        \framebox{\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule}{%
        "
        \sethlcolor{color1}\hl{Plumage, legs and beak orange. < 0.585 >}
        \sethlcolor{color2}\hl{The bill and the legs are both black. < 0.943 >}
        \sethlcolor{color3}\hl{The house is large with enormous windows. < 0.045 >}
        \sethlcolor{color4}\hl{This is something random, but the sexes are similar. < 0.486 >}
        \sethlcolor{color5}\hl{Nuclear power might solve the world's energy crisis. < 0.026 >}
        \sethlcolor{color6}\hl{The tree has a brown bark and the leaves are pointed < 0.722 >}
        \sethlcolor{color7}\hl{The trunk bears conical spines to deter animal attacks. < 0.530 >}
        \sethlcolor{color8}\hl{By growing in shaded places, the plant reduces evaporation. < 0.162 >}
        \sethlcolor{color9}\hl{Seeds are 3 cm. < 0.964 >}
        \sethlcolor{color10}\hl{Branches usually in whorls of 3. < 0.999 >}
        "}}   
        \caption{A random piece of text broken down into single sentences and classified.}
        \label{fig:webcrawler_sents_pred}
    \end{subfigure}
    \caption[An example how the web crawler 'sees' the text data]{A small example how text from URLs is processed. Text is broken into single sentences by the Sentecizer of \textcite{honnibal_spacy_2020} and the classifier classifies each sentences. Sentences with a value of 0.50 or higher are stored in the database. The darker the colour green, the higher the prediction value. The prediction value is also shown after each sentence.}
    \label{fig:webcrawler_sents}
\end{figure}


When the web page meets all criteria, the raw text is retrieved and broken down into single sentences. 
This makes sure the descriptions will be stored as much as possible per unique species trait.
The text is cleaned using several regular expressions. % LINK TO THE EXPRESSIONS
We use a pretrained sentecizer from \textcite{wolf_huggingfaces_2020} to split the text into single sentences.
Every sentence is checked against the train description classifier.
If a description is recognised by the model, the sentence is stored.
We use a threshold of 0.5 to determine if a sentence is a description or otherwise.
In Figure \ref{fig:webcrawler_sents} a small example can be found how the web crawler will process web pages and stores relevant information. 




\begin{comment}


\begin{figure} [h!]
     \centering
     \begin{subfigure}[b]{1.00\textwidth}
         \centering
         %\hspace{-1.0cm}
         \frame{\includegraphics[width=\textwidth]{web_crawler_example_sents_2.pdf}}
         \caption[Raw text span example]{The raw text span from a web page. This a small portion of a web page about Bombax ceiba (source: \href{http://www.llifle.com/Encyclopedia/TREES/Family/Bombacaceae/31994/Bombax_ceiba}{Llilfe.com/Bomax\_ceiba}).}
         \label{fig:webcrawler_sents_nopred}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{1.00\textwidth}
         \centering
         %\hspace{-0.5cm}
         \frame{\includegraphics[width=\textwidth]{web_crawler_example_sents_preds_2.pdf}}
         \caption[Cleaned and classified text span example]{The cleaned and classified text. The text is coloured by the prediction value (high value is dark green, a low value near white). The prediction value of each sentence can be found at the end of the sentence.}
         \label{fig:webcrawler_sents_pred}
     \end{subfigure}
     \caption[Example of the web crawler process]{A small example how the web crawler processes and classifies data from a web page. The raw text span is cleaned from brackets, sources and other textual punctuation irregularities. After cleaning the text is split into single sentence. Each sentence is classified by the model. Sentences that reach or exceed the threshold of 0.5 are stored. Although this example is never seen by the model (not used in training), the classifier classifies the sentences with high certainty in most cases.}
     \label{fig:webcrawler_sents}
\end{figure}


\end{comment}
\begin{comment}

\subsubsection{Sentence Similarity}
There is a change that different web pages use the a common source for describing a species.
By checking for similar sentence within the same species we make sure the same sentence is not appended twice to the dataset and the train and test set are completely disjoint.
Like \textcite{reimers_sentence-bert_2019} we use the last hidden state of BERT (distilBERT in our case) and compute the cosine similarity to measure the distance between text spans.
Fortunately, this last hidden state is the output of a base BERT architecture.
However, the last hidden state output contain a matrix of 512 x 768. 
It is not feasibly to compute the cosine similarity distance for multiple matrices of this size.
By using a mean-pooling operation we create a vector of size 768 for every text span and compute the cosine distance for these text spans.
We first create a new dictionary that corresponds to the dictionary initialised by the tokeniser, \verb|{'input_ids': [], 'attention_mask': []}|.
We tokenise all the sentences we are comparing and append the tokens and masking to the correct key.
After all the sentences have been processed, we reformat the dictionary into a single tensor and push it trough the model.
This will result in a tensor of the number of sentences times 512 times 768. 
We sum the 3D matrix along the first axis, resulting in a 2D matrix.
The size of this matrix is the number of sentences time 768.
Each sentence is represented by a vector of size 768
Finally we calculate the cosine similarity between the sentences and drop sentences that exceed the threshold of 0.99.
In Figure \ref{fig:similarity_matrix} a comparison of a few fictional sentences can be found.
\begin{figure} [htbp]
    \centering
    \vspace{0cm}
    \hspace{-1cm}
    %\textbf{Similarity Matrix}
    \makebox[\textwidth][c]{\includegraphics[width=0.7\textwidth]{similarity_matrix.pdf}}
    \caption[Example of sentence similarity]{An example for the calculation of sentence similarity. 
    %In this example the following sentences are compared.
     %0 - "The European robin has a orange plumage."
     %1 - "Black bill, plumage orange."
     %2 - "The bear is large and the claws are sharp."
     %3 - "Bill black; plumage orange."
     %4 - "The bill is black and the plumage is orange."
     %5 - "The bear is large, and the claws are sharp."
    In this case the sentences "Bill black; plumage orange." and "Black bill, plumage orange." reach a similarity score of 0.98. The word order and the punctuation is slightly different. In this case both sentences are stored in the database as it is highly likely they come from different sources. However in the case of "The bear is large and the claws are sharp." and "The bear is large, and the claws are sharp." the sentences are completely similar except for the punctuation. The sentences reach a similarity score of 1 (although this will be because of rounding). In the second case the sentences will come from a the same source and one of them will not be stored.}
    \label{fig:similarity_matrix}
\end{figure}

%Some databases store their description as one long sentence, describing multiple attributes in a single sentence e.g. "Leaves 5–9-foliolate; leaflets narrowly elliptic-obovate, entire, acuminate, 7–20 x 1.8–6.5 cm, glabrous; petiole 5.5 - 25 cm long, at the apex expanded into an almost circular disk." (source: \href{http://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:1166232-2}{PoWO/Ceiba\_pentandra}). 
%This sentence is describing the leaves and the attributes in the same sentence. 

\subsubsection{Knowledge Graph}
A knowledge graph (KG) is a graph that contains interlinked descriptions, information has a formal structure that allows both people and computer to process the data unambiguously \autocite{petkova_crafting_2020}, like in Figure \ref{fig:graph_example}.
If we apply this to species descriptions we would expect the end up with a single graph per species, or with a large graph per class (e.g. mammalia).
Traits are not longer unique and can be shared per species.
Processing the data this will will result in a clean semantic database for both humans and computers.
This database will only contain subjects, relations and objects, also known as semantic triples. 
When feeding this data to the model it cannot be become dependent on artefacts such as word order, punctuation's, etc.

\begin{figure} [h!tb]
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        %\vspace{0cm}
        \includesvg[inkscapelatex=false, width=\textwidth]{PoS_example.svg}
        \caption[Example of part of speech tagging (1)]{An example of part of speech and dependency parsing for the sentence "The Brown bear has brown fur.". The arrows contain the dependency tags and the words contain the part of speech tags.}
        \label{fig:PoS_example}
    \end{subfigure}
    \vfill
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        %\vspace{0cm}
        \includesvg[inkscapelatex=false, width=\textwidth]{PoS_example2.svg}
        \caption[Example of part of speech tagging (2)]{An example of part of speech and dependency parsing for the sentence "The claws are sharp.". The arrows contain the dependency tags and the words contain the part of speech tags.}
        \label{fig:PoS_example2}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        %\vspace{-2cm}
        \includegraphics[width=\textwidth]{kn_example.pdf}
        \caption[Example of a knowledge graph]{The sentences result in five nodes (subject/object) and four edges (relations). The circles contain the objects/subjects and the arrows represent the relations.}
        \label{fig:graph_example}    
    \end{subfigure}
    \caption[Part of Speech tagging and knowledge graph]{The sentences are first broken down with the NLP pipeline of \textcite{honnibal_spacy_2020}. The extracted PoS tags and dependency tags are then used to construct the semantic triples In first case the first node is the normal subject (nsubj) combined witht the compound. The root verb of this subject is the relation and the direct object (dobj) of the verb is the object of the first semantic triple. For the last relation and node, the adjective modifier (amod) of the last node (fur) is used as object. In this case the relation is dependent on the PoS tag of the adjective modifier. In the last case the claws are a normal subject, but are treated as an object.}
    \label{fig:pos_pipeline}
\end{figure}

For PoS extracting we use the library of \textcite{honnibal_spacy_2020}.
This is a pretrained NLP model that can extract PoS tags and their dependencies.
We set up a rule-based system to extract data from the sentences.
In Figure \ref{fig:pos_pipeline} two (very simple) example sentences can be found that are processed using PoS tagging and dependency parsing with the pretrained pipelines of \textcite{honnibal_spacy_2020}.
We have the benefit that we already know the broad subject of every sentence, as each sentence corresponds to the queries species.
This means that we can have the species as starting node, even when the subject is not the species itself, see Figure \ref{fig:PoS_example2}.
Most researches do not have this and also need to determine the subject of every sentence \autocite{hutchison_knowledge_2013} like we do in \ref{fig:PoS_example}.
By processing this in a rule-base system we expect to create a knowledge graph like shown in Figure \ref{fig:graph_example}.
\end{comment}


\subsection{Retrieving Keywords} \label{par:keywords}
We use two different way to extract the most important words from the description sentences.
(1) several different post hoc explanation techniques (see Section \ref{par:attribution}), and (2) a rule-bases system of PoS in combination with dependency parsing (see Section \ref{par:PoS}). 
For comparing the techniques, we use the \href{http://www.vision.caltech.edu/visipedia/CUB-200-2011.html}{Caltech-UCSD Birds 200 dataset} (CUB-200) from  \textcite{welinder_caltech-ucsd_2010}.
The CUB-200 dataset is a dataset with 200 bird species from Northern America.
Each bird is represented by approximately 60 images and each of these images is hand annotated with traits that are seen\footnote{Images are annotated with a certainty of the trait being present, we only use the traits that have the highest certainty in the image.}.

Before extracting keywords, we first match every sentence from our own dataset against bird glossary from \href{https://en.wikipedia.org/wiki/Glossary_of_bird_terms}{Wikipedia/Glossary/Birds}.
The CUB-200 dataset only contains visual traits, where our own bird dataset can also contain non-visual traits like behavioural traits or vocal traits.
Matching our own textual data against the bird glossary will also remove sentences that are incorrectly classified by our classifier and also saves computing power as we can first select the bird, then select a trait and finally match the results against the CUB-200 dataset.

To compute the text similarity we use an untrained version of distillBERT and compute cosine similarity between the two outputs.
We tokenise each sentence and push it trough the distilled version of BERT.
For each sentence we sum the last hidden state along the first axis.
This will result in a matrix where the hidden state if summed for each token:
\begin{equation}
     \sum\nolimits_{j}^{} x_{ij} 
\end{equation}
Where~$i$ is 768 and~$j$ is the number of tokens. 
After this we simple compute the cosine similarity of each text chunk against all the text chunks of the CUB-200 dataset:
\begin{equation}
\cos ({\bf A},{\bf B})= {{\bf A} {\bf B} \over \|{\bf A}\| \|{\bf B}\|} = \frac{ \sum_{i=1}^{n}{{\bf A}_i{\bf B}_i} }{ \sqrt{\sum_{i=1}^{n}{({\bf A}_i)^2}} \sqrt{\sum_{i=1}^{n}{({\bf B}_i)^2}} }
\end{equation}
Where~$A$ is the vector of text chunk 1 and~$B$ the vector of text chunk 2. 
In Figure \ref{fig:similarity_matrix} a comparison of a few fictional text chunks can be found. 
Finally we assess which of the technique delivers the best performance and select this for the larger dataset.

For each unique bird, part and technique combination of the CUB-200 data and the data from the internet we compute a similarity matrix.
We extract the highest value of each unique combination, resulting in a large array of similarity values per attribution.
The harmonic mean is computed to get the best attribution method:
\begin{equation}
    H = \frac{n}{\sum\limits_{i=1}^n \frac1{x_i}}
\end{equation}
Where~$H$ is the harmonic mean,~$n$ is the number of samples and~$x$ is the similarity value. 

\begin{figure} [th]
    \centering
    %\hspace{-2cm}
    \hspace{-2cm}
    %\textbf{Similarity Matrix}
    \includegraphics[width=0.8\textwidth]{similarity_matrix.pdf}
    \caption[Example of sentence similarity]{An example for the calculation of sentence similarity. 
    In this case reconstructed text chunks from the CUB-200 dataset are shown on the y-axis and reconstructed text chunks from scraped data is shown on the x-axis. In this case 'Bill color yellow', and 'Bill yellow' is almost the same, hence the high similarity score of 0.91. Note that the similarity matrix does not necessarily have equal rows and columns.}
    \label{fig:similarity_matrix}
\end{figure}


\subsubsection{Post Hoc Attribution Techniques} \label{par:attribution}
For retrieving key words from the description data we try several different posthoc explanation techniques\footnote{We do not use jittering techniques as this makes no sense in the case of text data, which results in fixed values when tokenised. }. 
We implement all techniques with the Captum library from \textcite{kokhlikyan_captum_2020}.
\begin{itemize}
    \item Layer Integrated Gradients (LIG), based on the paper of \textcite{sundararajan_axiomatic_2017}. We use LIG with 20 steps and with 100 steps.
    \item Occlusions, based on the paper of \textcite{fleet_visualizing_2014}. Although this is originally used for explaining CNNs it can also be used to explain NLP models. We use two different version of occlusion, (1) with a window and stride of both 1. (2) a window of 3\footnote{The windows is 1-dimensional in this case as the input vector is a tensor of the tokenised text.} and a stride of 2. In both cases the word(s) will be replaced with a tensor of zeros to check the prediction changes of the model. 
    \item Shapley Value Sampling (SVS) based on the papers of \textcite{castro_polynomial_2009, strumbelj_efficient_2010}. We use the Captum default of 25 samples.
    \item Layer Activation (LA), which simple computes the activation of the internal BERT embeddings for the inputs.
    \item Layer Gradient X Activation (LGXA), which is the element-wise multiplication of the gradients and the layers activation for a giver target with respect to the layer.
\end{itemize}

We train a classification model on a subset of the BOW dataset.
We make sure that all the bird of the CUB-200 dataset are present in the birds that are presented to the model.
We random sample additional birds from the BOW dataset so the CUB birds make up about 10\% of the total data and resulting in 2,000 birds, but birds with a high amount of HTML pages have priority during the sampling.
This way we make sure birds with a decent number of descriptions are used during training.
We use the web crawler with the trained binary classifier for descriptions to enrich the BOW dataset with sentences from the internet.
We do not split the model into train and test data, as evaluating with a test set has no use.
We use the same hyperparameters and architecture as we did for training the binary classification model (see Section \ref{par:Architecture}).
We only change the last linear output layer so it matches the number of species that are used during training.
After the model has been trained we compute the attribution value for every word per sentence with the aforementioned techniques.


\subsubsection{Part-of-Speech \& Dependency Parsing} \label{par:PoS}
A knowledge graph (KG) is a graph that contains interlinked descriptions fo traits, the information has a formal structure that allows both people and computer to process the data unambiguously \autocite{petkova_crafting_2020}.
If we apply this to species descriptions we would expect the end up with a single graph per species, that contain descriptive information about that species in a semantic way.
These graphs can also be linked if species share common traits.
Creating a knowledge graph of the scraped data will results in a clean database.
This database will only contain subjects, relations and objects, also known as semantic triples. 
We use PoS tagging and dependency parsing to create a clean knowledge graph.
For PoS extracting we use the library of \textcite{honnibal_spacy_2020}.
This is a pretrained NLP model that can extract PoS tags and their dependencies.
We set up a rule-based system to extract data from the sentences.
In Figure \ref{fig:pos_pipeline} two example sentences can be found that are processed using PoS tagging and dependency parsing with the pretrained pipelines of \textcite{honnibal_spacy_2020}.
We have the benefit that we already know the broad subject of every sentence, which is the species.
Most researches do not have this and also need to determine the subject of every sentence if they want to link all data \autocite{hutchison_knowledge_2013} like we do in \ref{fig:PoS_example}.
We know that every sentence describes the queried species, this means that we can have the species as starting node, even when the subject is not the species itself, see Figure \ref{fig:PoS_example2}.
By processing this in a rule-base system we create a knowledge graph like shown in Figure \ref{fig:graph_example}.
\begin{figure} [h!tb]
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        %\vspace{0cm}
        \includesvg[inkscapelatex=false, width=\textwidth]{PoS_example.svg}
        \caption[Example of part of speech tagging (1)]{An example of part of speech and dependency parsing for the sentence "The Brown bear has brown fur.".}
        \label{fig:PoS_example}
    \end{subfigure}
    \vfill
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        %\vspace{0cm}
        \includesvg[inkscapelatex=false, width=0.6\textwidth]{PoS_example2.svg}
        \caption[Example of part of speech tagging (2)]{An example of part of speech and dependency parsing for the sentence "The claws are sharp.".}
        \label{fig:PoS_example2}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        %\vspace{-2cm}
        \includegraphics[width=0.65\textwidth]{kn_example_v2.pdf}
        \caption[Example of a knowledge graph]{A knowledge graph example. The circles contain the objects/subjects and the arrows represent the relations.}
        \label{fig:graph_example}    
    \end{subfigure}
    \caption[Part of Speech tagging and dependency parsing]{An example of part-of-speech tagging and dependency parsing to extract semantic triples for the creation of a knowledge graph. The arrows indicate the dependency tags and the abbreviations below the sentence contain the PoS tags. The sentences result in five nodes (subject/object) and four edges (relation). }
    \label{fig:pos_pipeline}
\end{figure}

\subsection{Species Predictions}
We use the the technique with the highest harmonic mean.
We use this technique to extract the most important word/trait combinations to create a database.
The methods from extracting useful plant term is almost the same as for birds.
We first match every sentence against \href{https://en.wikipedia.org/wiki/Glossary_of_plant_morphology}{Wikipedia/Glossary/Plants}.


In case PoS and dependency parsing will have the highest mean, we will use the same technique as we did with the birds.





\begin{comment}
We use the integrated gradients from \textcite{sundararajan_axiomatic_2017} to retrieve the most important keywords.
Integrated gradients are best used (1) to identify feature importance, in our case: which are the most important words used for a prediction and (2) to identify data skew, in our case this can be very important. %%% Add something about this.

Our code to calculate the integrated gradients is based on \textcite{gardner_allennlp_2017} and especially this  \href{https://github.com/allenai/allennlp/tree/master/allennlp/interpret}{this part} of their repository.
We use a zero input embedding as a baseline.
All irrelevant words will correspond to the baseline in this case.
We simply compute all the the gradients in a simple loop for a text span:
\begin{equation}
    x' +  \frac{\mathrm{k}}{\mathrm{m}} * (x - x')
\end{equation}
For this we use 20 steps to calculate the gradients as this is the minimum that \textcite{sundararajan_axiomatic_2017} found that could approximate the gradients by 5\%\footnote{The gradients are calculated from 0,00 to almost, but not exactly 1,00. As we are using 20 steps to calculate the gradients, we approximate the gradients by 5\% (\(\frac{1}{20} * 100 = 5\)). By increasing the number of steps, a better approximation of the gradients can be calculated. However, the computation time scales linearly with the number of steps.}.
We calculate the gradients for every input token and sum the gradients.
This leads to the following equation that approximates the gradients \autocite{sundararajan_axiomatic_2017}:
\begin{equation} \label{eq:integrad}
    {IntegratedGrads_i^{approx}} (x) ::= (x_i - x'_i) *  \sum\nolimits_{k=1}^{m}  \frac{\partial F(x' +  \frac{\mathrm{k}}{\mathrm{m}} * (x - x') }{\partial x_i} 
\end{equation}
\(i\) = feature (token/word)\newline
\(x\) = input (text span tokenised)\newline
\(x'\) = baseline (zero input embedding)\newline
\(k\) = scaled feature perturbation constant\newline
\(m\) = number of steps\newline
\((x - x')\) = difference from the baseline. \newline
\newline
By combining Equation \ref{eq:integrad} with PoS tagging and, we can extract the relevant PoS tags used for the prediction.
However, using a tokeniser can split certain words.
For example, for the sentence "The plant has 7 antheriferous stamens." we would expect nine token (beginning, words, point and ending).
Applying the BERT tokeniser on this sentence results in thirteen tokens:
%\newline
"\verb|[CLS] the plant has 7 ant ##her ##iferous st ##amen ##s. [SEP]|".
%\newline
if this happens we sum the gradients, because the sum of the gradients should equal 1.
In this example, both \verb|antheriferous| and \verb|stamens| will consist of three integrated gradients. 



\end{comment}


\newpage
\section{Results} \label{par:results}
\markboth{Results}{Results}
\subsection{Extensive dataset}
\subsubsection{Description Classification Model}
Scraping the structured sources and using their paragraph titles resulted in 1,461,884 samples before splitting the sample into random text chunks.
1,105,576 samples correspond to label zero and 356,308 correspond to label one.
%Each sample consists of a tuple, containing the label (0/1) and the text span.
Splitting the samples into text chunks resulted in 2,258,162 text chunks.
1,678,830 chunks have a label that correspond to something different (label 0) 579,332 chunks have a label that corresponds to a description (label 1).
In Figure \ref{fig:text_length_distribution} the text length distribution before and after splitting the original samples in random chunks can be found.
As can be seen from Figures \ref{fig:text_length_1}, \ref{fig:text_length_3}, \ref{fig:text_length_2}, and \ref{fig:text_length_4}, all distributions of the text lengths are left-skewed. 

\begin{figure} [h!]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{figures/histogram_text_length_1.pdf}
         \caption{The text length of non-description data before splitting.}
         \label{fig:text_length_1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{histogram_text_length_3.pdf}
         \caption{The text length of non-description data after splitting.}
         \label{fig:text_length_3}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{histogram_text_length_2.pdf}
         \caption{The text length of description data before splitting.}
         \label{fig:text_length_2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{histogram_text_length_4.pdf}
         \caption{The text length of description data after splitting.}
         \label{fig:text_length_4}
     \end{subfigure}
    \caption[Text length distribution for training web crawler model]{The result of scraping structured sources to train  a binary classification model for descriptions classification. The text spans are randomly split into chunks between 10 words and their length, with a maximum of 512 words to prevent truncation. Text spans with a length below 10 words are not split, resulting in some text lengths below 10. Note that outliers of the boxplots are not shown.}
    \label{fig:text_length_distribution}
\end{figure}


The model is evaluated on a left out test set of 5\% and is evaluated on two left-out datasets, the \href{http://www.llifle.com/}{LLifle} dataset and the \href{https://www.worldagroforestry.org/}{AgroForestry} dataset
The summary performance metrics can be found in Table \ref{tab:precision_recall_metrics}.
The model reaches an high precision, recall and f1-score for both the "Non-Description" and "Description" classes.
%These datasets are completely left out of the training/testing fase of the model.
%Together, these dataset contain 17,204 samples.
%771 text spans correspond to a description, and 16,433 text spans corresponds to something different.
%After the text has been cleaned and split into sentences there are 74,836 samples.
%There are 8,590 samples with a descriptions sentence and 66,246 samples that contain a sentence describing something %different.
%In Table \ref{tab:precision_recall_descriptionsmodel_external} the precision-recall summary for the external datasets can be found.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[ht]
\centering
\caption[Precision-recall metrics test and two left out datasets]{The precision-recall metrics for the binary classification model tested on the test dataset and two external datasets (LLifle and AgroForestry).}
\label{tab:precision_recall_metrics}
\begin{tabular}{@{}lcccccccc@{}}
\cmidrule(l){2-9}
 & \multicolumn{2}{c}{\textbf{Precision}} & \multicolumn{2}{c}{\textbf{Recall}} & \multicolumn{2}{c}{\textbf{f1-score}} & \multicolumn{2}{c}{\textbf{Support}} \\ \cmidrule(l){2-9} 
                 & Test & Extern & Test & Extern & Test & Extern & Test    & Extern \\ \midrule
Other            & 0.98 & 0.96   & 0.99 & 0.96   & 0.99 & 0.97   & 167,955 & 64,639 \\
Description      & 0.97 & 0.83   & 0.95 & 0.77   & 0.96 & 0.83   & 57,864  & 10,197  \\ \midrule
Accuracy         &      &        &      &        & 0.98 & 0.95   & 225,819 & 74,836 \\
M Average        & 0.98 & 0.90   & 0.97 & 0.87   & 0.98 & 0.89   & 225,819 & 74,836 \\
W Average        & 0.98 & 0.95   & 0.98 & 0.95   & 0.98 & 0.95   & 225,819 & 74,836 \\ \bottomrule
\end{tabular}
\end{table}
As the the classes are imbalanced in both cases we  evaluated the model with a precision-recall plot and not with an receiver operator curve (ROC) figure, as an precision recall curves give a better overview when it comes to imbalanced datasets \autocite{saito_precision-recall_2015}.
The results of this plot can be found in Figure \ref{fig:precision-recall}, Figure \ref{fig:precision_recall_curve_test} contains the precision-recall plot for the test set and Figure \ref{fig:precision_recall_curve_test_external} contains the precision-recall plot for the left out datasets.
\begin{figure} [h!]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{precision_recall_plot.pdf}
         \caption{The Precision-Recall curve for the test data.}
         \label{fig:precision_recall_curve_test}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{precision_recall_plot_extern.pdf}
         \caption{The Precision-Recall curve for the left out data.}
         \label{fig:precision_recall_curve_test_external}
     \end{subfigure}
     \caption[Precision recall curves for test and left-out datasets]{The precision-recall curve for the test dataset and the left out datasets. The model seems to perform reasonable in both cases. The the Average Precision (AP) reaches 99\% in the case of the test dataset and the AP reaches 87\% in the case of the left-out datasets. The area under the curve (AUC) reaches 99.49\% for the test set and 93.14\% for the left-out datasets.}
     \label{fig:precision-recall}
\end{figure}

In Figure \ref{fig:predictionvalues_external} the prediction values of the classifier for the two left-out datasets can be found.
The classifier seems to work correctly.
Most values are pushed towards zero or one.
It seems there are significant more samples that qualify as a non-description then description samples.
The bottom figure provides a zoomed overview between x-axis 0.2 and x-axis 0.8 as values lower than 0.2 and higher than 0.8 are treated as correct because of Equation \ref{eq:softloss_ifthen}.
It seems that the classifier miss classifies more non-description samples as descriptions, the miss classified samples from the threshold value of 0.5 to 0.8 seem significant higher than the miss classified samples from 0.2 to 0.5.

\begin{figure}[h!]
 \centering
 %\hspace{-0.5cm}
 \includegraphics[width=\textwidth]{figures/predictionvalues_external.pdf}
 \caption[Prediction values of left-out datasets]{The distribution of the two left-out datasets. In the first figure all the values are shown. In the second only values between 0.2 and 0.8 are shown as values below 0.2 and above 0.8 are always treated as correct. Note that the bars are stacked in the second figure, making the correctly classified and miss classified as high a the corresponding bars from the first figure. In both figures each bin has a width of 0.01.}
 \label{fig:predictionvalues_external}
\end{figure}

\subsubsection{Dataset results}
%\textbf{URLs.}
We ran the web crawler for a little more than two weeks\footnote{We ran the crawler with a minimum of a thousand species each run, because of this we ran the crawler a little longer then two weeks, resulting in a large even number.}.
In these two weeks we managed to query 20,000 different plant species.
Every species returned at least one URL.
For 20,000 species there are 519,197 unique URLs returned from 27,612 unique base URLs (URL homepage).
For 16,193 species the URL returned text that could by classified.
3,807 species URLs did not return any useable text\footnote{The POWO website is excluded from the web crawler as this database is already available.}
In Figure \ref{fig:URL_distribution}, the distributions per species can be found.
Figure \ref{fig:URL_distribution_1} shows that for most species around 17 to 33 unique URLs are found, Figure \ref{fig:URL_distribution_2} shows that for several species multiple pages from the same base URL are retrieved.
Finally Figure \ref{fig:URL_distribution_3} shows that most URL do not contain any description information for the species.
This happens when the headers of a certain page does not match anything from the species name, or the text of the URL does not contain any description information according to the classifier.

%\begin{figure}[h!]
% \centering
% %\hspace{-0.5cm}
% \includegraphics[width=\textwidth]{figures/URL_distribution.pdf}
% \caption[The URL, base URL and URLs with text per Species]{The distribution of the unique URLs per species, base URLs %per species and URLs with text per species.}
% \label{fig:URL_distribution}
%\end{figure}

\begin{figure} [h!]
     \centering
     \begin{subfigure}[b]{1\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{figures/URL_distribution_1.pdf}
         \caption{The number of unique URLs per species.}
         \label{fig:URL_distribution_1}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{1\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{figures/URL_distribution_2.pdf}
         \caption{The number of unique base URLs per species.}
         \label{fig:URL_distribution_2}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{1\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{figures/URL_distribution_3.pdf}
         \caption{The number of URLs with text per species.}
         \label{fig:URL_distribution_3}
     \end{subfigure}
     \caption[The URL, base URL and URLs with text per Species]{The distribution of the unique URLs per species, base  URLs per species and URLs with text per species.}
     \label{fig:URL_distribution}
\end{figure}





In Figure \ref{fig:URL_top20} the most returned base URLs can be found.
Research.net is by far the most returned base URL when querying species, followed by Academia.edu and the international version of Wikipedia.com.
Academic sites seems to contain the most information about the queried species.
\begin{figure}[h!]
 \centering
 %\hspace{-0.5cm}
 \includegraphics[width=\textwidth]{figures/URL_top20.pdf}
 \caption[The top 20 returned base URLs]{The top 20 base URLs that are returned the most. Above each bar the part of the overall total is plotted. This is the percentage compared to all unique base URLs. The top 20 unique base URLs make up 46.49\% of the total.}
 \label{fig:URL_top20}
\end{figure}
The URL titles are checked against the species name, if the species name exists within the title the sentences are retrieved, resulting in 4,284,057 Sentences.
We used the binary description classifier to classify these sentences, resulting in 218,106 that are classified as a description.
We found description data for 14,557 different species.
We appended the data from the structured sources (PoWo, Agro Forestry and LLife) to these description sentences and dropped sentenced that contained two words or less per sentence, resulting in 484,336 description sentences.
Figure \ref{fig:text_distribution} shows the distribution of sentences before and after classification.
In this figure the sentences are before classification without sentences from the structured datasets.
The description sentences contain all structured sources and the classified sentences from the web crawler.

\begin{figure} [h!]
     \centering
     \begin{subfigure}[b]{1\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{figures/text_distribution_1.pdf}
         \caption{The number of sentences per species.}
         \label{fig:text_distribution_1}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{1\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includegraphics[width=\textwidth]{figures/text_distribution_2.pdf}
         \caption{The number of description sentences per species.}
         \label{fig:text_distribution_2}
     \end{subfigure}
     \caption[The sentences and descriptions sentences per species]{The sentences and descriptions sentences per species. The sentences per species are the sentences before classification. The description sentences are sentences that exceeded the threshold of .5 in the binary descriptions classifier. Note that the x-axis is a log-scaled axis and the boxplot is based on the normal values, hence the outliers on the right.}
     \label{fig:text_distribution}
\end{figure}




%\begin{figure}[h!]
% \centering
 %\hspace{-0.5cm}
% \includegraphics[width=\textwidth]{figures/text_distribution.pdf}
% \caption[The sentences and descriptions sentences per species]{The sentences and descriptions sentences per species. The sentences per species are the sentences before classification. The description sentences are sentences that exceeded the threshold of .5 in the binary descriptions classifier. Note that the x-axis is a log-scaled axis.}
% \label{fig:text_distribution}
%\end{figure}

\subsection{Retrieving Key Words}
\subsubsection{Similarity Results CUB}
In Figure \ref{fig:CUB_distribution} the number of descriptions per bird for our created dataset and the CUB-200 dataset can be found.
For our own dataset we only display the 200 birds that exist within the CUB-200 dataset as these birds are used for further analysis.
The number of descriptions per bird is higher on average for the CUB data.
The CUB-200 dataset has a mean of 148.86 descriptions per bird, while our dataset had 119.21 descriptions per bird.
For one Bird, the Cape Starling (Lamprotornis nitens), we did not find any descriptions.



\begin{figure}[h!]
 \centering
 %\hspace{-0.5cm}
 \includegraphics[width=\textwidth]{figures/CUB_distribution.pdf}
 \caption[Bird description distribution]{The number of descriptions per bird for our data set that consists of random data and the Birds of the World dataset, and the CUB-200 dataset from the California Institute for Technology \autocite{welinder_caltech-ucsd_2010}. }
 \label{fig:CUB_distribution}
\end{figure}

From \href{https://en.wikipedia.org/wiki/Glossary_of_bird_terms}{Wikipedia/Glossary/Birds} we extracted 1,163 glossary terms related to birds \footnote{This list include the most common terms, but also term that are deemed similar. For example 'addled eggs ' has to glossary terms 'wind eggs' and 'hypanema' that mean the same. We append all three term to the glossary list.}.
Matching our dataset against the glossary list resulting a 23,839 sentences for 199 birds. 
These sentences described 231 unique parts. 
We extracted 4,537 unique potential adjectives and with the parts this resulted in 39,451 unique combinations.
In Table \ref{tab:attribution_unique} the complete overview for all unique combinations can be found.
Layer Attention and Layer Gradient X Activation seem to fail as the amount of unique potential adjectives and traits is significant lower than the other attribution methods.
From these two Layer Gradient X Activation seems to perform the best of the two with 1,588 potential adjectives and 8,911 traits.



\begin{table}[]
\centering
\caption{The unique combination of Birds, Sentences, Adjectives, Traits and Trait Combinations per attribution method.}
\label{tab:attribution_unique}
\begin{tabular}{@{}lccc@{}}
\toprule
 & \textbf{Adjectives} & \textbf{Parts} & \textbf{Traits} \\ \midrule
\begin{tabular}[c]{@{}l@{}}Part-of-Speech and\\ Dependency Parsing\end{tabular} & 1988 & 336 & 8329 \\ \midrule
\begin{tabular}[c]{@{}l@{}}Integrated Gradients \\ 20 Steps\end{tabular} & 2,301 & 231 & 15,288 \\ \midrule
\begin{tabular}[c]{@{}l@{}}Integrated Gradients\\ 50 Steps\end{tabular} & 2,296 & 231 & 15,239 \\ \midrule
\begin{tabular}[c]{@{}l@{}}Occlusion\\ Window 1, Stride 1\end{tabular} & 2,259 & 231 & 14,966 \\ \midrule
\begin{tabular}[c]{@{}l@{}}Occlusion\\ Windows 3, Stride 2\end{tabular} & 2,184 & 231 & 14,132 \\ \midrule
\begin{tabular}[c]{@{}l@{}}Shapley Value \\ Sampling\end{tabular} & 2,094 & 231 & 13,617 \\ \midrule
\begin{tabular}[c]{@{}l@{}}Layer Attention\\ -\end{tabular} & 212 & 231 & 3,135 \\ \midrule
\begin{tabular}[c]{@{}l@{}}Layer Gradient \\ X Activation\end{tabular} & 1,588 & 231 & 8,911 \\ \midrule
\textbf{All Attributions} & 4,537 & 231 & 3,9451 \\ \bottomrule
\end{tabular}
\end{table}

In Figure \ref{fig:hmean_violin} the results of computing the similarity of the attribution technique against the CUB-200 dataset can be found.
The values are sorted on the median values (lowest at the top, highest at the bottom).


\begin{figure}[h!]
 \centering
 %\hspace{-0.5cm}
 \includegraphics[width=\textwidth]{figures/densities.pdf}
 \caption[Similarity results]{The Results of computing the similarity of the different methods of our own dataset against the CUB-200 dataset. The pentagon corresponds to the median, the circle corresponds to the mean and the diamond corresponds to the harmonic mean. The densities of the violin plot do not extend past extreme values.}
 \label{fig:hmean_violin}
\end{figure}

\subsection{Species Classification}


\newpage
\section{Discussion} \label{par:discussion}

\subsection{Dataset Creation}
We gathered enough samples (See Figure \ref{fig:text_length_distribution}) to properly train a binary classifier.
Figures \ref{fig:text_length_1} and \ref{fig:text_length_2} show that the number of samples for the negative class is much higher than the positive class. 
After splitting the features into chunks of a random length between 10 and the text length the data is still skewed; the non-description class is over represented in the data (see Figures \ref{fig:text_length_3} and \ref{fig:text_length_4}). 
This skewed data set could lead to problems as the classifier will not see enough samples of the underrepresented class. 
The classifier will only update the parameters for the non-description class and still reach good result on the test data.
We evaluated the dataset with a precision recall plot (Figures \ref{fig:precision_recall_curve_test} and \ref{fig:precision_recall_curve_test_external} because of this skewness.
The results of the binary description classifier perform excellent on the test dataset. 
The loss function of \textcite{reed_training_2015} (Equation \ref{eq:softloss}) seems to do an excellent job.
As can be seen from Table \ref{tab:precision_recall_metrics} and Figure \ref{fig:precision_recall_curve_test} the classifier reaches an excellent AP and f-1 score on the left out test set.
The classifier identifies almost all description text chunks from the data (recall).
The text chunks that are qualified as a description are almost all correctly classified (precision)
The classifier performs worse on the two left-out datasets.
The number of results returned is still quite high with 0.90, however the precision is lower with 0.83; The labels that are returned are incorrect for 17\% of the time.
However as both f-1 values exceeded the threshold value set in the beginning of this thesis deemed the model sufficient and deployed it in the web crawler.
However, we do have some suggestions to improve the binary classifier in future work.
%Unfortunately we due to lack of time we only implemented of \(\beta\) of 0.80. 
%By starting with a lower \(\beta\) and gradually increase \(\beta\) during the parameter optimisation %process, the model might optimise the parameters better as it gets penalised less hard.

With the two left-out datasets, we processed the text the same way as we would have when the text was processed by the web crawler. 
We clean the text, and split the text into single sentences with the sentecizer of \textcite{honnibal_spacy_2020}.
As we do not have the ground truth labels of this dataset, we implement Equation \ref{eq:softloss_ifthen}.
This equation will treat prediction values with a 0.8 or higher as correct.
The classifier still performs very reasonable on the two left out datasets.
However, with a f1-score of 0.86 the classifier does perform worse than on the test dataset.
This performance issue is likely to occur due to the following reasons:

First, during the training of the binary classifier, text is randomly split into chunks. 
In some cases this will result in text chunks that will contain part of a description and something else.
Even with an appropriate \(\beta\), the model cannot optimise the parameters correctly as a single features contains information from both labels.
This will also result in a model that cannot utilise the loss function from \textcite{reed_training_2015} optimally.
The model will perform well on correctly labeled sentences, but its prediction values will be less certain.
With the two left-out test sets, there will be miss labelled sentences (see Section \ref{par:reedloss}).
Because the parameters are not fully optimised, the model will not reach the \(\beta\) threshold of 0.8.
This will result in miss labelled sentences, where the model cannot use the high prediction value in case it is certain about a prediction, because the parameters are not fully optimised.
In future research this binary description classification model could be improved by during training split the text into sentences or into smaller chunks to make sure labels are completely seperate inside a single feature.
This way it will be less likely for a single sentence or text chunk to contain multiple pieces of information.
During the training process the parameters are better optimised, resulting in higher prediction values.
During testing on external datasets, it would make sense to use a lower \(\beta\).
This would help the model better for the miss labeled samples.

Second, the lower f1-value could be specifically related to the two left-out datasets from Llife and Agroforestry.
In the second part of Figure \ref{fig:predictionvalues_external} it is clearly visible that more samples are miss labeled as descriptions; there are relatively more sample miss labeled from the threshold value of 0.5 towards 0.8 than below 0.5.
A possible explanation is that paragraphs that are labeled as descriptions (because of their header), have relatively more non-description sentences than non-description paragraphs have description sentences.
If the prediction value of the classifier stay below \(\beta\) the sentences will be miss labaled in both cases.
To test this theory, one could test the classifier on addition datasets that are completely separated from the training process.

After improvement of binary classifier, it would also be interesting to change the threshold values of the binary classifier.
In this research we simple used a threshold of 0.5 to classify whether something is a description or not.
As can be seen from Table \ref{tab:precision_recall_metrics} and Figure \ref{fig:precision_recall_curve_test_external} the f1-score on the external datasets was 0.83. 
This score was reached with a recall of 0.77 and a precision of 0.83.
By increasing the threshold value for classification to recall will go down and precision will go up.
A lower recall in this case would mean less sentences would classified by model.
However, with a higher precision, more sentences that are classified as descriptive, are correct, resulting in a higher quality database.
By lowering the recall enough, a precision 1.0 could be reached.

Lowering the recall will result in less retrieved possible description candidate sentences.
To compensate for this, the web crawler could be improved.
In this research we only crawled through the first pages of results returned by DuckDuckGo and Bing
We did not use the Google search engine as Google creates a unique token for each query to prevent automatic searching without using their payed API.
As can be seen from Figure \ref{fig:URL_distribution_1} the web crawler did not found many unique URLs per species; most values lay between 17 and 33.
As far as we know there is not literature available on the number of online available databases and website that contain descriptions, but these number do not seem that high.
When looking at Figure \ref{fig:URL_distribution_2}, the number of usefull URLs is even lower.
For most species, the number of URLs that is useful, i.e. at least one descriptive sentence is found, lays between 5 and 10.
About two thirds of the URLs found does not contain any useful information for this thesis.
In future research more time should be devoted to building a better web crawler for querying species.

This entails (1) making better use of the search engines.


We only used the first pages of DuckDuckGo and Bing to save time.
We queried DuckDuckGo and Bing with a browser controlled by Python with the Selenium package.

Figure \ref{fig:URL_top20} shows that more than 15\% of the unique base URLs come from scientific databases.
While these databases contain a rich scholarly content, we did not use their data.
Most of these data is offered as a PDF download (or sometimes a viewable PDF).
reconstructing PDF files into their original structure proven to be very difficult.
Some (older) PDF files are scanned, resulting in a single image per two pages. 
It is possible to split this page, but not all PDF files are scanned the same. 
This can result in pages that are not split correctly.
Also PDF files do not have any structural information left about the text itself. 
It is possible to reconstruct the original text using the location of the indentations for every new paragraph.
However the indentation depends on the text margins and this varies across the PDF files.
As for now, we skipped PDF (and similar files). 
In the web crawler we left room to easily implement a PDF reader that could break down PDF files into single sentences.



\newpage
\section{Conclusion} \label{par:conclusion}
In this thesis we investigated how we can automatically query web pages, retrieve the text and assess which sentences are part of a species descriptions and which are not.
We found that a deep learning binary classification with a custom loss function can be quickly trained without annotating any data.
By first gathering text from several structured web sources and using the paragraph and titles as labels and the text split into chunks of random size as features, we reached an f1-score of 0.96 on the test set and and f1-score of 0.83 on two completely left-out datasets.
The custom loss function allows the model for some semi-supervised learning by allowing the model to chance the label of a sample if a predefined threshold is reached.
We deployed this model into a web crawler.
We used scientific databases to find all currently known plant species and bird species.
By deploying the trained binary classifier into this web crawler we were able to create a large dataset that contains description sentences per species.



\newpage
\section{Recommendations}
To make sure a retrieved text page is about the species, we now split the species name, and title page, resulting in two lists. 
We intersected both lists, and if this yielded any result, we extracted and processed the text.
This is a very strict way, as we used the Latin names of the species for the querying.
In some cases the query is successful and correct pages about species are retrieved.
However not all pages contain the Latin name of the species in the title.
Some pages contain the English name, some contain a general name like description and some contain no title at all. 
All these pages are skipped by the web crawler why they still might contain valuable information about the species.
A better approach would be to determine if the web page or even the paragraph is related to the queried species.

\printbibliography
\end{document}