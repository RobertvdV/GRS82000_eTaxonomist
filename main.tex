\documentclass{article}
\usepackage{inputenc}
\emergencystretch=2em
\usepackage[breaklinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=magenta,
    citecolor=black}
    
\usepackage[english]{babel}
\usepackage{biblatex}
% For the scheme
\usepackage{tikz}
\usetikzlibrary{matrix,shapes}
%\setlength{\parindent}{0pt}


%%% For the thesis rings %%%
%\renewcommand{\baselinestretch}{2.0}
\usepackage[left]{lineno}
\linenumbers


\addbibresource{references.bib}
\title{Explainable Artificial Intelligence: Using Natural Language as Intermediate Result Between Two Deep Learning Models}
\author{Robert Vlasakker, van de}
\date{August 2021}

\begin{document}

\maketitle

\section{Introduction}
Deep learning allows for incredible applications from the automatic classification of text and images, natural language processing to reinforcement learning.
In many cases, especially in the case of computer vision, deep learning models have already surpassed human experts \cite{he_delving_2015}.
Because of their success, deep learning vision models are already found quite often in consumer-grade electronics, like smartphones and used in a lot of different application like the \href{https://www.inaturalist.org/}{iNaturalist} application.
%With the \href{https://www.inaturalist.org/}{iNaturalist} model, species can automatically be recognized just by taking a picture of species \cite{radford_learning_2021}.
In natural language processing (NLP), auto-regressive models like GPT-3 (Generative Pre-trained Transformer 3) can produce text that barely can be distinguished from a text produced by real humans \cite{brown_language_2020}.
Microsoft acquired an exclusive licensing on GPT-3 on 22 September 2020 and will integrate the model into their products and services.
While these results and their applications of the deep learning models are incredible, the reasoning behind the results remains a mystery in most cases \cite{li_interpretable_2021, losch_interpretability_2019}.

Unlike classic machine learning models, deep learning models can automatically extract features needed for detection or classification.
Domain knowledge, in combination with careful engineering to extract the necessary features for the detection and/or classification, is no longer needed \cite{lecun_deep_2015}.
While this automatic feature extraction is very convenient, it reduces the interpretability of the models.
To extract the features of the input data, deep learning models use multiple simple neurons that take the input, process it to a slightly more abstract representation and pass it through the next neuron.
Provided enough neurons are 'stacked' upon each other, very complex features can be extracted and correctly detected and/or classified by such a network.
At passing the data also becomes a bit more abstract and less interpretable.
Stacking multiple neurons on top of each often results in millions of parameters in most recent deep learning models (or even billions in the case of GPT-3).
All of these neurons use  non-linear activation functions that increase the overall complexity of the network.
These parameters are the coefficients of the model
They are initially chosen by the designer or randomly initialized by the model and get updated by the model as it 'learns'\footnote{The learning method varies across different deep learning models, but is based on derivatives.} from the data.
After training, the parameters of the model can no longer easily interpreted.
This behaviour is often referred to as a 'back box', which means that the reasoning behind the result is tough to understand or is lacking at all.
The increasing complexity already resulted in several regulations, where the European General Data Protection Regulation (GDPR) is maybe the most famous one.
With sensitive fields, like health care, financial classification or autonomous driving, this black box behaviour could raise issues as it hampers the trustworthiness of the models. \cite{carvalho_machine_2019}.
In these fields it is very important to keep the models comprehensible for humans as the decision of a model will have an impact on human lives.
Also when the reasoning behind deep learning behaviour is better understood, the insights could also lead to the improvements of these models \cite{amershi_modeltracker_2015} and deep learning models can be expanded to more fields \cite{lei_opening_2018}.
Different algorithms and techniques have been proposed to increase the interpretation of the models, like feature reduction algorithms \cite{ribeiro_why_2016}, inference of training sample contribution \cite{koh_understanding_2020} or by changing jittering test samples and see how the prediction changes \cite{li_understanding_2017}.
%One of the most popular techniques for making deep learning models easier to interpret are decomposition and partial derivatives techniques \cite{samek_explainable_2017}.
%Both techniques use heat maps.
%The heat map of a decomposed model will show how much each pixel contributes to the model's prediction.
%The heat map of a model with partial derivatives will show how much the changes in each pixel affect the model's prediction.
Van Lent et al. already coined the term explained artificial intelligence (XAI) in 2004 as the ability to explain behaviour of AI in game applications. \cite{van_lent_explainable_2004}.
While some advances have been made in model understanding, large leaps forward in the field of XAI remain limited \cite{lipton_mythos_2017, li_interpretable_2021}.

In the taxonomy, new species are described now by experts in the field.
Estimated is that 50\% of the species are yet to be discovered, and many species will go extinct before every described.
Automatic monitoring with deep learning is more important than ever in this field.
Deep learning models can help discover new species, automate and speed up this process.
It is, however, important to better understand the reasoning of a deep learning model in such a sensitive case \cite{carvalho_machine_2019}, why are certain species labelled as new and others are not?
It is important to 'teach' deep learning models how taxonomy experts describe new species and in the meantime keep the model output interpretable for humans.
This way reasoning of deep learning can be tracked and evaluated.
A regular classification model will predict species and predict if a species does not exist within a known database based on an image.
In this case, it is vital to see how the model's reasoning works in the case of new predicted species.
Are the species really new species or is the deep learning model not accurate enough?
By splitting a classification model into two models this classification problem might be solved solve this problem and in the meantime create explainable AI.
The input of the first model will still be an image. 
but, instead of predicting species, the model will infer descriptions in natural language based on the image without knowing anything about the species itself.
it is impossible to label all possible description labels 
The second model will take these descriptions to predict to which species the descriptions belong and if there are not any existing species matching the descriptions predict new species.





\section{Objective}
\emph{What is the best way to create a database with species names and their corresponding description data without revealing the name of the species in the description text?}
\newline
\newline
\noindent    
The first objective of this research is to create a large database with species names and their corresponding descriptions. There should not be any double species name. if the description will be stored be species. If the species description is not available the descriptions will be stored per genus or family. The descriptions should consists of single sentences and semantically there should not be any double sentences about the same species.
\noindent    
\newline
\newline
\emph{What is the best way to train a deep learning model to infer species names based on generated description sentences?}
\newline
\newline
\noindent  
A first model (not part of this research will generate description sentences based on an image. The next models needs to take these sentences and make a prediction. This could either be a existing species or the model could suggest a new species. It should be tractable which description sentences are important for inferring a certain (new) species.
    




\section{Methodology}
%Transferlearning
% The first is a visual-language hybrid model that takes an image as input and generates descriptions of the image in natural language using the vocabulary of expert taxonomists, but is not aware of species names.

As description labels can be theoretically limitless, e.g. 'The fur is brown.', 'The brown bear has brown fur.'
Or more sentences with a more difficult semantic, e.g. 'The fur of the Brown bear is similar to that of the Grizzly bear'.
In this case it will not be feasible to use a classic machine learning approach that requires a rule-based system for the feature extraction. 
A deep learning model that can automatically extract the features and classify the text is needed in this case.
However, to properly train a deep learning model that is able to classify text data, a large, accurate and consistent dataset is needed \cite{munappy_data_2019}.
This dataset needs to be labelled so that the model can extract the features.
For the creation of this dataset several structured web sources like \href{http://www.Wikipedia.com}{Wikipedia}, \href{https://birdsoftheworld.org}{Birds of the World} and \href{http://powo.science.kew.org/}{Plants of the World Online} can be used.
These websites do not actually contain labels, but the paragraph do contain paragraph titles like 'Habitat', 'Characteristics'. 
These titles can be used as labels to label the scraped text (1/True in case of 'Characteristics' and 0/False in case of 'Habitat').

If enough data is gathered from structured sources a deep learning model for text classification can be trained.
Text classification in deep learning aims to assign labels to sentences, paragraphs or even complete documents. 
Using a pretrained model with word embeddings can help models achieve much better results, than models trained from scratch \cite{mikolov_distributed_2013}.  
BERT \cite{devlin_bert_2019} is such a pretrained language model that can be used for this task.
BERT is already trained on a large corpus of English words.
By adding a linear layer and with a softmax/sigmoid activation function the BERT model can be used for text classification.
\cite{sun_how_2020} already investigated what the best way is to fine tune BERT for text classification. 
Their results will be used to build a model in PyTorch \cite{paszke_pytorch_2019}.




\section{Time Schedule}
\section{Feasibility}

\printbibliography



\end{document}
