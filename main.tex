%\documentclass{article}
\documentclass[a4paper, 12pt, oneside]{book} % Uses the book template for quick start, but scrbook may be better because it is more flexible
\usepackage{inputenc}
\emergencystretch=2em
\usepackage[breaklinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=magenta,
    citecolor=black}
\usepackage[english]{babel}
%\usepackage{biblatex}
\usepackage[backend=biber, style=apa]{biblatex}
\setlength\bibitemsep{\baselineskip}
\usepackage{svg}
\usepackage{graphicx}
\usepackage[labelfont=bf]{caption}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[left]{lineno} % For line numbers
%\renewcommand{\baselinestretch}{2.0}
%\linenumbers
\usepackage[T1]{fontenc} % Include italic fonts
\usepackage{geometry} % Page margins
\usepackage{titling} % Title page container
\usepackage{wrapfig} % Picture container
\usepackage{titlesec} % Remove chapter header
\renewcommand{\familydefault}{\sfdefault} % Set default f

% COLORS
%\usepackage{xcolor} 
%\usepackage{soul}
%\definecolor{color1}{HTML}{a5ddd0}
%\definecolor{color2}{HTML}{00441b}
%\sethlcolor{color1}\hl{Its trunk bears spikes to deter attacks by animals. <0.35> }
%\sethlcolor{color2}\hl{Branches usually in whorls of 3. <1.0> }




\addbibresource{references.bib}
% Note: Fill these in with correct data, they are used throughout the document
\title{A Step Towards Explainable AI: Infer Species Names Based on Partial Descriptions in Natural Language}
\author{Robert van de Vlasakker}
% This can be left as-is to automatically update
\date{\today}
\geometry{top=2.25cm, 
            bottom=2.91cm,
            inner=2.91cm,
            outer=2.91cm,
            foot=0cm,
            includeheadfoot} % First page margins
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Large}

\begin{document}
 \begin{titlingpage}


  %\newgeometry{top=1.25cm,bottom=1.25cm,inner=0.66cm,outer=0.53cm,foot=1.19cm,includeheadfoot} % Subsequent page margins
  % Note: this uses the MS Word template margins, you might want to increase them a bit for printing (e.g. inner=1.91cm,outer=1.91cm)
  \thispagestyle{empty}
  
  \begin{center}
  {\bfseries \Large \thetitle}
  \newline
  \newline
  \newline
  \newline
  %{\bfseries \itshape Subtitle}\vspace{2.7cm}
  
  {\Large \theauthor}\vspace{0.8cm}
  
  {Registration number 920523897020}\vspace{2.5cm}
  
  {\large \underline{Supervisors}:}\vspace{1.1cm}
  
  {Diego Marcos}
  
  {Ioannis Athanasiadis}\vspace{3.0cm}
  
  %{A thesis submitted in partial fulfilment of the degree of Master of Science}
  
  %{at Wageningen University and Research Centre,}
  
  %{The Netherlands.}\vspace{2.7cm}
  \end{center}
  
  \begin{center}
    {\thedate}
  
    {Wageningen, The Netherlands}
  \end{center}\vspace{6cm}

    Thesis code number: GRS-80400
  
    Thesis Report: Proposal
  
    {Wageningen University and Research Centre}
  
    {Laboratory of Geo-Information Science and Remote Sensing}
 \end{titlingpage}
\graphicspath{ {./figures/} }



\thispagestyle{empty}
\tableofcontents
\thispagestyle{empty}
\listoffigures
\thispagestyle{empty}
\listoftables
\thispagestyle{empty}
\newpage


\renewcommand{\thesection}{\arabic{section}}
\section{Introduction}
\markboth{Introduction}{Introduction}

Estimated is that 50\% of the species are yet to be discovered, and many species will go extinct before ever being described \autocite{lees_species_2015}.
Scalable technologies that can help monitor diversity and help discover new species are more needed than ever.
Deep neural networks (DNNs) can help discover new species, automate and speed up this process \autocite{van_horn_inaturalist_2018}.
However, DNNs are quite rigid, and their black-box behaviour could raise issues as it hampers the trustworthiness of the models \autocite{carvalho_machine_2019}.
It is essential to get more insights into the reasoning of a deep learning model in sensitive fields like taxonomy.
This way, we can learn a DNN to reason like an experienced taxonomist when describing existing and new species.

Deep neural networks (DNNs) allow for remarkable performance in applications: from the automatic classification of text and images, natural language processing (NLP), to reinforcement learning.
DNNs outperform most classic machine learning approaches \autocite{he_delving_2015, brown_language_2020}.
The key to their success is end-to-end training.
Unlike classic machine learning models, deep learning models can automatically extract features needed for detection or classification.
Domain knowledge, in combination with careful engineering to extract the necessary features for the detection or classification, is no longer needed \autocite{lecun_deep_2015}.
However, end-to-end training also results in DNNs that are rigid, difficult to interpret and explain.
To extract the features from the input data, deep learning models use multiple neurons that take the input, process it to a slightly more abstract representation and pass it through the next layer of neurons \autocite{schmidhuber_deep_2015}.
Provided enough layers are stacked upon each other, very complex features can be extracted and correctly detected or classified by such a network.

Because the network's parameters are updated based on its input data, the reasoning of DNNs remains challenging to understand \autocite{li_interpretable_2021, losch_interpretability_2019}, and they do not perform well on long-tailed datasets, like most real-world datasets \autocite{van_horn_inaturalist_2018}.
Stacking multiple layers of neurons on top of each other often results in millions of parameters, and all of these neurons use non-linear activation functions that decrease the interpretability of the network.
In long-tailed datasets, the parameters are not well optimised for less represented classes as the neurons cannot extract the necessary features.
While this automatic feature extraction is very convenient, it will become difficult to track models' reasons, and it can hamper performance.

\begin{figure} [htbp]
    \centering
    \vspace{0cm}
    \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{architecture.pdf}}
    \caption[Proposed architecture]{The proposed architecture by \textcite{ishikawa_contextual_2021} for species classification. The model input is an image. The first model will describe the attributes present in the image in natural language. The second model will take the output of the first model and will make a prediction. The intermediate results remain interpretable by using natural language.}
    \label{fig:intro}
\end{figure}

A more explainable, less rigid, artificial intelligence (AI) might be created by extending the concept of the semantic bottleneck layer from \textcite{ishikawa_contextual_2021} and splitting a regular convolution neural network (CNN) for image classification into two separate models that communicate using natural language.
The first model will describe species features present in the image, and the second model will take these descriptions and tries to infer the species (see Figure \ref{fig:intro}).
This way, DNNs reasonings for species classification can be tracked, mistakes can be spotted, and the models' performance can be improved.
\newpage

\section{Research needs}
Many studies have already stipulated the importance of biodiversity for human life \autocite{pimentel_economic_1997, gowdy_value_1997, raffaelli_links_2010, joppa_biodiversity_2011, pimm_how_2018}.
With the current extinction rate, over 50\% of the species will be gone before ever discovered \autocite{lees_species_2015}.
Protecting biodiversity is now more needed than ever.
However, we cannot conserve undiscovered species; we first need to describe them to protect them \autocite{joppa_biodiversity_2011}.
DNNs can help discover new species and speed up this identification process \autocite{van_horn_inaturalist_2018}.
However, many species look alike.
They are difficult to distinguish and are easily miss-classified, especially with less abundant species.
We need a better understanding of how a DNN identifies and classifies species. 
This way, a less rigid DNN can be created, results of the DNN can be interpreted more easily, and the results can be improved.

Different algorithms and techniques have been proposed to increase the interpretability of the models.
Common approaches are feature reduction algorithms \autocite{ribeiro_why_2016}, inference of training sample contribution \autocite{koh_understanding_2020}, adding jittering to test samples and see how the prediction changes \autocite{li_understanding_2017} and decomposition and partial derivatives techniques \autocite{samek_explainable_2017}.
These algorithms and techniques all rely on posthoc explanations; they try to interpret an already trained DNN and explain its decisions a posteriori.
They try to identify important features via attributions \autocite{zintgraf_visualizing_2017, selvaraju_grad-cam_2017} or assign meaning to features \autocite{fleet_visualizing_2014}.
While some advances have been made in model understanding, giant leaps forward in the field of explainable AI remain limited \autocite{lipton_mythos_2017, li_interpretable_2021}.
These approaches might explain some of the inner workings of a DNN, but they do not help reason a DNN like a taxonomist, the models remain rigid, and the intermediate results still remain challenging to interpret. 

An a priori approach entails designing an architecture network with a semantic bottleneck layer that is interpretable for humans \autocite{bucher_semantic_2019}. 
\textcite{ishikawa_contextual_2021} extended this bottleneck layer architecture by training a model that first learns the intermediate results.
First, a model is trained that can predict semantic attributes.
The second model will take the intermediate results to make a final prediction.
By using their architecture, a less rigid, more explainable AI for species classification might be created.

The first model will be a visual-language model that extracts the species attributes from an image and describes its results in natural language. 
This vision hybrid model will be based on the researches of \textcite{radford_learning_2021} and \textcite{huang_interpretable_2020}.
Their findings allow the first model to learn to extract information from images by looking at raw text data that comes with image captions and describe objects present in those images.
This visual-language hybrid model will describe learned species attributes and use zero-shot learning to describe unseen (new) distinctive species attributes.
By first describing traits, less abundant species can benefit from more abundant species if they share common traits.
The second model is a pure natural language processing (NLP) model that takes the partial descriptions and infer the species.
This way, a models reasoning for species predictions might be tracked by investigating the intermediate results \autocite{ishikawa_contextual_2021} and the final model results will be less rigid.

For the training of both models, an extensive labelled database with species and their descriptions is needed. 
Unfortunately, such a database does not yet exist and needs to be created for this research to happen.
This dataset needs as many unique descriptions as possible for each species, so for both a taxonomist and the NLP model, it would be possible to infer the species name based on provided descriptions.
The NLP model needs to make predictions on partial descriptions, but in the meantime, it should be clear which parts of the description data is used to make to prediction so the reasoning can be evaluated.
This research will focus on creating and curating the dataset containing species and their descriptions and the training and evaluation of the NLP model.

\newpage
\section{Objective}
\markboth{Objective}{Objective}
These aforementioned problems led to the following research questions for this research:
\noindent 
\begin{itemize}
    
    \item \emph{How can a high-quality database be created containing species names and a combination of unique descriptions per species?}
    
    \item \emph{How should a deep learning model be built, trained and evaluated to predict existing species with natural language?}

    \item \emph{What interpretation techniques can be used with the natural language model to clarify which focus points are used for the predictions?}

\end{itemize}

The first objective is to create a high-quality database with species and their descriptions.
If the description is unavailable on a species level, the descriptions will be stored per genus or even per family.
The descriptions are preferably stored per attribute rather than entire text spans.
In this research, two ways of storing the attributes will be explored, (1) storage per sentence and (2) storage as a \href{https://www.ontotext.com/knowledgehub/fundamentals/what-is-a-knowledge-graph/}{semantic triple (object, predicate, object)}.
The second objective will be creating an NLP model that can infer species names based on description data. 
For this objective, two different deep learning models will be explored.
The first is a network with several linear layers that end in a softmax activation function.
The second is a metric deep learning model that does not output a probability but a distance value per input.
The third and final objective is to maintain traceability throughout the deep learning models mentioned in objective two; which description data points are essential for inferring a species?


\newpage
\section{Methodology}
\markboth{Approach}{Approach}
In Figure \ref{fig:workflow} a simplified version of the workflow can be found.
This chapter will give a detailed description of the steps.

\begin{figure} [htbp]
    \centering
    %\vspace{-2cm}
    \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{workflow.pdf}}
    \caption[Workflow]{A simplified workflow. Text data is used to train a model to recognize species description. In this case the paragraph titles are used as labels. The model is deployed in a web crawler that searches the web for species descriptions. These data are then used to train the NLP model independently of the visual-language hybrid model. The NLP model will focus on important words to recognise the species.}
    \label{fig:workflow}
\end{figure}

\subsection{Creating an Extensive Dataset}
The World Wide Web also has potentially an endless amount of species descriptions available.
This makes the internet suitable for harvesting training data.
However, description sentences can be theoretically limitless, e.g. for a Brown Bear description text could be: "The fur is brown", "The brown bear has brown fur".
Sentences can have a more difficult semantic, e.g. "The fur of the Brown bear is similar to that of the Grizzly bear".
A classic machine learning approach that requires a rule-based match system for sentence classification is not possible. 
We train a DNN that can classify text into description or non-description.
To properly train a deep learning model that can classify text data, a large, accurate and consistent labelled dataset is needed \autocite{munappy_data_2019}.

\subsubsection{Training Data}
We create this large dataset by scraping web pages from structured sources.
These sources (Wikipedia, Plant of the World Online (PoWO), Birds of the Word(BoW)), have a rich scholarly content about species.
These websites divide their information into paragraphs such as "Introduction", "Appearance", "Characteristics", "Habit".
These paragraph titles are used as labels and the text inside is used as data.
In our case we stored text from paragraphs with titles like:  "Introduction" and "Habit" as negative values and "Introduction" and "Appearance" as positive values, making this a two class classification problem. 
Random pages from Wikipedia that are not about species are also used to gather additional negative values.
We hypothesised that the model will be better in classifying text not related to species descriptions.


%BERT models use two specials tokens.
%This first token is always \verb|[CLS]|, which stands for "Classification".
%Using \verb|[CLS]|, the model knows the sequence is used for classification.
%The second token is \verb|[SEP]|, which stand for "Separation". 

\subsubsection{BERT for Classification}
As a base for the text classification model, We used a distilled version of the Bidirectional Encoder Representations from Transformers (BERT) \autocite{devlin_bert_2019}, called distilBERT \autocite{sanh_distilbert_2020}. 
This version is 60\% faster, has 40\% less parameters and still reaches 97\% on general language understanding.
Using a pre-trained model can provide significantly improvements over models trained from scratch \autocite{mikolov_distributed_2013}.
The use of a pre-trained model is called transfer learning and could speed up the training process and increase the accuracy of the deep learning model.
Both distillBERT and BERT are already trained on a large corpus of English words and can be used freely.
\textcite{sun_how_2020} already investigated how to fine-tune BERT for text classification.
We use their findings on the full BERT model, to fine-tune the distilBERT model for description text classification

Like \textcite{sun_how_2020} we first fit a fully connected linear layer with an input size of 768 and an output size of 512 on top of the BERT architecture. 
This linear layer is followed by a dropout layer (0.1) to enable some regularization.
The next fully connected linear layer has a input size of 512 and an output size of 2, as we are predicting two classes.
The final activation layer is a log-likelihood (log-softmax) activation function.
The log-softmax will slightly increase the error factor of the model, punishing mistakes a bit higher.
By taking the exponent, the standard likelihood (softmax) probabilities can be computed.

The basis distillBERT architecture can be fine-tuned further by updating the inner parameter of the transformer layers during training. 
This can be useful if BERT is utilised in the non-general domain \autocite{devlin_bert_2019, sun_how_2020, sanh_distilbert_2020}, which is the case with description data.
However, we decided not to fine-tune the inner workings of BERT for two reasons: 
(1) We used additional Wikipedia to increase the text data with negatives values.
These random Wikipedia pages are highly likely to contain information from the general
domain, resulting in a  data distribution close(r) to the general domain.
(2) A binary classification problem is a relatively simple task; updating the inner parameters during training cost additional time and computing power, while only resulting a small accuracy increase.

Instead of a cross entropy loss, we use a loss from \textcite{reed_training_2015}:
\begin{equation} \label{eq:softloss}
 SoftLoss(q, t) = \sum_{k=1}^{L}[\beta t _k + (1- \beta )q _k]log(q _k)
\end{equation}
This allows the model for semi-supervised learning.
Not all data will be labelled correctly as the paragraph titles are used as labels.
E.g. some species description might be located in the paragraph "Introduction" and some general information might be in the paragraph "Characteristics".
This softloss is based on a normal cross entropy loss.
In equation \ref{eq:softloss}, \(q\) is directly used to calculate regression targets for each batch.
If the predefined \(\beta\) reaches a threshold, the prediction is treated as correct and the loss is calculated accordingly.
Consider the text "The European robin is red-breasted with distinctive eyes.".
This text is likely to be found in a paragraph about descriptions.
However, the text can also occur in the introduction paragraph about an European robin or it could be part of an image caption of an entirely different paragraph. 
In second case the text will be miss classified. 
By implementing the semi-supervised loss function of \textcite{reed_training_2015}, this will be prevented to a certain extent.
If the model prediction reaches the set threshold, the prediction is seen a description label, but if the model prediction are below the threshold, the loss will be calculated accordingly.
In this thesis we use a \(\beta\) of 0.80. 

To prepare the text spans, we use the tokenizer of \textcite{wolf_huggingfaces_2020}.
The used BERT architecture can only process up to 512 token at once \autocite{sanh_distilbert_2020, devlin_bert_2019}.
Most tokenizers will truncate text spans larger then 512 tokens as most text information is in the beginning (head-encoding), end (tail-encoding) or beginning and end (head+tail-encoding).
We decide not truncated the text.
Instead we randomly split the text into text spans with a minimum of 10 words and a maximum of 512 words.
We hypothesised that the model might be better in recognising shorter description spans by splitting the raw spans.  
This way we enlarge the dataset, prevent loosing information with truncating spans larger then 512 tokens. 
We use a minimum of 10 words to prevent the model fixating on specific words.
The tokenizer of \textcite{wolf_huggingfaces_2020} returns a Python dictionary with the input ID's and the attention mask of the text.
E.g. by tokenizing the text "This is a test.", the tokenizer returns the following:
\verb|{'input_ids': [101, 2023, 2003, 1037, 3231, 1012, 102]|, 
\verb|'attention_mask': [1, 1, 1, 1, 1, 1, 1]}|.
\verb|[101]| indicates the start of a sequence and \verb|[102]| indicates the end of a sequence.
During the training the text is also padded to the maximum of 512 tokens and the attention masks are randomised between 0 and 1.


\subsubsection{Training BERT for Text Classification}
We train the model on a Google Colab instance.
We used the Adam optimiser \autocite{kingma_adam_2017}.
The Adam optimiser has shown good performance when fine-tuning BERT for text classification \autocite{you_large_2020}.
The hyperparameters were set based on the paper of \textcite{sun_how_2020}.
The learning rate was set to 3e-5 and the batch size was set to 32.
During the training the gradients are clipped to prevent the gradients from becoming to large.
The model was trained for 35 epochs.

\subsubsection{Evaluation BERT for Text Classification}
As the dataset is will be unbalanced it will be evaluated with a precision-recall summary, receiver operating characteristics (ROC) and a precision-recall curve.
The precision-recall summary and curves give more reliable results with an unbalanced dataset.
When the model reaches an f1-score of at least 0.9, the model is tested on two additional left out databases, the \href{http://www.llifle.com/}{LLifle} dataset and the \href{https://www.worldagroforestry.org/}{AgroForestry} dataset.
The data will be first cleaned (e.g. references, footnotes) and will be split into sentences using a senticizer from \textcite{honnibal_spacy_2020}.
The text paragraphs are again used as labels. 
To correct for missclassified labels, a modified version of Equation \ref{eq:softloss} is used to calculated the precision-recall report, see Equation \ref{eq:softloss_ifthen} for the label correction.
\begin{equation} \label{eq:softloss_ifthen}
(\hat{\gamma}^{(i)} >= \beta \rightarrow \hat{\gamma}^{(i)} = \gamma^{(i)}) \wedge ( \leftharpoondown \hat{\gamma}^{(i)} = \hat{\gamma}^{(i)})
\end{equation}
This is the loss function from \textcite{reed_training_2015}, only notated in an If-Then statement to compensate for the miss labelled sentences of the external datasets.

\subsubsection{Web Crawler Deployment}
We deploy the classification model in a web crawler.
This web crawler can automatically query search engines with a predefined list of birds and plants.
The list for plants is based on the \href{https://www.ipni.org/}{International Plant Name Index} (IPNI) and the list for birds is based on the database of \href{https://birdsoftheworld.org/bow/home}{Birds of the World} (BoW).
The IPNI database contains over 1,3 million plant species and the BoW database contains over ten thousand birds.
We sort the plant species based on their number of description in the PoWO dataset that is used to train the classifier.
The IPNI database is large, and there is a high chance that not all species in this database can be queried within the time-frame of this research.
Plant species with a description have a higher chance of being described somewhere else on the web as the PoWO dataset is closely linked to the IPNI database.
This way we make sure that plant that will be queried have a high chance to contain a description somewhere on the web.
This is not an issue for the bird species; the BoW only contains 10k+ described birds.

The web crawler queries the search engines. 
This results in a list URLs with possible species descriptions.
We use five different ways of constructing a query:
\begin{itemize}
    \item "Species"
    \item "Species"+description
    \item "Species"+diagnosis
    \item "Species"+attributes
    \item "Species"+captions
\end{itemize}
We wrap quotations around the species to force the search engine to only return the exact species name. This way search engines cannot return any results that it deems similar.
Each query will return a list of candidate URLs and are appended to a list for a single species.
The URL list is checked for duplicates as this is computationally less expense than calculating the cosine similarity for each sentence to drop duplicates (see Section \ref{Sentence Similarity} about Sentence Similarity).
The URLs are checked for text data without any markup, like raw text data and PDF files.
Finally the header of the URL is checked against the queried species to make sure that the page really contains information about the queried species.
Not every query will return correct websites, i.e. websites about the queried species.
This way we prevent accidentally storing description data for the wrong species.

If the URL meets all criteria, the raw text is retrieved and broken down into single sentences. 
This makes sure the descriptions will be stored as much as possible per unique species trait.
The text is cleaned using several regular expressions. % LINK TO THE EXPRESSIONS
We use a pretrained sentecizer from \textcite{wolf_huggingfaces_2020} to split the text into single sentences.
Every sentence is checked against the train description classifier.
If a description is recognised by the model, the sentence is stored.
We use a threshold of 0.5 to determine if a sentence is a description or otherwise.
In Figures \ref{fig:webcrawler_sents_nopred} and \ref{fig:webcrawler_sents_pred} a small example can be found how the web crawler will process web pages and stores relevant information. 
Figure \ref{fig:webcrawler_sents_nopred} contains the original text before processing and in \ref{fig:webcrawler_sents_pred} the original text is processed into single sentences and each sentence is classified.


\begin{figure} [t]
     \centering
     \begin{subfigure}[b]{1.00\textwidth}
         \centering
         %\hspace{-1.0cm}
         \frame{\includegraphics[width=\textwidth]{web_crawler_example_sents.pdf}}
         \caption[Raw text span example]{The raw text span from a web page. This a small portion of a web page about Bombax ceiba (source: \href{http://www.llifle.com/Encyclopedia/TREES/Family/Bombacaceae/31994/Bombax_ceiba}{Llilfe.com/Bomax\_ceiba}).}
         \label{fig:webcrawler_sents_nopred}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{1.00\textwidth}
         \centering
         %\hspace{-0.5cm}
         \frame{\includegraphics[width=\textwidth]{web_crawler_example_sents_preds.pdf}}
         \caption[Cleaned and classified text span example]{The cleaned and classified text. The text is coloured by the prediction value (high value is dark green, a low value near white). The prediction value of each sentence can be found at the end of the sentence.}
         \label{fig:webcrawler_sents_pred}
     \end{subfigure}
     \caption[Example of the web crawler process]{A small example how the web crawler processes and classifies data from a web page. The raw text span is cleaned from brackets, sources and other textual punctuation. After cleaning the text is split into single sentence. Each sentence is classified by the model. Sentences that reach or exceed the threshold of 0.5 are stored.}
\end{figure}

\subsubsection{Storage Preprocessing} \label{Preprocessing}
Before storage the sentences are processed.
Description sentences often name the species in the description, e.g. "Fagus sylvatica is a large tree, capable of reaching heights of up to 50 m (160 ft) tall." (source: \href{https://en.wikipedia.org/wiki/Fagus_sylvatica}{Wikipedia/wiki/Fagus\_sylvatica}).
Species names are replaced by the word "the species". 
In the example above the sentence is stored as: "The species is a large tree, capable of reaching heights of up to 50 m (160 ft) tall."
This way the NLP model will not have access to (parts of) the labels of the input data. In Figure \ref{fig:webcrawler_sents_nopred} and Figure \ref{fig:webcrawler_sents_pred} and example of a piece of text before and after processing can be found.

\subsubsection{Sentence Similarity} \label{Sentence Similarity}
There is a change that different web pages use the a common source for describing a species.
By checking for similar sentence within the same species we make sure the same sentence is not appended twice to the dataset and the train and test set are completely disjoint.
Like \textcite{reimers_sentence-bert_2019} we use the last hidden state of BERT (distilBERT in our case) and compute the cosine similarity to measure the distance between text spans.
Fortunately, this last hidden state is the output of a base BERT architecture.
However, the last hidden state output contain a matrix of 512 x 768. 
It is not feasibly to compute the cosine similarity distance for multiple matrices of this size.
By using a mean-pooling operation we create a vector of size 768 for every text span and compute the cosine distance for these text spans.

We first create a new dictionary that corresponds to the dictionary initialised by the tokenizer, \verb|{'input_ids': [], 'attention_mask': []}|.
We tokenize all the sentences we are comparing and append the tokens and masking to the correct key.
After all the sentences have been processed, we reformat the dictionary into a single tensor and push it trough the model.
This will result in a tensor of the number of sentences times 512 times 768. 
We sum the 3D matrix along the first axis, resulting in a 2D matrix.
The size of this matrix is the number of sentences time 768.
Each sentence is represented by a vector of size 768
Finally we calculate the cosine similarity between the sentences and drop sentences that exceed the threshold of 0.99.
In Figure \ref{fig:similarity_matrix} a comparison of a few sentences can be found.
\begin{figure} [htbp]
    \centering
    \vspace{0cm}
    \hspace{-1cm}
    %\textbf{Similarity Matrix}
    \makebox[\textwidth][c]{\includesvg[inkscapelatex=false, width=1\textwidth]{similarity_matrix.svg}}
    \caption[Example of sentence similarity]{An example for the calculation of sentence similarity. 
    %In this example the following sentences are compared.
     %0 - "The European robin has a orange plumage."
     %1 - "Black bill, plumage orange."
     %2 - "The bear is large and the claws are sharp."
     %3 - "Bill black; plumage orange."
     %4 - "The bill is black and the plumage is orange."
     %5 - "The bear is large, and the claws are sharp."
    In this case the sentences "Bill black; plumage orange." and "Black bill, plumage orange." reach a similarity score of 0.98. The word order and the punctuation is slightly different. In this case both sentences are stored in the database as it is highly likely they come from different sources. However in the case of "The bear is large and the claws are sharp." and "The bear is large, and the claws are sharp." the sentences are completely similar except for the punctuation. The sentences reach a similarity score of 1 (although this will be because of rounding). In the second case the sentences will come from a the same source and one of them will not be stored.}
    \label{fig:similarity_matrix}
\end{figure}

Some databases store their description as one long sentence, describing multiple attributes in a single sentence e.g. "Leaves 5–9-foliolate; leaflets narrowly elliptic-obovate, entire, acuminate, 7–20 x 1.8–6.5 cm, glabrous; petiole 5.5 - 25 cm long, at the apex expanded into an almost circular disk." (source: \href{http://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:1166232-2}{PoWO/Ceiba\_pentandra}). 
This sentence is describing the leaves and the attributes in the same sentence. 

\subsubsection{Part of Speech}
Creating a knowledge graph of the scraped data will results in a clean database.
This database will only contain objects, relations and subjects. 
When feeding this data to the model it cannot be become dependent on artefacts such as word order, incorrect punctuation etc.
For PoS extracting we use the library of \autocite{honnibal_spacy_2020}.
This is a pretrained NLP model that can extract PoS tags and their dependencies.
In Figure \ref{fig:PoS_example}, an example of PoS tagging and dependency parsing can be found.
\begin{figure} [htb]
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        %\vspace{0cm}
        \includesvg[inkscapelatex=false, width=\textwidth]{PoS_example.svg}
        \caption[Example of part of speech tagging]{An example of part of speech and dependency parsing. The arrows contain the dependency tags and the words contain the part of speech tags.}
        \label{fig:PoS_example}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        %\vspace{-2cm}
        \includegraphics[width=\textwidth]{kn_example.pdf}
        \caption[Example of a knowledge graph]{The sentence  results in three nodes (subject/object) and two edges (relations). The circles contain the objects/subjects and the arrows represent the relations.}
        \label{fig:graph_example}    
    \end{subfigure}
    \caption[Part of Speech tagging and knowledge graph]{A small example for the sentence "The Brown bear has brown fur.". The sentence is first broken down with the NLP pipeline of \textcite{honnibal_spacy_2020}. The extracted PoS tags and dependency tags are then used to construct the semantic triple containing three nodes and two edges. In this case the first node is the normal subject (nsubj) combined witht the compound. The root verb of this subject is the relation and the direct object (dobj) of the verb is the object of the first semantic triple. For the last relation and node, the adjective modifier (amod) of the last node (fur) is used as object. In this case the relation is dependent on the PoS tag of the adjective modifier.}
\end{figure}

\subsection{Inferring Species}
For inferring species on partial descriptions, we explore two different ways of inferring species.
Both model will use a base version of BERT \autocite{devlin_bert_2019}
\subsection{Retrieving Keywords}
We use the integrated gradients from \textcite{sundararajan_axiomatic_2017} to retrieve the most important keywords.
Integrated gradients are best used (1) to identify feature importance, in our case: which are the most important words used for a prediction and (2) to identify data skew, in our case this can be very important. %%% Add something about this.

Our code to calculate the integrated gradients is based on \textcite{gardner_allennlp_2017} and especially this  \href{https://github.com/allenai/allennlp/tree/master/allennlp/interpret}{this part} of their repository.
We use a zero input embedding as a baseline.
All irrelevant words will correspond to the baseline in this case.
We simply compute all the the gradients in a simple loop for a text span:
\begin{equation}
    x' +  \frac{\mathrm{k}}{\mathrm{m}} * (x - x')
\end{equation}
In this case (
For this we first use 20 steps to calculate the gradients as this is the minimum that \textcite{sundararajan_axiomatic_2017} found that could approximate the gradients by 5\%\footnote{The gradients are calculated from 0,00 to almost, but not exactly 1,00. As we are using 20 steps to calculate the gradients, we approximate the gradients by 5\% (\(\frac{1}{20} * 100 = 5\)). By increasing the number of steps, a better approximation of the gradients can be calculated. However, the computation time scales linearly with the number of steps.}.
We calculate the gradients for every input token and sum the gradients.
This leads to the following equation that approximates the gradients:
\begin{equation} \label{eq:integrad}
    {IntegratedGrads_i^{approx}} (x) ::= (x_i - x'_i) *  \sum\nolimits_{k=1}^{m}  \frac{\partial F(x' +  \frac{\mathrm{k}}{\mathrm{m}} * (x - x') }{\partial x_i} 
\end{equation}
\(i\) = feature (token/word)\newline
\(x\) = input (text span tokenized)\newline
\(x'\) = baseline (zero input embedding)\newline
\(k\) = scaled feature perturbation constant\newline
\(m\) = number of steps\newline
\((x - x')\) = difference from the baseline. \newline
\newline
By combining Equation \ref{eq:integrad} with PoS tagging and dependency parsing, we can extract the relevant PoS tags used for the prediction.
However, using a tokenizer can split certain words.
For example, for the sentence "The plant has 7 antheriferous stamens." we would expect nine token (beginning, words, point and ending).
Applying the BERT tokenizer on this sentence results in thirteen tokens:
%\newline
"\verb|[CLS] the plant has 7 ant ##her ##iferous st ##amen ##s. [SEP]|".
%\newline
We average the gradients if a word get split by the tokenizer.
In this example, both \verb|antheriferous| and \verb|stamens| will consist of three integrated gradients. 



\newpage
\section{Results}
\markboth{Results}{Results}
\subsection{Extensive dataset}
\subsubsection{Description Classification}

\begin{figure} [t]
    \centering
    \vspace{0cm}
    \makebox[\textwidth][c]{\includesvg[inkscapelatex=false, width=\textwidth]{histogram_text_length.svg}}
    \caption[Text length distribution for training web crawler model]{The text length distribution before tokenization. The text spans are randomly split into chunks between 10 words and their length, with a maximum of 512 words to prevent truncation. Text spans with a length below 10 words are not split, resulting in some text lengths below 10. There are some text length with the maximum value of 512. This count is barely visible in the plot, so the x-axis limit is set to a max text length of 250.}
    \label{fig:text_length_distribution}
\end{figure}

Scraping the structured sources and using their paragraph titles resulted in 1,086,576 samples.
Each sample consists of a tuple, containing the label (0/1) and the text span.
The text spans of the samples are randomly split into chunk resulting in 1,867,932 tuples with label and text.
451,862 tuples have a label that corresponds to a description (label 1) and 1,416,070 tuples have a label that correspond to something different (label 0).
In Figure \ref{fig:text_length_distribution} the distribution after splitting the original samples in random chunks can be found.

The test data is evaluated with a precision-recall plot.
This plot can be found in Table \ref{tab:precision_recall_descriptionsmodel}.
The model seems to perform excellent on both classes.
The model reaches an high precision, recall and f1-score for both the "Non-Description" and "Description" class.

\begin{table}[h]
\centering
\caption[Precision-recall report for test data]{The precision-recall report for the test set for the binary classification model on descriptions. 10\% of the data is left out for testing. The model is trained for 35 epochs.}
\label{tab:precision_recall_descriptionsmodel}
\begin{tabular}{@{}lcccr@{}}
\cmidrule(l){2-5}
 & \multicolumn{1}{l}{Precision} & \multicolumn{1}{l}{Recall} & \multicolumn{1}{l}{f1-score} & \multicolumn{1}{l}{Support} \\ \midrule
Non-Description  & 0.98 & 0.99 & 0.99 & 167,955 \\
Description      & 0.97 & 0.95 & 0.96 & 57,864  \\ \midrule
Accuracy         &      &      & 0.98 & 225,819 \\
Macro Average    & 0.98 & 0.97 & 0.98 & 225,819 \\
Weighted Average & 0.98 & 0.98 & 0.98 & 225,819 \\ \bottomrule
\end{tabular}
\end{table}

\noindent
The corresponding ROC and precision-recall curve can be found in Figure \ref{fig:ROC_test} and Figure \ref{fig:precision_recall_curve_test}.
Both the ROC curve area and the AP reaches 99\% indicating the binary classifier separates both classes very well.

\begin{figure} [h]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         %\hspace{-1.0cm}
         \includesvg[inkscapelatex=false, width=\textwidth]{AUC-ROC.svg}
         \caption{The Receiver Operating Characteristics (ROC) for the test data.}
         \label{fig:ROC_test}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includesvg[inkscapelatex=false, width=\textwidth]{precision_recall_plot.svg}
         \caption{The Precision-Recall curve for the test data.}
         \label{fig:precision_recall_curve_test}
     \end{subfigure}
     \caption[ROC and precision recall curves for test set]{The ROC and the precision-recall curve for the left out test set. The models seems to perform excellent in both cases. The model is evaluated on a test set that contains 10\% of the data. The Area Under the Curve (AUC) reaches 99\% and the the Average Precision (AP) reaches 99\%.}
\end{figure}
The model is evaluated on two additional datasets, the \href{http://www.llifle.com/}{LLifle} dataset and the  \href{https://www.worldagroforestry.org/}{AgroForestry} dataset
These datasets are completely left out of the training/testing fase of the model.
Together, these dataset contain 17,204 tuples.
771 text spans correspond to a description, and 16,433 text spans corresponds to something different.
After the text has been cleaned and split into sentences there are 74,836 tuples.
There are 8,590 tuples with a descriptions sentence and 66,246 tuples that contain a sentence describing something different.
In Table \ref{tab:precision_recall_descriptionsmodel_external} the precision-recall summary for the external datasets can be found.
The corresponding ROC and precision-recall curve can be found in \ref{fig:ROC_test_external} and \ref{fig:precision_recall_curve_test_external}.
The overall performance of the classifier on the external datasets is excellent. 
The ROC curve area reaches 99\%, the same performance as on the test set (visually it seems a bit less).
The AP reaches 96\%.
\begin{table}[h]
    \centering
    \caption[Precision-recall report for left out datasets]{The precision-recall report for the binary classification model tested on two external datasets (LLifle and AgroForestry).}
    \label{tab:precision_recall_descriptionsmodel_external}
    \begin{tabular}{@{}lcccr@{}}
    \cmidrule(l){2-5}
     & \multicolumn{1}{l}{Precision} & \multicolumn{1}{l}{Recall} & \multicolumn{1}{l}{f1-score} & \multicolumn{1}{l}{Support} \\ \midrule
    Non-Description  & 0.99 & 0.98 & 0.98 & 66,051 \\
    Description      & 0.83 & 0.90 & 0.86 & 8,785  \\ \midrule
    Accuracy         &      &      & 0.97 & 74,836 \\
    Macro Average    & 0.91 & 0.94 & 0.92 & 74,836 \\
    Weighted Average & 0.97 & 0.97 & 0.97 & 74,836 \\ \bottomrule
    \end{tabular}
\end{table}

\begin{figure} [h]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         %\hspace{-1.0cm}
         \includesvg[inkscapelatex=false, width=\textwidth]{AUC-ROC_external.svg}
         \caption{The Receiver Operating Characteristics (ROC) for the left out data.}
         \label{fig:ROC_test_external}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         %\hspace{-0.5cm}
         \includesvg[inkscapelatex=false, width=\textwidth]{precision_recall_plot_external.svg}
         \caption{The Precision-Recall curve for the left out data.}
         \label{fig:precision_recall_curve_test_external}
     \end{subfigure}
     \caption[ROC and precision recall curves for left out datasets]{The ROC and the precision-recall curve for the left out external datasets. The models seems to perform reasonable in both cases. The Area Under the Curve (AUC) reaches 99\% and the the Average Precision (AP) reaches 96\%. In both plots the blue dashed line indicates a baseline classifier.}
\end{figure}

\subsubsection{Web Crawler}

\subsubsection{Sentence Similarity}

\subsection{Species Classification}
\subsection{Key Words}

\section{Conclusion}
By training a deep learning classification model a large dataset that contains species and their descriptions can be created. 

\section{Discussion}

\printbibliography
\end{document}